{
  "articles": [
    {
      "path": "assignments.html",
      "title": "Assignments",
      "author": [],
      "contents": "\n\nContents\nAssignment materials and\ndates\nAssignment expectations and\ngrading\n\n\n\nTO UPDATE THIS PAGE: Open and edit the assignments.Rmd\nfile, in the project root, to delete this placeholder text and customize\nwith your own!\n\nAssignment materials and\ndates\nAssignment materials\nAssigned\nDue date\nA link to the repo\n2021-01-01\n2021-01-08\nA link to the repo\n2021-01-15\n2021-01-23\nA link to the repo\n2021-02-01\n2021-02-09\nA link to the repo\n2021-02-14\n2021-02-23\nAssignment expectations and\ngrading\nThis might also go on the home page & in syllabus\nOr could reinforce here\n\n\n\n\n",
      "last_modified": "2022-08-24T09:08:03-07:00"
    },
    {
      "path": "code_of_conduct.html",
      "title": "Code of Conduct",
      "author": [],
      "contents": "\n\nContents\nOverview\nExamples\nof behavior that contributes to creating a positive environment\ninclude:\nExamples\nof unacceptable behavior by class participants include:\nOther resources at UCSB\n\nAll enrolled students, auditors, and course visitors are expected to\ncomply with the following code of conduct. We expect cooperation from\nall members to help ensure a safe and welcoming environment for\neverybody.\nOverview\nWe are determined to make our courses welcoming, inclusive and\nharassment-free for everyone regardless of gender, gender identity and\nexpression, race, age, sexual orientation, disability, physical\nappearance, body size, or religion (or lack thereof). We do not tolerate\nharassment of class participants, teaching assistants, or instructors in\nany form. Derogatory, abusive, demeaning or sexual language and imagery\nis not appropriate or acceptable. Saying something “as a joke” does not\nmake it less offensive, harmful, or consequential.\nAnything not covered here but that exists in the UCSB\nStudent Conduct Code also applies, and will be enforced by UCSB\nPolicy.\nThese expectations and consequences apply to synchronous discussions,\noffice hours, the course Slack workspace, and all other modes of\ncommunication, posting or discussion by course participants.\nExamples\nof behavior that contributes to creating a positive environment\ninclude:\nUsing welcoming and inclusive language\nBeing an aware and respectful colleague (raise your hand when asked,\nrespect others’ time and space, include peers in small discussions,\ndon’t dominate meetings, etc.)\nGiving proper credit to the creator (of an\nidea/material/solution/etc.)\nBeing respectful of differing viewpoints and experiences\nShowing empathy towards all community members\nUnderstanding that an individual’s experience and worldview are\ninfluenced by multiple (and often compounding) facets of their identity,\nand that your perception of a situation/topic/reaction may be very\ndifferent from your classmates’\nExamples\nof unacceptable behavior by class participants include:\nDistracting other students in classes in labs, or otherwise\ndistracting from their education\nAny abuse, disrespect or harassment of teaching assistants, other\nstudents, or teachers, is not tolerated and will result in disciplinary\naction as needed\nThe use of unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments or language, and personal or\npolitical attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or\nelectronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in\na professional setting\nMembers asked to stop any harassing behavior are expected to comply\nimmediately. If you are being harassed, notice that someone else is\nbeing harassed, or have any other concerns in or related to these\nclasses, you are welcome to contact Allison or use outside\nresources.\nOther resources at UCSB\nAt Campus Advocacy, Resources and Education (CARE) you can chat with\na UCSB staff member in a confidential setting. CARE assists faculty,\nstaff and students who have been impacted by sexual harassment, sexual\nassault, domestic/dating violence, and stalking. The office is\nconfidential so you can talk with a staff member in private without any\nreporting obligations.\nPhone: (805) 893-4613.\nOffice is located in the Student Resource Building (SRB) near\nparking lot 23.\nAnother confidential resource is the Campus Ombuds office. The Ombuds\noffice is particularly helpful if you would like to describe a sensitive\nissue in a confidential setting and learn more about campus resources to\naddress the issue. The Ombuds office is located at 1205-K Girvetz Hall\nand their phone number is 805-893-3285. The Ombuds office provides\nconsultation, mediation, and facilitation, among other services, for\nfaculty, staff and students.\nUCSB Academic Counseling is a resource outside of the Bren department\nthat can help with a number of topics from academic planning to\n“balancing personal difficulties in academics.”\nGraduate counselor: Ryan Sims\nPhone: 805-893-2068\nEmail: ryan.sims@graddiv.ucsb.edu\nThis is a living document, that we are always hoping to improve. If\nyou have suggestions, questions or ideas for how we can update our Code\nof Conduct, we encourage you to reach out to us and will be grateful for\nyour feedback.\nContributions by:\nAllison Horst\nJessica Couture {“mode”:“full”,“isActive”:false}\n\n\n\n",
      "last_modified": "2022-08-24T09:08:04-07:00"
    },
    {
      "path": "day_1.html",
      "title": "Reproducible & Collaborative Workflows",
      "author": [],
      "contents": "\n\nContents\nDay 1\nIntroduction\nReproducible Workflows\nLecture 1: Definitions and\nConcepts\nWhy going reproducible\nHow\n\nHands-on: Planing things\nFrom drawings to\npseudocode\nHands-on\n\n\nWorking on a remote server\nLearning\nObjectives\nWhy working on a remote\nmachine?\n\nFurther\nreading\nData and scientific\nworkflow management:\nOpen Science\n\n\nDay 1\nTime (PST)\nActivity\n9:00am - 9:50am\nIntro & Reproducible and Collaborative workflow (50\nmin)\n9:50am - 10:00am\nBreak 1 (10 min)\n10:00am - 11:00am\nInteractive Session: Drawing board + reporting (60\nmin)\n11:30am - 12:00am\nFlex time 1: Intro to MEDS Analytical server (Kat) (60\nmin)\n12:00pm - 1:15pm\nLunch (75 min)\n1:15pm - 2:05pm\nWorking on a remote server (50 min)\n2:15pm - 2:25pm\nBreak 2 (10 min)\n2:25pm - 3:25pm\nRStudio server & the command line (60min)\n3:25pm - 3:35pm\nRecap of the day (10 min)\n3:35pm - 5:00pm\nQ&A Support with Casey (30 min)\nIntroduction\nA\nfew words…\nReproducible Workflows\nLecture 1: Definitions and\nConcepts\nSlide\ndeck\nWhy going reproducible\nThere are many reasons why it is essential to make your science\nreproducible and how the necessity of openness is a cornerstone of the\nintegrity and efficacy of the scientific research process. Here we will\nalso be focusing on why making your work reproducible will empower\nyou to iterate quickly, integrate new information more\neasily to iterate quickly, scale your analysis to larger data sets, and\nbetter collaborate by receiving feedback and contributions from others,\nas well as enable your “future self” to reuse and build from your own\nwork.\nTo make your data-riven research reproducible, it is important to\ndevelop scientific workflows that will be relying on\nprogramming to accomplish the necessary tasks to go\nfrom the raw data to the results (figures, new data, publications, …) of\nyour analysis. Scripting languages, even better open ones, such as\nR and python, are well-suited for scientists\nto develop reproducible scientific workflows. Those scripting languages\nprovide a large ecosystem of libraries (also referred to as packages or\nmodules) that are ready to be leveraged to conduct analysis and\nmodeling. In this course we will introduce how to use R,\ngit and GitHub to develop such workflows as a\nteam.\n\n\n\nFigure 1: Workflow example using the tidyverse.\nNote the program box around the workflow and the iterative nature of the\nanalytical process described. Source: R for Data Science https://r4ds.had.co.nz/\n\n\n\nTwo points to stress about this figure:\nWorkflows are rarely linear… even less so their implementation\nNote the programming box – yes, you’ll need to code this :)\nWorkflows are developed iteratively, and one of the most\nhelpful things you can do as a data scientist is to talk about them with\nyour research team.\nHow\nWe recommend shying away from spreadsheets as an analytical tool, as\nwell as Graphical User Interfaces (GUI) where you need to click on\nbuttons to do your analysis. Although convenient for data exploration,\nGUI will limit the reproducibility and the scalability of your analysis\nas human intervention is needed at every step. Spreadsheets can be\nuseful to store tabular data, but it is recommended to script their\nanalysis, as copy-pasting and references to cells are prone to mistake\n(see\nReinhart and Rogof example). It is also very difficult to track\nchanges and to scale your analysis using spreadsheets. In addition,\nauto-formatting (number, date, character, …) can silently introduce\nmodifications to your data (e.g. One\nin five genetics papers contains errors thanks to Microsoft\nExcel).\nHands-on: Planing things\nDon’t start implementing nor coding without\nplanning! It is important to stress that scientists write\nscripts to help them to investigate scientific question(s). Therefore\nscripting should not drive our analysis and thinking. We strongly\nrecommend you take the time to plan ahead all the steps you need to\nconduct your analysis. Developing such a scientific workflow will help\nyou to narrow down the tasks that are needed to move forward your\nanalysis.\nFrom drawings to pseudocode\nMaterials\nCherubini et al., 2007Hands-on\nInvestigating the impacts of\nHurricane on stream chemistry in Puerto Rico\nWorking on a remote server\nLearning Objectives\nIn this lesson, you will learn:\nHow to connect to a remote server\nGet familiar with RStudio server\nGet a short introduction to the command line (CLI)\nWhy working on a remote\nmachine?\nOften the main motivation is to scale your analysis beyond\nwhat a personal computer can handle. R being pretty memory\nintensive, moving to a server often provides you more RAM and thus\nallows to load larger data in R without the need of slicing your data\ninto chunks. But there are also other advantages, here are the main for\nscientist:\nPower: More CPUs/Cores (24/32/48), More\nRAM (256/384GB)\nCapacity: More disk space and generally\nfaster storage (in highly optimized RAID arrays)\nSecurity: Data are spread across multiple\ndrives and have nightly backups\nCollaboration: shared folders for code,\ndata, and other materials; same software versions\n=> The operating system is more likely going to be\nLinux!!\nMaterials\nCERN computing centerFurther reading\nHere are a few selected publications to help you to learn more about\nthese topics.\nData and scientific\nworkflow management:\nThe Practice of Reproducible Research-Case Studies and Lessons from\nthe Data-Intensive Sciences:\nKitzes, Justin, Daniel Turek, and Fatma Deniz. n.d. http://www.practicereproducibleresearch.org/\nGood enough practices in Scientific Computing:https://doi.org/10.1371/journal.pcbi.1005510\nScript your analysis:https://doi.org/10.1038/nj7638-563a\nPrinciples for data analysis workflows:https://doi.org/10.1371/journal.pcbi.1008770\nBenureau, F.C.Y., Rougier, N.P., 2018. Re-run, Repeat, Reproduce,\nReuse, Replicate: Transforming Code into Scientific Contributions.\nFront. Neuroinform. 0. https://doi.org/10.3389/fninf.2017.00069\nSandve, G.K., Nekrutenko, A., Taylor, J., Hovig, E., 2013. Ten\nSimple Rules for Reproducible Computational Research. PLOS Computational\nBiology 9, e1003285. https://doi.org/10.1371/journal.pcbi.1003285\nSome Simple Guidelines for Effective Data Management:https://doi.org/10.1890/0012-9623-90.2.205\nBasic concepts of data management:https://www.dataone.org/education-modules\nOpen Science\nThe Tao of open science for ecology:https://doi.org/10.1890/ES14-00402.1\nChallenges and Opportunities of Open Data in Ecology:https://doi.org/10.1126/science.1197962\nScientific computing: Code alerthttps://doi.org/10.1038/nj7638-563a\nOur path to better science in less time using open data science\ntoolshttps://doi.org/10.1038%2Fs41559-017-0160\nFAIR data guiding principleshttps://doi.org/10.1038/sdata.2016.18\nSkills and Knowledge for Data-Intensive Environmental Research https://doi.org/10.1093/biosci/bix025\nLet go your datahttps://doi.org/10.1038/s41563-019-0539-5\n\n\n\n",
      "last_modified": "2022-08-24T09:08:04-07:00"
    },
    {
      "path": "day_2.html",
      "title": "Developing Analytical Workflows as a Team",
      "author": [],
      "contents": "\n\nContents\nDay 2\nCoding\ntogether\nSetting git on Taylor\nSetting GitHub on Taylor\nCollaborating through\nforking\nCollaborating through\nbranches\ngit commit messages\n\nGit Therapy\nGitHub\nconflicts\n\nFurther\nreading\nCollaborative coding\nCode Review\nBranches\nGitHub\nWorkflow\nGit using\nRStudio\nUndoing\nthings\n\n\n\nDay 2\nTime (PST)\nActivity\n9:00am - 9:50am\nCoding together (50 min)\n9:50am - 10:00am\nBreak (10 min)\n10:00am - 10:50am\nCollaborating with GitHub – Forks (50 min)\n10:50am - 11:00am\nBreak (10 min)\n11:00am - 12:30pm\nFlex time 1: Resumé Workshop (Myia) (90 min)\n12:30pm - 1:30pm\nLunch (60 min)\n1:30pm - 2:15pm\nBranches and Pull Requests (45 min)\n2:15pm - 2:25pm\nBreak (10 min)\n2:25pm - 3:25pm\nCollaborating with GitHub – Branches (60 min)\n3:25pm - 3:35pm\nDay 2 Recap (10 min)\n3:35pm - 5:00pm\nFlex session with Casey\nCoding together\nMaterials\nSetting git on Taylor\ngit config --global user.name \"Jane Doe\" \ngit config --global user.email janedoe@example.com\ngit config --global credential.helper 'cache --timeout=10000000'\ngit config --list\nSetting GitHub on Taylor\n\n\n# On your laptop\nusethis::create_github_token() # This should open a web browser on GitHub\n\n# On Taylor \ngitcreds::gitcreds_set()\nusethis::git_sitrep()\n\n\nCollaborating through\nforking\nMaterials\nCollaborating through\nbranches\nMaterials\ngit commit messages\nMaterials\nGit Therapy\nGitHub conflicts\nFirst thing to know is that actually git pull is a two\nstep process: git pull = git fetch + git merge\nSecond: you did nothing wrong!! Git tries to merge files\nautomatically. When the changes are on the same file far apart, git will\nfigure it out on his own and do the merge automatically. However if\nchanges are overlapping, git will call you to the rescue\nand ask you how to best merge the two versions.\nMaterials\nFurther reading\nCollaborative coding\nA new grad’s guide to coding as a team - Atlassian: https://www.atlassian.com/blog/wp-content/uploads/HelloWorldEbook.pdf\n10 tips for efficient programming: https://www.devx.com/enterprise/top-10-tips-for-efficient-team-coding.html\nAgile Manifesto: https://moodle2019-20.ua.es/moodle/pluginfile.php/2213/mod_resource/content/2/agile-manifesto.pdf\nCode Review\nSmall-Group Code Reviews For Education: https://cacm.acm.org/blogs/blog-cacm/175944-small-group-code-reviews-for-education/fulltext\nBranches\nInteractive tutorial to learn more about git branches and more https://learngitbranching.js.org/\nGitHub Workflow\nGitHub:\nguides on how to use GitHub: https://guides.github.com/\nGitHub from RStudio: http://r-pkgs.had.co.nz/git.html#git-pull\n\nForking:\nhttps://help.github.com/articles/fork-a-repo/\nhttps://guides.github.com/activities/forking/\n\nComparison of git repository host services: https://www.git-tower.com/blog/git-hosting-services-compared/\nBranches\ninteractive tutorial on branches: http://learngitbranching.js.org/\nusing git in a collaborative environment: https://www.atlassian.com/git/tutorials/comparing-workflows/centralized-workflow\nhttps://moodle2019-20.ua.es/moodle/pluginfile.php/2213/mod_resource/content/2/agile-manifesto.pdf\n\nGit using RStudio\nHappy Git and GitHub for the useR: http://happygitwithr.com/\nR packages - Git and GitHub: http://r-pkgs.had.co.nz/git.html#git-init\nGit mainly from the command line:\nInteractive git 101: https://try.github.io/\nVery good tutorial about git: https://www.atlassian.com/git/tutorials/what-is-version-control\nGit tutorial geared towards scientists: http://nyuccl.org/pages/gittutorial/\nShort intro to git basics: https://github.com/mbjones/gitbasics\nGit documentation about the basics: http://gitref.org/basic/\nGit documentation - the basics: https://git-scm.com/book/en/v2/Getting-Started-Git-Basics\nGit terminology: https://www.atlassian.com/git/glossary/terminology\nIn trouble, guide to know what to do: http://justinhileman.info/article/git-pretty/git-pretty.png\nWant to undo something? https://github.com/blog/2019-how-to-undo-almost-anything-with-git\nGit terminology: https://www.atlassian.com/git/glossary/terminology\n8 tips to work better with git: https://about.gitlab.com/2015/02/19/8-tips-to-help-you-work-better-with-git/\nGitPro book (2nd edition): https://git-scm.com/book/en/v2\nUndoing things\nHelp to decide how to undo your problem: http://justinhileman.info/article/git-pretty/git-pretty.png\nUndo almost everything with git https://blog.github.com/2015-06-08-how-to-undo-almost-anything-with-git/\nDifference between git reset soft, mixed and hard https://davidzych.com/difference-between-git-reset-soft-mixed-and-hard/\nResetting, Checking Out & Reverting https://www.atlassian.com/git/tutorials/resetting-checking-out-and-reverting\n\n\n\n",
      "last_modified": "2022-08-24T09:08:06-07:00"
    },
    {
      "path": "day_3.html",
      "title": "Managing your Code and Data on a Remote Server",
      "author": [],
      "contents": "\n\nContents\nDay 3\nGetting Data\nIntroduction to APIs\nHands-on –\nAPIs\n\nManaging data-driven\nprojects as a team\nGroup\nproject\n\nDay 3\nTime (PST)\nActivity\n10:00am - 10:10am\nRecap of previous days (10min)\n10:10am - 11:00am\nIntroduction to APIs (50min)\n11:00am - 11:10am\nBreak 2 (10 min)\n11:10am - 12:00am\nAPIs hands-on (60 min)\n12:00pm - 1:15pm\nLunch (75 min)\n1:15pm - 2:00pm\nManaging data-driven projects as a team\n2:00pm - 2:30pm\nIntroduction to the group project (30 min)\n2:30pm - 3:30pm\nGroup Projects (60min)\n3:35pm - 5:00pm\nQ&A with Casey\nGetting Data\nIntroduction to APIs\nMaterials\nHands-on – APIs\nMaterials\nManaging data-driven\nprojects as a team\nMaterials\nGroup project\nGroup project\n\n\n\n",
      "last_modified": "2022-08-24T09:08:06-07:00"
    },
    {
      "path": "day_4.html",
      "title": "Scientific Programming as a Team",
      "author": [],
      "contents": "\n\nContents\nDay 4\nCoding as a\nteam\nCode\nreview exchange\nPair\nprogramming\nA few more thoughts on\ncoding together\nMore is not always better\nProject\nmanagement\n\n\nDocumenting things\n\nDay 4\nTime (PST)\nActivity\n9:00am - 9:50am\nCode Exchange & pair programming (50min)\n9:50am - 10:00am\nBreak 1 (10 min)\n10:00am - 11:00am\nGroup Project (60min)\n11:00am - 12:00pm\nUpdating your GitHub profile (Sam)\n12:00pm - 1:15pm\nLunch (75 min)\n1:15pm - 2:15pm\nDocumenting things (60 min)\n2:15pm - 2:25pm\nBreak (10min)\n2:25pm - 3:25pm\nGroup Project (60min)\n3:30pm - 4:30pm\nQ&A with Casey\n4:30pm - …\nHappy hour with Ginger ;)\nCoding as a team\nCode review exchange\nCode review exchange is an asynchronous team\nactivity. Despite its impressive name, code review should be seen by the\nperson asking for the review (submitter) as a great opportunity to have\none more pair of eyes looking at your code and providing feedback to\nmake your code better. The “reviewer” should see this activity as a\ngreat way to learn from others and provide constructive feedback.\nYou actually have already practice this a little when practicing\nmerging pull request!!\nPair programming\nMaterial\nA few more thoughts on\ncoding together\nMore is not always better\nAdding more analysts late into a project can be counterproductive\n(Brooks’\nlaw)\nOvertime is not the solution as it will increase likeliness of\nerrors and thus frustration. Try to focus on finding at what time of the\nday you are the most productive at coding instead\nProject management\nThis is a big topic and will be highly influenced by the team you are\nworking with both in terms of practices and tools used to manage the\nproject. On the coding side, there is a lot to borrow from Agile\ndevelopment approach for scientist. In a nutshell: “put it out there\nfast and iterate”. In other words to wait to try to have the perfect\ncode or analysis before sharing it with your collaborators. It is more\nefficient to share and discuss an early / draft version, gather feedback\nand iterate.\nDocumenting things\nMaterials\n\n\n\n",
      "last_modified": "2022-08-24T09:08:07-07:00"
    },
    {
      "path": "day_5.html",
      "title": "Sharing things",
      "author": [],
      "contents": "\n\nContents\nDay 5\nSharing research products\nWhy does\nit matter\n\nGroup Projects\nPresentations\n\nDay 5\nTime (PST)\nActivity\n9:00am - 9:45am\nSharing things (45min)\n9:45am - 10:00am\nXarigan & Quarto Presentations (15 min)\n10:00am - 10:10am\nBreak 1 (10min)\n10:10am - 12:00pm\nGroup project (110min)\n11:00am - 12:00pm\nMeet your next instructor – Kelly Caylor, Earth Research\nInstitute, UCSB\n12:00pm - 1:15pm\nLunch (75 min)\n1:15pm - 1:45pm\nOpen\nSource Software for Data Science (30 min)\n1:45pm - 2:45pm\nGroup project presentations - p1 (60 min)\n2:45pm - 2:55pm\nBreak 2 (10min)\n2:55pm - 3:55pm\nGroup project presentations - p2 (60 min)\n3:55pm - 4:05pm\nBreak 3 (10min)\n4:05pm - 4:30pm\nFinal thoughts and feedback\nSharing research products\nWhy does it matter\n\n\n\nMaterial\nGroup Projects Presentations\n\n\n\n",
      "last_modified": "2022-08-24T09:08:08-07:00"
    },
    {
      "path": "day1-cli_advanced.html",
      "title": "Working on a remote server",
      "author": [],
      "contents": "\n\nContents\nAdvanced Topics at the\ncommand line\nConnecting to a remote\nserver via ssh\nUnix systems are\nmulti-user\nGetting help\nfinding\nstuff\nGetting\nthings done\nSome useful,\nspecial commands using the Control key\nProcess\nmanagement\nWhat about\n“space”\nHistory\n\nA sampling\nof simple commands for dealing with files\nGet into the flow, with\npipes\nText editing\nSome editors\n\nSearching advanced utilities\ngrep\nLet’s look at our text file\n\nCreate custom commands with\n“alias”\n\nAknowledgements\n\nAdvanced Topics at the\ncommand line\nWe will not cover this during class, it is for your\nreference. You will also have the opportunity to further\npractice and learn about the command line in EDS-215.\nConnecting to a remote\nserver via ssh\nFrom the gitbash (MS Windows) or the terminal (Mac) type:\nssh taylor.bren.ucsb.edu\nYou will be prompted for your username and password.\naurora_sshYou can also directly add your username:\nssh brun@taylor.bren.ucsb.edu\nIn this case, you will be only asked for your password as you already\nspecified which user you want to connect with.\nUnix systems are multi-user\nWho else is logged into this machine? who\nWho is logged into “this shell”? whoami\nGetting help\n<command> -h,\n<command> --help\nman, info, apropos,\nwhereis\nSearch the web!\nfinding stuff\nShow me my Rmarkdown files!\nfind . -iname '*.Rmd'\nWhich files are larger than 1GB?\nfind . -size +1G\nWith more details about the files:\nfind . -size +1G -ls\nGetting things done\nSome useful,\nspecial commands using the Control key\nCancel (abort) a command: Ctrl-c Note: very different\nthan Windows!!\nStop (suspend) a command: Ctrl-z\nCtrl-z can be used to suspend, then background a\nprocess\nProcess management\nLike Windows Task Manager, OSX Activity Monitor\ntop, ps,\njobs (hit q to get out!)\nkill to delete an unwanted job or process\nForeground and background: &\nWhat about “space”\nHow much storage is available on this system?\ndf -h\nHow much storage am “I” using overall?\ndu -hs <folder>\nHow much storage am “I” using, by sub directory?\ndu -h <folder>\nHistory\nSee your command history: history\nRe-run last command: !! (pronounced “bang-bang”)\nRe-run 32th command: !32\nRe-run 5th from last command: !-5\nRe-run last command that started with ‘c’: !c\nA sampling\nof simple commands for dealing with files\nwc count lines, words, and/or characters\ndiff compare two files for differences\nsort sort lines in a file\nuniq report or filter out repeated lines in a file\nGet into the flow, with pipes\nstdin, stdout,\nstderr$ ls *.png | wc -l\n$ ls *.png | wc -l > pngcount.txt\n$ diff <(sort file1.txt) <(sort file2.txt)\n$ ls foo 2>/dev/null\nnote use of * as character wildcard for zero or more\nmatches (same in Mac and Windows)\n? matches single character; _ is SQL query\nequivalent\nText editing\nSome editors\nvim\nemacs\nnano\n$ nano .bashrc\nSearching advanced utilities\ngrep search files for text\nsed filter and transform text\nfind advanced search for files/directories\ngrep\nShow all lines containing “bug” in my R scripts\n$ grep bug *.R\nNow count the number of occurrences per file\n$ grep -c bug *.R\nPrint the names of files that contain bug\n$ grep -l bug *.R\nLet’s look at our text file\ncat print file(s)\nhead print first few lines of file(s)\ntail print last few lines of file(s)\nless “pager” – view file interactively (type\nq to quit command)\nod --t “octal dump” – to view file’s underlying\nbinary/octal/hexadecimal/ASCII format\n$ brun@aurora:~/data$ head -3 env.csv\nEnvID,LocID,MinDate,MaxDate,AnnPPT,MAT,MaxAT,MinAT,WeatherS,Comments\n1,*Loc ID,-888,-888,-888,-888,-888,-888,-888,-888\n1,10101,-888,-888,-888,-888,-888,-888,-888,-888\n\n$ brun@aurora:~/data$ head -3 env.csv | od -cx\n0000000   E   n   v   I   D   ,   L   o   c   I   D   ,   M   i   n   D\n           6e45    4976    2c44    6f4c    4963    2c44    694d    446e\n0000020   a   t   e   ,   M   a   x   D   a   t   e   ,   A   n   n   P\n           7461    2c65    614d    4478    7461    2c65    6e41    506e\n0000040   P   T   ,   M   A   T   ,   M   a   x   A   T   ,   M   i   n\n           5450    4d2c    5441    4d2c    7861    5441    4d2c    6e69\n0000060   A   T   ,   W   e   a   t   h   e   r   S   ,   C   o   m   m\n           5441    572c    6165    6874    7265    2c53    6f43    6d6d\n0000100   e   n   t   s  \\r  \\n   1   ,   *   L   o   c       I   D   ,\n           6e65    7374    0a0d    2c31    4c2a    636f    4920    2c44\n0000120   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -\n           382d    3838    2d2c    3838    2c38    382d    3838    2d2c\n0000140   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -   8\n           3838    2c38    382d    3838    2d2c    3838    2c38    382d\n0000160   8   8   ,   -   8   8   8  \\r  \\n   1   ,   1   0   1   0   1\n           3838    2d2c    3838    0d38    310a    312c    3130    3130\n0000200   ,   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,\n           2d2c    3838    2c38    382d    3838    2d2c    3838    2c38\n0000220   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -\n           382d    3838    2d2c    3838    2c38    382d    3838    2d2c\n0000240   8   8   8   ,   -   8   8   8  \\r  \\n\n           3838    2c38    382d    3838    0a0d\nCreate custom commands with\n“alias”\nalias lwc='ls *.jpg | wc -l'\nYou can create a number of custom aliases that are available whenever\nyou log in, by putting commands such as the above in your shell start-up\nfile, e.g. .bashrc\nAknowledgements\nThis section reuses a lot of materials from NCEAS Open Science for Synthesis\n(OSS) intensive summer schools and other training. Contributions to\nthis content have been made by Mark Schildhauer, Matt Jones, Jim Regetz\nand many others.\n\n\n\n",
      "last_modified": "2022-08-24T09:08:09-07:00"
    },
    {
      "path": "day1-cli_practice.html",
      "title": "Practicing CLI",
      "author": [],
      "contents": "\n\nContents\nFiles and directories\nYour turn\nYour turn\n\nAknowledgements\n\nFiles and directories\nIn the following code examples, you need to type the command, but not\ninclude the command prompt (e.g., brun@taylor:~$) which\njust shows that the computer is ready to accept a command.\nWe’ll start by:\ncreating two directories with mkdir (make\ndirectory)\ncreate a simple text file using echo\nShow the contents of that file using cat\n(concatenate)\nbrun@taylor:~$ mkdir cli_intro\nbrun@taylor:~$ mkdir cli_intro/data\nbrun@taylor:~$ echo \"# Tutorial files related to CLI\" > cli_intro/README.md\nbrun@taylor:~$ cat cli_intro/README.md\n# Tutorial files related to CLI\nbrun@taylor:~$ \nYour turn\nNext,\nlet’s copy another file and look around in the directories:\ncopy a file into your directory with cp (copy)\nchange our working directory to that newly created directory using\ncd (change directory)\nlist the files in the directory with ls (list)\nlook where we are in the filesystem using pwd (print\nworking directory)\nget an overview of the directory contents using\ntree\nUpload files from your local machine to the server using the\ndifferent techniques mentioned above, You can download the\n10min-loop.R file to your local machine from https://aurora.nceas.ucsb.edu/~brun/10min-loop.R\nTry with a folder:\ndownload the data to your laptop: https://aurora.nceas.ucsb.edu/~brun/sampledata.zip\nunzip the folder\nupload the folder to Taylor using scp or cyberduck\nYour should end up with something like this:\nbrun@taylor:~$ ls -l\ntotal 16\n-rw-r--r-- 1 brun esmdomainusers   90 Aug 25 05:36 10min-loop.R\ndrwxr-xr-x 3 brun esmdomainusers 4096 Aug 24 23:05 github\ndrwxr-xr-x 3 brun esmdomainusers 4096 Aug 16 05:00 R\ndrwxr-xr-x 2 brun esmdomainusers 4096 Aug 25 05:37 sampledata\nYour turn\nNow, let’s create two subdirectories in the data directory:\nmammals and plots\nmove using cp all the mammals files from the\nsampledata folder to the mammals subdirectory;\nhint: you can use the wildcard *\nmove using mv the other files files from the\nsampledata folder to the plots\nsubdirectory\ndouble check it is done using cd and\nls\nremove rm the sampledata directory; hint:\nrmdir can only remove empty directories\nbonus: add a text file data_listing.txt in the\ndata folder that lists all the files in it\nAknowledgements\nAdapted from Matt Jones, OSS 2017, https://github.com/NCEAS/oss-lessons\n\n\n\n",
      "last_modified": "2022-08-24T09:08:10-07:00"
    },
    {
      "path": "day1-hands-on_drawings_p2.html",
      "title": "Hands-on: Planning your work",
      "author": [],
      "contents": "\n\nContents\nGood news we\nfound data for one more watershed :)\nThis is your lucky day!!\n\nGood news we\nfound data for one more watershed :)\nActually our team also collected data for an adjacent watershed!!\n\n\n\n=> How should you modify your workflow to handle this new\nsituation? (10min)\nThis is your lucky day!!\nWe found data for additional chemistry measurements while skimming\nthrough the database :)\nWe now want to be able to produce the following plot\n\n\n\n=> How should you modify your workflow to handle this new\nsituation? (5min)\n\n\n\n",
      "last_modified": "2022-08-24T09:08:10-07:00"
    },
    {
      "path": "day1-hands-on_drawings.html",
      "title": "Hands-on: Planning your work",
      "author": [],
      "contents": "\n\nContents\nGoal\nA few\nthings to know:\nStudy sites\nOutput\n\nTo Do:\nRemember\nOh wait…\n\nReferences\n\n\nGoal\nDraw the workflow to combine 4 datasets about stream flow and\nwater chemistry in a way that will let you investigate the impact of the\n1989 Hurricane Hugo on Stream Chemistry in the Luquillo Mountains of\nPuerto Rico\nA few things to know:\nTable structures are different => only some variables / columns\noverlap among the different sites\nUnits used among the various sites are different\nPeriod covered is not perfectly aligned for all the time-series.\nSome sites start or end before others, but there is a period of\noverlap\nStudy sites\n\n\n\nOutput\nWe want to be able to produce this plot\n\n\n\nTo Do:\nDraw the data processing steps to harmonize those data into\none dataset that will let you compare time-series data and reproduce the\nabove plot (20min)\nRemember\nEach node represents a step or an\ninput/output\nEach connecting edge represents data flow or\nprocessing\nOh wait…\nActually here is\nmore\nReferences\nExercise based on: Schaefer, D., McDowell, W., Scatena, F., &\nAsbury, C. (2000). Effects of hurricane disturbance on stream water\nconcentrations and fluxes in eight tropical forest watersheds of the\nLuquillo Experimental Forest, Puerto Rico. Journal of Tropical Ecology,\n16(2), 189-207. doi:10.1017/S0266467400001358\nData available here: McDowell, W. 2021. Chemistry of stream water\nfrom the Luquillo Mountains ver 4923052. Environmental Data Initiative.\nhttps://doi.org/10.6073/pasta/ddb4d61aea2fb18b77540e9b0c424684\n(Accessed 2021-08-06).\n\n\n\n",
      "last_modified": "2022-08-24T09:08:11-07:00"
    },
    {
      "path": "day1-pseudocode.html",
      "title": "Flow charts and pseudocode",
      "author": [],
      "contents": "\n\nContents\nFlow charts\nPseudocode\nFurther\nreading\n\nFlow charts\nFlowcharts are useful to visualize and develop analytical workflow.\nIt guides planning and anticipating the various computing and analytical\ntasks that will be required to complete an analysis. It also helps\nexplaining the different steps to your collaborators and team. You can\nuse a flowchart to spell out the logic behind a analytical workflow\nbefore ever starting to code. It can help to organize big-picture\nthinking and provide a guide when it comes time to code. More\nspecifically, flowcharts can:\nVisualize the sequence of the different phases of the analytical\nprocess from data collection to implementing analyses\nBetter define the scope and resources needed both in terms of\nproject data management and computing resources needed\nDiscuss who will be in charge and involved in the development of the\ndifferent parts of the workflow\nList the products / outputs that will result from your analysis -\nsuch as data, codes, publications, web presence, … - and discuss how to\nbest preserve and share them\nThere are conventions on how to use symbols to represent different\nparts of a workflow (see here for example).\nAlthough it is good to be aware of those conventions for sharing a\nworkflow with your community, within a team the most important aspect is\nto be coherent and stick to this usage overtime.\nThe main benefit for the project is the process and discussion as a\nteam to develop the workflow. It will ensure that everybody is on the\nsame page and has the opportunity to provide inputs on the project. This\nworkflow should be updated regularly as the project evolves.\nA\nfew workflow examples\nPseudocode\nOften, programmers may write pseudocode as a next step, to provide\ngreater detail than the flowchart in terms of processing steps and\nimplementation. It will help you to define where iterations will be\nneeded but also detect repeating blocks that might be well suited to be\nhandled via the development of a function.\nThis technique aims at developing a sequence of pragrammatical steps\nin a manner that is easy to understand for anyone with basic programming\nknowledge. Pseudocode can be implemented more or less formally and at\nvarious levels of details. One additional advantage of going through\nthis process is that it is agnostic of the tools / programming languages\nthat you will be using to develop your analytical workflow.\nIn this course, we will be focusing on the process more than the\nexact syntax to use, keeping the level of details at a level that\nprovide more details than a flow chart.\nFurther reading\nWhat is a flowchart: https://www.lucidchart.com/pages/what-is-a-flowchart-tutorial\nHow to write pseudocode: https://www.wikihow.com/Write-Pseudocode\nAn Introduction to Writing Good Pseudocode: https://towardsdatascience.com/pseudocode-101-an-introduction-to-writing-good-pseudocode-1331cb855be7\nHow to write Pseudocode: A beginner’s guide https://blog.usejournal.com/how-to-write-pseudocode-a-beginners-guide-29956242698\n\n\n\n",
      "last_modified": "2022-08-24T09:08:12-07:00"
    },
    {
      "path": "day1-remote_server.html",
      "title": "Working on a remote server",
      "author": [],
      "contents": "\n\nContents\nLearning\nObjectives\nWhy working on a remote\nmachine?\nCollaborative Setup\nWhat does working on\na remote server means?\nRStudio Server\nConnecting to MEDS\nAnalytical Server\nFile\nstructure\nR packages\n\n\nCommand line\nIntroduction to UNIX and\nits siblings\nSome\nUnix hallmarks\n\nThe Command Line Interface\n(CLI)\nWhy the CLI is worth\nlearning\nThe Terminal from RStudio\nNavigating and\nmanaging files/directories in *NIX\nPermissions\n\nGeneral command syntax\nCLI\nPractice\n\n\nUploading Files to a server\nRStudio\nsFTP\nSoftware\nscp\n\nAdvanced Topics at the\ncommand line\nAknowledgements\n\nLearning Objectives\nIn this lesson, you will learn:\nHow to connect to a remote server\nGet familiar with RStudio server\nGet a short introduction to the command line (CLI)\nWhy working on a remote\nmachine?\nOften the main motivation is to scale your analysis beyond\nwhat a personal computer can handle. R being pretty memory\nintensive, moving to a server often provides you more RAM and thus\nallows to load larger data in R without the need of slicing your data\ninto chunks. But there are also other advantages, here are the main for\nscientist:\nPower: More CPUs/Cores (24/32/48), More\nRAM (256/384GB)\nCapacity: More disk space and generally\nfaster storage (in highly optimized RAID arrays)\nSecurity: Data are spread across multiple\ndrives and have nightly backups\nCollaboration: shared folders for code,\ndata, and other materials; same software versions\n=> The operating system is more likely going to be\nLinux!!\nCollaborative Setup\nThere are additional reasons of particular importance in a\ncollaborative set up, such as a working group:\nCentralizing data management: As you know synthesis\nscience is data intensive and often require to deal with a large number\nof heterogeneous data files. It can be complicated to make sure every\ncollaborators as access to all the data they need. It is even harder to\nensure that the exact same version of the data is used by everyone.\nMoving your workflow to a server, will allow to have only one copy of\nthe data that you can share with all your collaborators. Even better,\nsince everyone can access the same data, everybody will have the exact\nsame path in their script!!\nMake sure your files are safe: Generally, servers\nare managed by a System Administrator. This person is in charge of\nkeeping the server up-to-date, secured from malwares and set up back up\nstrategies to ensure all the files on the server are backed up. When\nusing cloud solutions, you should always check if a back up plan is\navailable for the resources your using.\nWhat does working on\na remote server means?\nWhat does it mean for your workflow? The good news is that RStudio\nServer makes it very easy for RStudio users to start using a server for\ntheir analysis. The main changes are about:\nFile management: you will need to learn to move files (including\nyour R scripts) to the server\nPackage installation: You can still install the R packages you need\nunder your user (with some limitations). However some R packages will be\nalready installed at the server level.\nRStudio Server\nFrom an user perspective, RStudio Server is your familiar RStudio\ninterface in your web browser. The big difference however is that\nwith RStudio Server the computation will be running on the remote\nmachine instead of your local personal computer. This also means that\nthe files you are seeing through the RStudio Server interface\nare located on the remote machine. And this also include your R\npackages!!! This remote file management is the main change you\nwill have to adopt in your workflow.\nTo help with remote files management, the RStudio Server interface as\nfew additional features that we will be discussing in the following\nsections.\n\n\n\nConnecting to MEDS\nAnalytical Server\nGot to: https://taylor.bren.ucsb.edu/\nEnter your credentials\nYou are in!\n\n\n\nClick on the New Session button. You can see that you\nare able to start both an R (Studio) and jupyter notebook session. Let’s\ntake a few minutes to experiment with the different options.\nFor this session, we are going to select the RStudio\noption and hit Start Session.\n\n\n\nYou should now see a very familiar interface :) Except it is\nrunning on the server with a lot of resources at your\nfingertips!!\nFile structure\nLet’s explore explore a little bit the file structure on the server.\nBy default on a Linux server, you are located in the home\nfolder. This folder is only accessible to you and it is where you can\nstore your personal files on a server. You should see 2 folders:\nR and H\n\n\n\nThe R folder is where your local R packages will be\ninstalled, you can ignore it. The H is your H drive that\nthe Bren School is offering to all its students. If you click on it you\nshould see any files you have uploaded there.\nLet us make a folder named github by click on the\nNew Folder button at the top of the tab. We will use this\nfolder (also named directory in linux/unix terms) to clone\nany GitHub repository.\nR packages\nIf we go to the Packages tab, we can see a long list of\npackages that have already be installed by our system administrator\n(Brad). Those packages have been installed server wide, meaning that all\nthe users have access to them.\n\n\n\nA user can also installed her/his own packages. Let’s try to install\nthe remote package that lets you install R packages\ndirectly from GitHub: install.packages(\"remotes\"). Once\ndone, note a new section that appeared on the Packages tab named\nUser Library. Each of us have now its own copy of the\npackage installed (in this R folder we were talking about a\nfew minutes ago).\n\n\n\nA few notes:\nIn this example we will have made a better choice to have the\nremotes package installed once at the system level\nSome R packages depend on external libraries that need to be\ninstalled on the server. Those libraries will have to be installed by\nthe system administrator first before you can install the R package\nInstalling an R package on a linux machine generally requires\ncompilation of the code and will thus take more time to install than\nwhen you install it from pre-compiled binaries\nLook now inside you R folder!!\nCommand line\nIntroduction to UNIX and\nits siblings\nUNIX\n\nOriginally developed at AT&T Bell Labs circa 1970. Has experienced a\nlong,\nmulti-branched evolutionary path\n\nPOSIX (Portable Operating System Interface)\n\na set of specifications of what an OS needs to qualify as “a Unix”, to\nenhance interoperability among all the “Unix” variants\n\nVarious Unices\nThe unix family treeOS X\n\nis\na Unix!\n\n\nLinux\n\nis not fully POSIX-compliant, but certainly can be regarded as\nfunctionally Unix\n\n\nSome Unix hallmarks\nSupports multi-users, multi-processes\nHighly modular: many small tools that do one thing well, and can be\ncombined\nCulture of text files and streams\nPrimary OS on HPC (High Performance Computing\nSystems)\nMain OS on which Internet was built\nThe Command Line Interface\n(CLI)\nThe CLI provides a direct way to interact with the Operating System,\nby typing in commands.\nWhy the CLI is worth\nlearning\nTypically much more extensive access to features, commands,\noptions\nCommand statements can be written down, saved (scripts!)\nEasier automation\nMuch “cheaper” to do work on a remote system (no need to transmit\nall the graphical stuff over the network)\nThe Terminal from RStudio\nYou can access the command line directly from RStudio by using the\nTerminal tab next to your R console.\n\nNavigating and\nmanaging files/directories in *NIX\n\npwd: Know where you are\nls: List the content of the directory\ncd: Go inside a directory\n~ : Home directory\n. : Here (current directory)\n..: Up one level (upper directory)\nLet’s put this into action:\ngo to my “Home” directory: cd ~\ngo up one directory level: cd ..\nlist the content: ls\nlist the content showing hidden files: ls -a note that\n-a is referred as an option (modifies the command)\nMore files/directories manipulations:\nmkdir: Create a directory\ncp: Copy a file\nmv: Move a file it is also how you rename a\nfile!\nrm / rmdir: Remove a file / directory\nuse those carefully, there is no return / Trash!!\nNote: typing is not your thing? the <tab>\nkey is your friend! One hit it will auto-complete the\nfile/directory/path name for you. If there are many options, hit it\ntwice to see the options.\nPermissions\nAll files have permissions and ownership.\nFile permissionsChange permissions: chmod\nChange ownership: chown\nList files showing ownership and permissions:\nls -l\nbrun@taylor:/courses/EDS214$ ls -l\ntotal 16\ndrwxrwxr-x+ 3 brun      esmdomainusers 4096 Aug 20 04:49 data\ndrwxrwxr-x+ 2 katherine esmdomainusers 4096 Aug 18 18:32 example    \nClear contents in terminal window: clear\nGeneral command syntax\ncommand [options] [arguments]\nwhere command must be an executable file on\nyour PATH * echo $PATH\nand options can usually take two forms * short form:\n-a * long form: --all\nYou can combine the options:\nls -ltrh\nWhat do these options do?\nman ls\nTip: hit q to exit the help!\nCLI\nPractice\nTo end an SSH session, simply type exit or\nlogout into the command line\nUploading Files to a server\nYou have several options to upload files to the server. Some are more\nconvenient if you have few files, like RStudio interface, some are more\nbuilt for uploading a lot of files at one, like specific software… and\nyou guessed it the CLI :)\nRStudio\nYou can only upload one file at the time (you can zip a folder to\ntrick it):\n\nsFTP Software\nAn efficient protocol to upload files is FTP (File Transfer\nProtocol). The s stands for secured. Any software\nsupporting those protocols will work to transfer files.\nWe recommend the following free software:\nMac users: cyberduck\nWindows: WinSCP\n\nscp\nThe scp command is another convenient way to transfer a\nsingle file or directory using the CLI. You can run it from Taylor or\nfrom your local computer. Here is the basic syntax:\nscp /source/path hostname:/path/to/destination/\nHere is an example of my uploading the file 10min-loop.R\nto Taylor from my laptop. The destination directory on Taylor is\n/Users/brun/:\nscp 10min-loop.R brun@taylor.bren.ucsb.edu:/Users/brun/\nBTW try to open and run that script for fun!!\nIf you want to upload an entire folder, you can add the\n-r option to the command. The general syntax is:\nscp -r /path/to/source-folder user@server:/path/to/destination-folder/\nHere is an example uploading all the images in the\nmyplot folder\nscp -r myplot brun@taylor.bren.ucsb.edu:/Users/brun/plots\nAdvanced Topics at the\ncommand line\nWe will not cover this during class, it is for your\nreference. You will also have the opportunity to further\npractice and learn about the command line in EDS-215.\nAdvanced topics CLI\nVim\nAknowledgements\nThis section reuses materials from NCEAS Open Science for Synthesis\n(OSS) intensive summer schools and other training. Contributions to\nthis content have been made by Mark Schildhauer, Matt Jones, Jim Regetz\nand many others.\n\n\n\n",
      "last_modified": "2022-08-24T09:08:13-07:00"
    },
    {
      "path": "day2-coding_together.html",
      "title": "Coding together",
      "author": [],
      "contents": "\n\nContents\nManaging\nScripts\nLearning\nObjectives\nWhy collaborative coding\nHow to\ncode together\nTools\n\n\nGitHub as a collaborative\ntool\nQuick recap on version\ncontrol\n\n\nManaging Scripts\nLearning Objectives\nIn this part of the lesson, you will learn:\nWhy git is useful for reproducible analysis\nHow to use git to track changes to your work over\ntime\nHow to use GitHub to collaborate with others\nHow to write effective commit messages\nHow to structure your commits so your changes are clear to\nothers\nHow to fork a repository to contribute to its content\nHow to create a pull request\nHow to review a pull request\nWhy collaborative coding\nSlides\nEnvironmental Data Science (EDS), as many other data-driven research\nfields, requires a transdisciplinary approach to tackle challenges that\noften span across several domains of expertise. Working as a team will\nleverage know-how from diverse collaborators and be the most efficient\nway to tackle complex problems in EDS. Consequently collaborative skills\nare required to work effectively as a member of a team. No matter their\nfocus, highly effective teams share certain characteristics:\nRight size\nDiverse group of people with the right mix of skills, knowledge, and\ncompetencies\nAligned purpose and incentives\nEffective organizational structure\nStrong individual contributions\nSupportive team processes and culture\nSince Analytical Workflows are rarely linear! and\nare developed iteratively, the most efficient way to iterate quickly on\nyour analysis is to use scripts and leave copy-pasting behind.\nProgramming as part of a team is different than writing a script for\nyour(present)self. However learning programming as part of a team\nis not only critical to the efficacy of your team, it will also you help\nyou to grow as a programmer by:\nMotivating you to document well your work\nHelping you to think how to make your work reusable (by you, your\nfuture you and others)\nLearning to read code from collaborators to build upon each others\nwork\nGain further knowledge in software development tools, such as\nversion control\nDeveloping those skills will accelerate your research and\nopen the door for you to contribute to open source\nprojects.\nHow to code together\nIt is important to acknowledge that there are many solutions to the\ncomplex research questions you will be facing in EDS. Each of those\nsolutions will have several possible implementations, meaning that more\nlikely you might code this implementation differently than your\ncollaborators. Integrated software engineer teams generally try to\nmitigate this by developing coding standards and conventions that will\nguide how to write code and develop specific implementation. In\nscientific teams in which the collaboration is more loose and maybe more\nephemeral as well, developing detailed coding standards will be too much\nof an overhead. However, we think it is important to acknowledge that\ncoding style may varies among the data scientists of a project and it is\na good discussion to have among the team at the beginning of the\nproject. For example, in R it could be trying to use the tidyverse\napproach as much as possible. We also think there are two activities\nthat will make the team more efficient: Code Review and Pair\nProgramming.\nTools\nThe good news is there are several tools out there that have been\ndesigned to make developing code as a team more efficient. In this\ncourse, we will focus on getting familiar with the following:\nVersion control system: say goodbye to save as\nCode repository: where we share code and communicate ideas and\nfeedback\n\n\n\nFigure 1:  https://xkcd.com/1597/\n\n\n\nGitHub as a collaborative\ntool\nQuick recap on version\ncontrol\ngit and GitHub quick\nrecap\n\n\n\n",
      "last_modified": "2022-08-24T09:08:13-07:00"
    },
    {
      "path": "day2-git_github_recap.html",
      "title": "git and GitHub recap",
      "author": [],
      "contents": "\n\nContents\nVersion Control with\ngit and GitHub\nThe problem with\nsave_as\ngit\nRepository\n\nGitHub\nLet’s look at a repository\non GitHub\n\n\n\nVersion Control with\ngit and GitHub\nAka – Say goodbye to script_JB_03v5b.R\n!!\nThe problem with\nsave_as\n\n\n\nEvery file in the scientific process changes. Manuscripts are edited.\nFigures get revised. Code gets fixed when problems are discovered. Data\nfiles get combined together, then errors are fixed, and then they are\nsplit and combined again. In the course of a single analysis, one can\nexpect thousands of changes to files. And yet, all we use to track this\nare simplistic filenames. You might think there is a better\nway, and you’d be right: version control.\nVersion control systems help you track all of the changes to your\nfiles, without the spaghetti mess that ensues from simple file renaming.\nIn other words, version control is a system that helps you to manage the\ndifferent versions of your files in an organized manner. It will help\nyou to never have to duplicate files using save as as a way\nto keep different versions of a file (see below). Version control help\nyou to create a timeline of snapshots containing the different versions\nof a file. At any point in time, you will be able to roll back to a\nspecific version. Bonus: you can add a short description (commit\nmessage) to remember what each specific version is about.\nWhat is the difference between git and\nGitHub?\ngit: is a version control software used to track\nfiles in a folder (a repository)\ngit creates a timeline or history of your files\n\nGitHub: is a code repository in the cloud that\nenables users to store their git repositories and share them with\nothers. Github also add many features to manage projects and document\nyour work.\ngit\n\nThis section focuses on the code versioning system called\nGit. Note that there are others, such as\nMercurial or svn for example.\nGit is a free and open source distributed\nversion control system. It has many functionalities and was\noriginally geared towards software development and production\nenvironment. In fact, Git was initially designed and developed in 2005\nby Linux kernel developers (including Linus Torvalds) to track the\ndevelopment of the Linux kernel. Here is a fun video of\nLinus Torvalds touting Git to Google.\nHow does it work?\nGit can be enabled on a specific folder/directory on your file system\nto version files within that directory (including sub-directories). In\ngit (and other version control systems) terms, this “tracked folder” is\ncalled a repository (which formally is a specific data\nstructure storing versioning information).\nWhat git is not:\nGit is not a backup per se\nGit is not good at versioning large files (there are workarounds)\n=> not meant for data\nRepository\nGit can be enabled on a specific folder/directory on your file system\nto version files within that directory (including sub-directories). In\ngit (and other version control systems) terms, this “tracked folder” is\ncalled a repository (which formally is a specific data\nstructure storing versioning information).\nAlthough there many ways to start a new repository, GitHub (or any other cloud solutions,\nsuch as GitLab) provide among\nthe most convenient way of starting a repository.\n\nGitHub\nGitHub is a company that hosts git repositories\nonline and provides several collaboration features (among which\nforking). GitHub fosters a great user community and has\nbuilt a nice web interface to git, also adding great\nvisualization/rendering capacities of your data.\nGitHub.com: https://github.com\nA user account: https://github.com/brunj7\nAn organization account: https://github.com/nceas\nNCEAS GitHub instance: https://github.nceas.ucsb.edu/\nLet’s look at a repository\non GitHub\nThis screen shows the copy of a repository stored on GitHub, with its\nlist of files, when the files and directories were last\nmodified, and some information on who\nmade the most recent changes.\n\nIf we drill into the “commits” for\nthe repository, we can see the history of changes made to all of the\nfiles. Looks like kellijohnson and\nseananderson were fixing things in June and July:\n\nAnd finally, if we drill into the changes made on June 13, we can see\nexactly what was changed in each file:\n Tracking these changes, and seeing\nhow they relate to released versions of software and files is exactly\nwhat Git and GitHub are good for. We will show how they can really be\neffective for tracking versions of scientific code, figures, and\nmanuscripts to accomplish a reproducible workflow.\n\n\n\n",
      "last_modified": "2022-08-24T09:08:14-07:00"
    },
    {
      "path": "day2-git-collaboration-conflicts.html",
      "title": "git conflicts",
      "author": [],
      "contents": "\n\nContents\nLearning\nObjectives\nMerge\nconflicts\nHow to resolve a conflict\nAbort,\nabort, abort…\nCheckout\nPull and edit the file\nProducing and resolving\nmerge conflicts\nMerge Conflict Challenge\n\n\nWorkflows to avoid merge\nconflicts\nYour turn\nAknowledgement\n\nLearning Objectives\nIn this lesson, you will learn:\nWhat typically causes conflicts when collaborating\nWorkflows to avoid conflicts\nHow to resolve a conflict\nMerge conflicts\nSo things can go wrong, which usually starts with a merge\nconflict, due to both collaborators making incompatible changes\nto a file. While the error messages from merge conflicts can be\ndaunting, getting things back to a normal state can be straightforward\nonce you’ve got an idea where the problem lies.\nThis is most easily avoided by good communication about who\nis working on various sections of each file, and trying to avoid\noverlaps. But sometimes it happens, and git is there\nto warn you about potential problems. And git will not allow you to\noverwrite one person’s changes to a file with another’s changes to the\nsame file if they were based on the same version.\n\nThe main problem with merge conflicts is that, when the Owner and\nCollaborator both make changes to the same line of a file, git doesn’t\nknow whose changes take precedence. You have to tell git whose changes\nto use for that line.\nHow to resolve a conflict\nAbort, abort, abort…\nSometimes you just made a mistake. When you get a merge conflict, the\nrepository is placed in a ‘Merging’ state until you resolve it. There’s\na command line command to abort doing the merge altogether:\ngit merge --abort\nOf course, after doing that you still haven’t synced with your\ncollaborator’s changes, so things are still unresolved. But at least\nyour repository is now usable on your local machine.\nCheckout\nThe simplest way to resolve a conflict, given that you know whose\nversion of the file you want to keep, is to use the command line\ngit program to tell git to use either your changes\n(the person doing the merge), or their changes (the other\ncollaborator).\nkeep your collaborators file:\ngit checkout --theirs conflicted_file.Rmd\nkeep your own file:\ngit checkout --ours conflicted_file.Rmd\nOnce you have run that command, then run add,\ncommit, and push the changes as normal.\nPull and edit the\nfile\nBut that requires the command line. If you want to resolve from\nRStudio, or if you want to pick and choose some of your changes and some\nof your collaborator’s, then instead you can manually edit and fix the\nfile. When you pulled the file with a conflict, git notices\nthat there is a conflict and modifies the file to show both your own\nchanges and your collaborator’s changes in the file. It also shows the\nfile in the Git tab with an orange U icon, which indicates\nthat the file is Unmerged, and therefore awaiting you help\nto resolve the conflict. It delimits these blocks with a series of less\nthan and greater than signs, so they are easy to find:\n\nTo resolve the conficts, simply find all of these blocks, and edit\nthem so that the file looks how you want (either pick your lines, your\ncollaborators lines, some combination, or something altogether new), and\nsave. Be sure you removed the delimiter lines that started with\n<<<<<<<, =======, and\n>>>>>>>.\nOnce you have made those changes, you simply add, commit, and push\nthe files to resolve the conflict.\nProducing and resolving\nmerge conflicts\nTo illustrate this process, we’re going to carefully create a merge\nconflict step by step, show how to resolve it, and show how to see the\nresults of the successful merge after it is complete. First, we will\nwalk through the exercise to demonstrate the issues.\nOwner and\ncollaborator ensure all changes are updated\nFirst, start the exercise by ensuring that both the Owner and\nCollaborator have all of the changes synced to their local copies of the\nOwner’s repositoriy in RStudio. This includes doing a\ngit pull to ensure that you have all changes local, and\nmake sure that the Git tab in RStudio doesn’t show any changes needing\nto be committed.\nOwner makes a change and\ncommits\nFrom that clean slate, the Owner first modifies and commits a small\nchange inlcuding their name on a specific line of the README.md file (we\nwill change line 4). Work to only change that one line, and add your\nusername to the line in some form and commit the changes (but DO NOT\npush). We are now in the situation where the owner has unpushed changes\nthat the collaborator can not yet see.\nCollaborator\nmakes a change and commits on the same line\nNow the collaborator also makes changes to the same of the README.md\nfile in their RStudio copy of the project, adding their name to the\nline. They then commit. At this point, both the owner and collaborator\nhave committed changes based on their shared version of the README.md\nfile, but neither has tried to share their changes via GitHub.\nCollaborator pushes the\nfile to GitHub\nSharing starts when the Collaborator pushes their changes to the\nGitHub repo, which updates GitHub to their version of the file. The\nowner is now one revision behind, but doesn’t yet know it.\nOwner pushes their\nchanges and gets an error\nAt this point, the owner tries to push their change to the\nrepository, which triggers an error from GitHub. While the error message\nis long, it basically tells you everything needed (that the owner’s\nrepository doesn’t reflect the changes on GitHub, and that they need to\npull before they can push).\n\nOwner pulls\nfrom GitHub to get Collaborator changes\nDoing what the message says, the Owner pulls the changes from GitHub,\nand gets another, different error message. In this case, it indicates\nthat there is a merge conflict because of the conflicting lines.\n\nIn the Git pane of RStudio, the file is also flagged with an orange\n‘U’, which stands for an unresolved merge conflict.\n\nOwner edits the\nfile to resolve the conflict\nTo resolve the conflict, the Owner now needs to edit the file. Again,\nas indicated above, git has flagged the locations in the file where a\nconflict occcurred with <<<<<<<,\n=======, and >>>>>>>. The\nOwner should edit the file, merging whatever changes are appropriate\nuntil the conflicting lines read how they should, and eliminate all of\nthe marker lines with with <<<<<<<,\n=======, and >>>>>>>.\n\nOf course, for scripts and programs, resolving the changes means more\nthan just merging the text – whoever is doing the merging should make\nsure that the code runs properly and none of the logic of the program\nhas been broken.\n\nOwner commits the resolved\nchanges\nFrom this point forward, things proceed as normal. The owner first\n‘Adds’ the file changes to be made, which changes the orange\nU to a blue M for modified, and then commits\nthe changes locally. The owner now has a resolved version of the file on\ntheir system.\n\nOwner pushes the\nresolved changes to GitHub\nHave the Owner push the changes, and it should replicate the changes\nto GitHub without error.\n\nCollaborator\npulls the resolved changes from GitHub\nFinally, the Collaborator can pull from GitHub to get the changes the\nowner made.\nBoth can view commit history\nWhen either the Collaborator or the Owner view the history, the\nconflict, associated branch, and the merged changes are clearly visible\nin the history.\n\nMerge\nConflict Challenge\nNow it’s your turn. In pairs, intentionally create a merge conflict,\nand then go through the steps needed to resolve the issues and continue\ndeveloping with the merged files. See the sections above for help with\neach of these steps:\nStep 0: Owner and collaborator ensure all changes are updated\nStep 1: Owner makes a change and commits\nStep 2: Collaborator makes a change and commits on the same\nline\nStep 3: Collaborator pushes the file to GitHub\nStep 4: Owner pushes their changes and gets an error\nStep 5: Owner pulls from GitHub to get Collaborator changes\nStep 6: Owner edits the file to resolve the conflict\nStep 7: Owner commits the resolved changes\nStep 8: Owner pushes the resolved changes to GitHub\nStep 9: Collaborator pulls the resolved changes from GitHub\nStep 10: Both can view commit history\n\nWorkflows to avoid merge\nconflicts\nSome basic rules of thumb can avoid the vast majority of merge\nconflicts, saving a lot of time and frustration. These are words our\nteams live by:\nCommunicate often\nTell each other what you are working on\nPull immediately before you commit or push\nCommit often in small chunks.\nA good workflow is encapsulated as follows:\nPull -> Edit -> Add -> Pull -> Commit -> Push\nAlways start your working sessions with a pull to\nget any outstanding changes, then start doing your editing and work.\nStage your changes, but before you commit, Pull again to see if any new\nchanges have arrived. If so, they should merge in easily if you are\nworking in different parts of the program. You can then Commit and\nimmediately Push your changes safely. Good luck, and try to not get\nfrustrated. Once you figure out how to handle merge conflicts, they can\nbe avoided or dispatched when they occur, but it does take a bit of\npractice.\nYour turn\nUse your favorite desserts to create a merge conflict. Resolve it\nusing RStudio\nMixing the fun: try to create a merge conflict using the forking\nworkflow. How would you do this? Try to resolve the conflict on the\nGitHub website! hint\nAknowledgement\nThis section had been adapted from NCEAS\nReproducible Research Techniques for Synthesis\nIndividual units were created by (in alphabetical order): Julien\nBrun, Amber Budden, Jeanette Clark, Matt Jones, Erin McLean, Bryce\nMecum, William Michener.\n\n\n\n",
      "last_modified": "2022-08-24T09:08:15-07:00"
    },
    {
      "path": "day2-github_branches.html",
      "title": "Topic 8: Dropdown list from a navigation bar item",
      "author": [],
      "contents": "\n\nContents\nCollaborating through\nwrite / push access\nBranches\nWorking with branches\nCreating a new branch\nUsing a\nbranch\n\nHands-on\n\n\nCollaborating through\nwrite / push access\nWhen you collaborate closely and actively with colleagues, you do not\nwant necessarily to have to review all their changes through pull\nrequests. You can then give them write access (git push) to\nyour repository to allow them to directly edit and contribute to its\ncontent. This is the workflow we will recommend to use within your\nworking group.\nAdding collaborators to a\nrepository\nClick on the repository\nOn the top tabs, click \nOn the left pane, click Manage access and click on\n“Invite a Collaborator” to enter the usernames you want to add\ncollaboratorsUnder this collaborative workflow, we recommend to use git\nbranches combined with pull requests to avoid conflicts and\nto track and discuss collaborators contributions.\nBranches\nadapted from https://www.atlassian.com/git/tutorials/git-mergeWhat are branches? Well in fact nothing new, as the main\nis a branch. A branch represents an independent line of development,\nparallel to the main (branch).\nWhy should you use branches? For 2 main reasons:\nWe want the main to only keep a version of the code that is\nworking\nWe want to version the code we are developing to add/test new\nfeatures (for now we mostly talk about feature branch) in our script\nwithout altering the version on the main.\nWorking with branches\nCreating a new branch\nIn RStudio, you can create a branch using the git tab.\nClick on the branch button\n\n\n\nFill the branch name in the new branch window; in this example, we\nare going to use test for the name; leave the other options\nas default and click create\n\n\n\nyou will be directly creating a local and remote branch and switch\nto it\n\n\n\nCongratulations you just created your first branch!\nLet us check on Github:\n\n\n\nAs you can see, now there are two branches on our remote repository:\n- main - test\nUsing a branch\nHere there is nothing new. The workflow is exactly the same as we did\nbefore, except our commits will be created on the test\nbranch instead of the main branch.\nHands-on\nGitHub branches using the\nwebsite\nGitHub branches using\nRStudio\n\n\n\n",
      "last_modified": "2022-08-24T09:08:15-07:00"
    },
    {
      "path": "day2-github_commit_messages.html",
      "title": "On good commit messages",
      "author": [],
      "contents": "\n\nContents\nReferences\n\n\n\n\nFigure 1: https://xkcd.com/1296/\n\n\n\nCommit messages are critical for others, including your future self,\nto understand the motivation behind the changes that were implemented.\nCombined into the history of your file and repository, those messages\nhelp to understand the content and its evolution over time\nSo what is a good commit message? Well I think the answer actually\nstarts with what is a good commit? Which actually begins when you are\nchoosing how to group files during the staging.\nA good commit:\nIncorporate changes that have one common\npurpose\nGroup files that need to be kept synchronized or that are working\ntogether towards this common purpose\nA good commit message:\nShould be short (50-72 characters max)\nShould be descriptive (what is the goal of those changes)\nUse imperative mood (to match git default messages)\nNote: you can add a body to a commit message if you want to describe\nthe content in greater details, but keep the first line (message)\nseparated.\nReferences\nHow to Write a Git Commit Message: https://chris.beams.io/posts/git-commit/\n\n\n\n",
      "last_modified": "2022-08-24T09:08:16-07:00"
    },
    {
      "path": "day2-github_forking.html",
      "title": "Collaborating using GitHub forking",
      "author": [],
      "contents": "\n\nContents\nHands-on\n\nA fork is a copy of a repository\nthat will be stored under your user account. Forking a repository allows\nyou to freely experiment with changes without affecting the original\nproject. We can create a fork on Github by clicking the “fork” button in\nthe top right corner of our repository web page. \nMost commonly, forks are used to either propose changes to someone\nelse’s project or to use someone else’s project as a starting point for\nyour own idea.\nWhen you are satisfied with your work, you can initiate a\nPull Request to initiate discussion about your\nmodifications and requesting to integrate your changes to the main\nrepository. Your commit history allows the original repository\nadministrators to see exactly what changes would be merged if they\naccept your request. Do this by going to the original repository and\nclicking the “New pull request” button!\n\n\n\nNext, click “compare across forks”, and use the dropdown menus to\nselect your fork as the “head fork” and the original repository as the\n“base fork”.\n\nThen type a title and description for the changes you would like to\nmake. By using GitHub’s @mention syntax in your Pull\nRequest message, you can ask for feedback from specific people or\nteams.\nThis workflow is recommended when you do not have push/write access\nto a repository, such as contributing to a open source software or R\npackage, or if you are heavily changing a project.\nHands-on\nUpdate the greetings R package to add the option to\nchange the color of the background of the text.\nRepo: https://github.com/brunj7/greetings\nBy team of two:\nTeam member 1: fork the repository\nTeam member 2: fork the fork that team member 1 just created\n(yes, you can do this!!)\nWork together on Team member 2’s computer to modify the\nsay_aloah() function to allow changing the text background\ncolor to blue in addition to the existing green (see here for\nmore about the crayon package)\nOnce done, create a pull request to integrate the new changes\ntagging team member 1 as reviewer\nTeam member 1, review the code, comment as needed on the pull\nrequest, and merge changes\n\n\n\n",
      "last_modified": "2022-08-24T09:08:17-07:00"
    },
    {
      "path": "day2-handson_github_rstudio.html",
      "title": "GitHub branches using RStudio",
      "author": [],
      "contents": "\n\nContents\nIs your\nfav’ iconic !?!\nNote\n\nIs your fav’ iconic !?!\nWrite and R script to compare the csv with your favorite desserts you\nmodified previously with the top 42 most iconic American desserts\nClone the repository you just created to Taylor\nCreate a new branch using RStudio and name it\nmyinitials_rstudio\nStart a new script named\ndesserts_match_myinitials.R\nAs pair programming, write a script should read a csv file of your\nfavorite dessert (create it if you do not have one yet) and the csv file\nwith the most iconic desserts (see link below) into R and try to\ndetermine if your favorite dessert is part of the most\niconic desserts!\nYou can download the iconic data here: https://github.com/brunj7/EDS-214-analytical-workflows/blob/main/data/iconic_desserts.csv?raw=TRUE\nNote\nHere is how we created the iconic listing\n\n\nlibrary(tidyverse)\nlibrary(rvest)\n\n# read the webpage code\nwebpage <- read_html(\"https://www.eatthis.com/iconic-desserts-united-states/\")\n\n# Extract the desserts listing\ndessert_elements<- html_elements(webpage, \"h2\")\ndessert_listing <- dessert_elements %>% \n  html_text2() %>% # extracting the text associated with this type of elements of the webpage\n  as_tibble() %>% # make it a data frame\n  rename(dessert = value) %>% # better name for the column\n  head(.,-3) %>% # 3 last ones were not desserts \n  rowid_to_column(\"rank\") %>% # adding a column using the row number as proxy for the rank\n  write_csv(\"data/iconic_desserts.csv\") # save it as csv\n\n\n\n\n\n",
      "last_modified": "2022-08-24T09:08:20-07:00"
    },
    {
      "path": "day2-handson_github_website.html",
      "title": "GitHub branches and more using GitHub website",
      "author": [],
      "contents": "\n\nContents\nOur asks\nPrompt\n\nIn this section we will be using the GitHub.com website and show you\ndo not need to be a programmer to use version control and edit files on\nGitHub, nor using branches\nChecks: If you have not already created a GitHub username, please do\nso now: - GitHub: https://github.com - Follow optional advice on choosing\nyour username\nOur asks\nAs a Team of two: - Help each other, everyone is bringing different\nskills! Talk it out! - Listen to each other; avoid judgment and\nsolutioneering. - Have fun!\nPrompt\nWe want to log the information about people favorite dessert using a\nrepository.\nPerson 1 (owner):\nCreate a repository using the following these instructions\nsteps 1-6\n\nAdd Person 2 as collaborator following these instructions\nEdit the README to:\nReplace the title (first line starting with #) with\nsomething better! Maybe Favorite Desserts\nAdd your name and your favorite dessert below the title:\ne.g. - Julien: crepes\n\n\n\n\nPerson 2: Create a branch\nCheck your email to accept the invitation\nGo to the repository website (link is provided in the\ninvitation)\nCreate a new branch named after your first name\n\n\n\nStart editing the README.md by clicking on pen at the top of the\nREADME.md file to edit it\n\n\n\nAdd your name and your favorite dessert below the title:\ne.g. - Sophia: chocolate\nAdd a descriptive message\n\n\n\nCommit your changes\n\n\n\nCreate a pull request to merge those changes to the main\nrepository\nPerson 1: add a file\nDownload this csv\nfile about your favorite desserts to your computer\nJust drag and drop it on the Github web page of your repository to\nupload it\nAdd a short message about the file\ne.g. Adding dessert csv & hit\nCommit changes\nYour have has been uploaded. Click on the filename to see it!\nYou should have something similar to this repo: https://github.com/brunj7/favorite-desserts\nBonus\nPerson 2: Try to edit the csv file directly on GitHub!\nPerson 1: Your turn!\nNo need to be a programmer to contribute to analytical\nworkflows with GitHub!!\n\n\n\n",
      "last_modified": "2022-08-24T09:08:21-07:00"
    },
    {
      "path": "day2-projects_team.html",
      "title": "Managing data-driven projects as a team",
      "author": [],
      "contents": "\n\nContents\nManaging your files as a\nteam\nNaming things\nExercise\n\nOrganizing\nthings\nCode\nScripting languages\nStructure of a script\nA few\nprogramming practices that will help a long way\nNotebooks and scripts\n\nData\nLeveraging existing framework\nAn example:\nrrtols\nOther workflow\nframeworks to look into\n\nAknowledgement\n\nManaging your files as a\nteam\nOur goal is to centralize the management of your files (data, codes,\n…). Try to avoid having data sets spread among laptops or other personal\ncomputers; this makes it difficult for other team members to redo a\nparticular analysis and it can become difficult to know which version of\nthe data was used for a specific analysis. We recommend asking your\ninstitution if there are servers or cloud services available to you and\nuse those tools to centralize your data management. This will also make\nsure that all your collaborators will be able to access the same version\nof the data using the same path.\nThere are many tools out there to help with file management. Here are\na few questions to ask your teammates when organizing your project:\nCan everybody have access to this tool? This should overrule the\n“best” tool => maximize adoption\nWhat team practices should you set on how to use these tools?\nExample: naming convention for files\nAllow flexibility – acknowledge the technological level varies among\ncollaborators. Empower them by showing how to best use these tools\nrather than doing it for them!\nNaming things\n\nDevelop naming conventions for files and folder:\nAvoid spaces (use underscores or dashes)\nAvoid punctuation or special characters\nTry to leverage alphabetical order (e.g. start with dates:\n2020-05-08)\nUse descriptive naming (lite metadata)\nUse folders to structure/organize content\nKeep it simple\nMake it programmatically useful:\nUseful to select files (Wildcard *, regular expression)\nBut don’t forget Humans need to read file names too!!\n\nExercise\nWhy do you think the second option would be best?\n\n\nx <- 9.81  #  gravitational acceleration\n\ngravity_acc <- 9.81  #  gravitational acceleration\n\n\nWhich filename would be the most useful?\n06-2020-08-sensor2-plot1.csv\n2020-05-08_light-sensor-1_plot-1.csv\nMeasurement 1.csv\n2020-05-08-light-sensor-1-plot-2.csv\n2020-05-08-windSensor1-plot3.csv\nThe most important is to make it consistent!\nGood reference\non this topic from Jenny Bryan (RStudio).\nOrganizing things\nAs we discussed previously, it is good practice to encapsulate your\nproject and repositories are a good unit to start with. In this section\nwe will talk about the case when your data sets start to be numerous or\nlarge enough that it is not possible anymore to keep them in your\nrepository. Generally when you reach this amount of data to deal with,\nit also means that your personal computer might not be the best computer\nto efficiently process those data sets. Thus using a remote server (on\npremise or in the cloud) might become necessary.\nCodes should be managed using version controlled (git and GitHub\nfor this course)   The repository should be stored in your home folder\nand the management of the different contributions should be resolved\nusing git and GitHub and not by sharing directly the scripts. The main\nreason is that git does already the work for us by tracking the changes\nbetween the various versions, but also tracking which collaborator has\nmade those changes and when.\nData sets should should be centralized in a shared folder that is\navailable to all   It is very rare in an Environmental Data Science\nproject that you will discover all the data you need from the start. In\naddition, a lot of environmental data sets are time-series, and one year\nlater might need to be updated as your project progresses. Our goal here\nis to setup ourselves in a way that will help us to avoid duplication of\ndata. Every time you duplicate a data set (for example on your own\nlaptop), there is a risk that at some point it will become its own\nversion.\nCode\nVersion control systems have been originally designed to track\nchanges by rows in small text files, in other words they are well suited\nto manage codes. It is thus recommended to use them to track changes\nthat you and your collaborators are making to the various scripts of\nyour project. Mote on how to best do this in the section below.\nScripting languages\nCompared to other programming languages (such as C,\nfortran, …), scripting languages are not required to be\ncompiled to be executable. One consequence is that, generally, scripts\nwill execute more slowly than a compiled executable program, because\nthey need an interpreter. However, the more natural language oriented\nsyntax of scripts make them easier to learn and use. In addition,\nnumerous libraries are available to streamline scientific analysis.\nStructure of a script\nA script can be divided into several main sections. Each scripting\nlanguage has its own syntax and style, but these main components are\ngenerally accepted:\nFrom the top to the bottom of your script:\nSummary explaining the purpose of the script\nAttribution: authors, contributors, date of last update, contact\ninfo\nImport of external modules / packages\nConstant definitions (g = 9.81)\nFunction definitions (ideally respecting the order in which they are\ncalled)\nMain code calling the different functions\nA few\nprogramming practices that will help a long way\nComment your code. This will allow you to inform your\ncollaborators (but also your future self!) about the tasks your script\naccomplishes\nUse variables and constants instead of repeating values in\ndifferent places of the code. This will let you update those values more\neasily\nChoose descriptive names for your variables and functions,\nnot generic ones. If you store a list of files, do not use\nx for the variable name, use instead files.\nEven better use input_files if you are listing the files\nyou are importing.\nBe consistent in terms of style (input_files,\ninputFiles,…) used to name variables and functions. Just\npick one and stick to it!\nkeep it simple, stupid (KISS). Do not\ncreate overly complicated or nested statements. Break your tasks in\nseveral simple lines of code instead of embedding a lot of executions in\none (complicated line). It will save you time while debugging and make\nyour code more readable to others\nGo modular! Break down tasks into small code fragments such\nas functions or code chunks. It will make your code reusable for you and\nothers (if well documented). Keep functions simple; they should only\nimplement one or few (related) tasks\nDon’t Repeat Yourself (DRY).\nIf you start copy/pasting part of your code changing a few parameters\n=> write a function and call it several times with different\nparameters. Add flow control such as loops and conditions. It will be\neasier to debug, change and maintain\nTest your code. Test your code against values you would\nexpect or computed with another software. Try hedge cases, such as NA,\nnegative values, ….\nIterate with small steps, implement few changes at a time\nto your code. Test, fix, and move forward!\nWe hope this overview section about scientific programming has raised\nyour interest in learning more about best practices and tools for\ndeveloping reproducible workflows using scripting languages.\nNotebooks and scripts\nWith the increasing popularity of notebooks in scientific projects,\nit can sometimes be confusing to know when to use one. The good news is\nthat there is not really a wrong or right here and that actually both\ncan be used in a complementary manner and each data scientist will have\nher/his preference. However here are a few tips to help you decide:\nKeep your Notebook at a length you will feel comfortable reading. It\nif starts to be a long it might be time to think about if some code\ncould be move to another document (scripts or another notebook)\nScripts might be better suited for tasks you need to rerun\nfrequently\nIf you have developed many functions for your analysis, it might be\nworth storing them in a script outside your main notebook\nData\nIt is recommended to keep the raw-data you are collecting\nseparated from any data you might generate at various steps of your\nworkflow. This will help you to trace back any problems, but\nalso make your work more reproducible because you started your\nprocessing directly from the original data. There are different ways of\nensuring this. A common one is to create a raw-data\n(sub)folder to store the data. You can even play with the file access\nsettings to make those files read-only.\nhttps://github.com/benmarwick/rrtoolsLeveraging existing\nframework\nPlease remember that every workflow is opinionated. This is also true\nfor the tools out there that can help you to develop reproducible\nworkflows. It is great practice to test them on a small project to see\nif hey could fit your project needs and way of working and\ncollaborating. In this section we will go over a few R packages that\ncould be of interest for your project.\nAn example: rrtols\nReproducible Papers\nA great overview of this approach to reproducible papers comes\nfrom:\n\nBen Marwick, Carl Boettiger & Lincoln Mullen (2018)\nPackaging Data Analytical Work Reproducibly Using R (and\nFriends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986\n\nThis lesson will draw from existing materials:\nrrtools\nReproducible\npapers with RMarkdown\nThe key idea in Marwick et al. (2018) is that of the “research\ncompendium”: A single container for not just the journal article\nassociated with your research but also the underlying analysis, data,\nand even the required software environment required to reproduce your\nwork. Research compendia make it easy for researchers to do their work\nbut also for others to inspect or even reproduce the work because all\nnecessary materials are readily at hand due to being kept in one place.\nRather than a constrained set of rules, the research compendium is a\nscaffold upon which to conduct reproducible research using open science\ntools such as:\nR\nRMarkdown\ngit and GitHub\nFortunately for us, Ben Marwick (and others) have written an R\npackage called rrtools that helps us\ncreate a research compendium from scratch.\nTo start a reproducible paper with rrtools, run:\n\n\nremotes::install_github(\"benmarwick/rrtools\")\nrrtools::use_compendium(\"mypaper\")\n\n\nYou should see output similar to the below:\n> rrtools::use_compendium(\"mypaper\")\nThe directory mypaper has been created.\n✓ Setting active project to '/Users/bryce/mypaper'\n✓ Creating 'R/'\n✓ Writing 'DESCRIPTION'\nPackage: mypaper\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R (parsed):\n    * First Last <first.last@example.com> [aut, cre]\nDescription: What the package does (one paragraph).\nLicense: MIT + file LICENSE\nByteCompile: true\nEncoding: UTF-8\nLazyData: true\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.1.1\n✓ Writing 'NAMESPACE'\n✓ Writing 'mypaper.Rproj'\n✓ Adding '.Rproj.user' to '.gitignore'\n✓ Adding '^mypaper\\\\.Rproj$', '^\\\\.Rproj\\\\.user$' to '.Rbuildignore'\n✓ Setting active project to '<no active project>'\n✓ The package mypaper has been created\n✓ Now opening the new compendium...\n✓ Done. The working directory is currently /Users/bryce\n\nNext, you need to:  ↓ ↓ ↓ \n● Edit the DESCRIPTION file\n● Use other 'rrtools' functions to add components to the compendium\nrrtools has created the beginnings of a research\ncompendium for us. At this point, it looks mostly the same as an R\npackage. That’s because it uses the same underlying folder structure and\nmetadata and therefore it technically is an R package. And this means\nour research compendium will be easy to install, just like an R\npackage.\nBefore we get to writing our reproducible paper, let’s fill in some\nmore structure. Let’s:\nAdd a license (always a good idea)\nSet up a README file in the RMarkdown format\nCreate an analysis folder to hold our reproducible\npaper\n\n\nusethis::use_apl2_license() # Change this\nrrtools::use_readme_rmd()\nrrtools::use_analysis()\n\n\nAt this point, we’re ready to start writing the paper. To follow the\nstructure rrtools has put in place for us, here are some\npointers:\nEdit ./analysis/paper/paper.Rmd to begin writing your\npaper and your analysis in the same document\nAdd any citations to\n./analysis/paper/references.bib\nAdd any longer R scripts that don’t fit in your paper in an\nR folder at the top level\nAdd raw data to ./data/raw_data\nWrite out any derived data (generated in paper.Rmd) to\n./data/derived_data\nWrite out any figures in ./analysis/figures\nIt would also be a good idea to initialize this folder as a git repo\nfor maximum reproducibility:\n\n\nusethis::use_git()\n\n\nAfter that, push a copy up to GitHub.\nHopefully, now that you’ve created a research compendium with\nrrtools, you can imagine how a pre-defined structure like\nthe one rrtools creates might help you organize your\nreproducible research and also make it easier for others to understand\nyour work.\nFor a more complete example than the one we built above, take a look\nat benmarwick/teaching-replication-in-archaeology.\nOther workflow\nframeworks to look into\nTarget\nThe target R package can be very useful if you have a\ncomplex workflow that is built of many parts that take time to rerun.\nTarget can detect and run only the strict necessary steps to rerun when\na specific change has been done to the workflow.\nHere for more: https://books.ropensci.org/targets/\nPointblank\nAlthough the pointblank package is mainly framed as a\nvalidating tool of the various part of your workflow, it provide a set\nof tools and great integration with the R Markdown ecosystem. It is also\nquite flexible making it possible to leverage this tool in a variety or\nproject setups.\nhttps://rich-iannone.github.io/pointblank/\nAknowledgement\nThe rrtools example had been ported from NCEAS\nReproducible Research Techniques for Synthesis\n\n\n\n",
      "last_modified": "2022-08-24T09:08:22-07:00"
    },
    {
      "path": "day3-apis.html",
      "title": "Using APIs to programatically retrieve data",
      "author": [],
      "contents": "\n\nContents\nGetting data\nThe manual\nway\nThe\nprogrammatic way\n\nHands-on\nUSGS\ndataRetrieval R package to retrieve hydrological data\nExercise 1\nBonus\n\nmetajam\nExercise 2\nBonus\n\n\n\nGetting data\nThe manual way\nUsing website to retrieve data is great interactive way to search and\nexplore data of interest. Let’s start our day by searching some data on\nthe data repository DateONE: https://search.dataone.org/data\nIn case you need some inspiration, look at this dataset about\nhistoric precipitation in Alaska: https://doi.org/10.5063/N29VCQ\n\nThe programmatic way\nAlthough discovering data through a web interface is convenient and\noffers a great experience, it is often hard to scale this approach or to\nintegrate it into a reproducible workflow.\nSlides\nHands-on\nUSGS\ndataRetrieval R package to retrieve hydrological data\nUSGS is managing a vast network\nof gauges to monitor freshwater across the US.\nExercise 1\nCreate a new repository on GitHub named\napi-practice\nClone is to Taylor\nStart a new Markdown document to plot the discharge time-series\nfor the Ventura River from 2019-10-01 to 2020-10-05\nWebpage: https://waterdata.usgs.gov/nwis/uv?site_no=11118500\nTutorial for the package can be found here: https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html#daily-data\nBonus\nHow would you try to determine when this stream gauge record started\nusing the API?\nmetajam\nThe metajam R package relies on the dataONE API to\ndownload data and metadata into your R Environment. It\nis currently supporting KNB, ADC and EDI repositories because they rely\non the metadata standard EML.\nShort intro\nto the package\nExercise 2\nLet’s determine what percentage of Alaskan household are speaking\nonly English!\nThe data: https://doi.org/10.5063/F1N58JPP\nRead the metadata\nDownload the data household_language.csv using\nmetajam\nRead the data into R using metajam\nWrite a piece of code that will compute the percentage of Alaskan\nhousehold speaking only English for the year 2009 to 2015\nCreate a plot to visualize this data\nBonus\nHow does it compare to French?\n\n\n\n",
      "last_modified": "2022-08-24T09:40:09-07:00"
    },
    {
      "path": "day4-documenting.html",
      "title": "Documenting things",
      "author": [],
      "contents": "\n\nContents\nThe\npower of README\nNeed\nsome inspiration ?\n\nMaking your code readable\nComments\nHeader\nInline\nFunctions\n\nLeveraging Notebooks\nHands-on\nDocumenting\nCommenting\n\nMetadata\nData provenance &\nsemantics\nLicensing\nFurther\nReading\nAcknowledgements\n\nThe power of README\nREADME files are not a new thing. They have been around computer\nprojects since the early days. One great thing about the popularization\nof supporting the markdown syntax and its web rendering in most code\nrepositories, is that you can move beyond a simple text file and start\nto present a compelling entry point to your project that can link to\nvarious parts and external resources.\nGood types of information to have on a README:\nTitle capturing the essence of the project\nA short explanation of the goal / purpose\nHow to install / where to start\nA quick demo on how to use the content (can be a link to another\ndocument as well)\nWhat to do if a bug is spotted\nHow to contribute\nLicensing\nAcknowledgements of authors, contributors, sponsors or other related\nwork\nAdding images, short videos / animations can make a README more\nengaging.\nNeed some inspiration ?\nHere is a interesting template: https://github.com/navendu-pottekkat/awesome-readme/tree/master\nWhen you start an R package with the usethis\npackage, a README will be created for you with all the relevant sections\nfor such type of project.\npick a package you like and inspect their README\nMaking your code readable\nIt is important to make your code easy to read if you hope that\nothers will reuse it. It starts with using a consistent\nstyle withing your scripts (at least within a project).\nHere is a good style guide for R: http://adv-r.had.co.nz/Style.html\nStyle guide for Python: https://www.python.org/dev/peps/pep-0008/\n\nimport this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\nThere is also the visual aspect of the code that\nshould not be neglected. Like a prose, if you receive a long text\nwithout any paragraphs, you might be not very excited about reading it.\nIndentation, spaces, and empty lines should\nbe leveraged to make a script visually inviting and easy to read. The\ngood news is that most of the Integrated Development Environment (IDE)\nwill help you to do so by auto formatting your scripts according to\nconventions. Note that also a lot of IDEs, such as RStudio, rely on some\nconventions to ease the navigation of scripts and notebooks. For\nexample, try to add four - or # after a\nline starting with one or several # in an R\nScript!\nComments\nReal Programmers don’t comment their code. If it was hard\nto write, it should be hard to understand.\nTom Van Vleck, based on people he knew_ (https://multicians.org/thvv/realprogs.html)\nJoke aside, it is really hard to comment too much your code, because\neven steps that might seem trivial today might not be so anymore in a\nfew weeks or months for now. In addition, a well commented code is more\nlikely to be read by others. Note also that comments should work\nin complement of the code and should not being seen as work\naround vague naming conventions of variables or functions.\n\n\nx <- 9.81  #  gravitational acceleration\n\ngravity_acc <- 9.81  #  gravitational acceleration\n\n\nHeader\nIt is good to add a header to your script that will provide basic\ninformation such as:\nPurpose of the script (Long title style)\nWho are the authors\nA contact email\nOptional:\nA longer description about the script purpose\nA starting date and potentially last updated one, although this\ninformation becomes redundant with repository information\nNote that R Studio does something similar by default when creating an\nnew R Markdown document!\nInline\nIt does not matter if you are using a script or notebook. It is\nimportant to provide comments along your code to complement it by:\nexplaining what the code does\ncapturing decisions that were made on the analytical side. For\nexample, why a specific value was used for a threshold.\nspecifying when some code was added to handle an edge case such as\nan unexpected value in the data (so a new user doesn’t have to guess\nwhat does lines of code and might want to delete them assuming it is not\nnecessary)\nOther thoughts:\nIt is OK to state (what seems) the obvious (some might disagree with\nthis statement)\nTry to keep comments to the point and short\nFunctions\nBoth Python and R have conventions on how to document functions.\nAdopting those conventions will help you to make your code readable but\nalso to automate part of the documentation development.\nRoxygen2\nThe goal of roxygen2 is to make documenting your code as easy as\npossible. It can dynamically inspect the objects that it’s documenting,\nso it can automatically add data that you’d otherwise have to write by\nhand.\nHow do we insert it? Make sure you cursor is inside the function you\nwant to document and from RStudio Menu Code -> Insert Roxygen\nSkeleton\nExample:\n\n\n#' Add together two numbers\n#'\n#' @param x A number\n#' @param y A number\n#' @return The sum of \\code{x} and \\code{y}\n#' @examples\n#' add(1, 1)\n#' add(10, 1)\nadd2 <- function(x, y) {\n  x + y\n}\n\n\nTry it! - Copy the function (without the documentation) in a new\nscript - Add a third parameter to the function such as it sums 3 numbers\n- Add the Roxygen skeleton - Fill it to best describe your function\nNote that when you are developing an R package, the Roxygen skeleton\ncan be leveraged to develop the help pages of your package so you only\nhave one place to update and the help will synchronize\nautomatically.\nPython Docstring\nA docstring is a string literal that occurs as the first statement in\na module, function, class, or method definition. Such a docstring\nbecomes the __doc__ special attribute of that object.\n\ndef complex(real=0.0, imag=0.0):\n    \"\"\"Form a complex number.\n\n    Keyword arguments:\n    real -- the real part (default 0.0)\n    imag -- the imaginary part (default 0.0)\n    \"\"\"\n    if imag == 0.0 and real == 0.0:\n        return complex_zero\n\nHere for more: https://www.python.org/dev/peps/pep-0257/\nLeveraging Notebooks\nAs we have discussed and experimented with Notebooks during the week.\nIt is because Notebooks provide space to further develop content, such\nas methodology, around the code you are developing in your analysis.\nNotebooks also enable you to integrate the outputs of your scientific\nresearch with the code that was used to produce it. Finally, notebooks\ncan be rendered into various format that let them share with a broad\naudience.\nNotebooks are not only used within the scientific community, see here for some thoughts\nfrom Airbnb data science team.\nHands-on\nDocumenting\n\n\ngetPercent <- function( value, pct ) {\n    result <- value * ( pct / 100 )\n    return( result )\n}\n\n\nTry adding the Roxygen Skeleton to this function and fill all the\ninformation you think is necessary to document the function\nCommenting\nLet’s try to improve the readability and documentation of this\nrepository: https://github.com/brunj7/better-comments. Follow the\ninstructions on the README\nFor inspiration, you can check out the NASA code for APOLLO 11 dating\nfrom 1969: https://github.com/chrislgarry/Apollo-11!!\nMetadata\nThis topic will be the focus of our Fall course EDS-213.\nThis a very important topic for scientific reproducibility and for today\nwe will be only provide a partial overview of this broader topic.\n\n\n\nFigure 1: Data life cycle, DataONE\n\n\n\nMetadata (data about data) is an important part of the data life\ncycle because it enables data reuse long after the original collection.\nThe goal is to have enough information for the researcher to understand\nthe data, interpret the data, and then re-use the data in another\nstudy.\nHere are good questions to answer with your metadata:\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding\nagency)?\nHow can this data be reused (licensing)?\nHow do you organize all this information? You could use a free form\nformat, like a README file or spreadsheet. But there is also great\nadvantage to use a more standardized way that will make the content not\nonly Human readable but also machine readable. This\nwill enhance the data discovery as specific information will be\npotentially tagged or attributed to specific aspect of your data\n(e.g. spatial or temporal coverage, taxonomy, …).\nThere are a number of environmental metadata\nstandards (think, templates) that you could use, including the\nEcological Metadata Language\n(EML), Geospatial Metadata\nStandards like ISO 19115 and ISO 19139, the Biological\nData Profile (BDP), Dublin\nCore, Darwin Core, PREMIS, the Metadata Encoding and\nTransmission Standard (METS), and the list goes on and on.\nData provenance & semantics\nData provenance is tracing the origin of a data set\nto the raw-data that were used as input of the processing / analysis\nthat led to the creation of this data set. It can be done more or less\nformally and this is an active area of research. For today we will be\nfocusing on capturing the information about the data you are collecting.\nHere are a set of good questions to help you in that process:\nSource / owner (Person, institution, website, ….)\nWhen was it acquired ?\nBy whom on the WG ?\nWhere is it currently located (Google drive, server, ….) ?\nShort description of the data\nTrack if it is used in your analysis\nHere\nis a template of a data log that could hep to store this information\nAnother important and related aspect and also active field of\nresearch is data semantics. Often data sets store\ncomplex information and concepts that can be describe more of less\naccurately. Let’s take an example, you have receive a csv file storing a\ntable with several variables about a fish stock assessment. One of the\nvariable is named “length”. However there are many ways to measure the\nlength of a fish. Which one is it?\n\n\n\nData semantics aims at clearly identify those concepts relying on\nvocabularies and ontologies, such as ENVO in\nenvironmental sciences. In addition, it enables the leverage relations\nbetween those concepts to help with (data) discovery.\nLicensing\nIt is a good practice to add a license to a repository / project. It\nwill help to clarify what are the expectations regarding using and\npotentially contributing to this work.\nHere is a good website to choose a license:\nHere is also good set of instructions on how to make this happen on a\nGitHub repository: https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/licensing-a-repository\nNote that for content (such as this course), there is also another\ntype of licensing that can be used: https://creativecommons.org/licenses/\nFurther Reading\nAwesome README: https://towardsdatascience.com/how-to-write-an-awesome-readme-68bf4be91f8b\nHow to write a great README: https://x-team.com/blog/how-to-write-a-great-readme/\nPython Hichhiker’s guide:\nstyle: https://docs.python-guide.org/writing/style/\ndocumentation: https://docs.python-guide.org/writing/documentation/\n\nDocumenting Python Code: A Complete Guide: https://realpython.com/documenting-python-code/\nRoxygen2: https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html\nIntroduction to Software Engineering by Jason Coposky: https://github.com/NCEAS/training/blob/master/2014-oss/day-09/IntroductionToSoftwareClass.pdf\nFunctional programming in R: http://adv-r.had.co.nz/Functional-programming.html#functional-programming\nScoping in R: http://adv-r.had.co.nz/Functions.html\nhttps://www.quora.com/What-is-the-difference-between-programming-languages-markup-languages-and-scripting-languages\nhttp://stackoverflow.com/questions/17253545/scripting-language-vs-programming-language\nSoftware carpentry: https://swcarpentry.github.io/r-novice-inflammation/06-best-practices-R/\nAcknowledgements\nLarge portion of this material have been adapted from NCEAS\nReproducible Research Techniques for Synthesis course https://learning.nceas.ucsb.edu/2021-02-RRCourse/\n\n\n\n",
      "last_modified": "2022-08-24T09:08:26-07:00"
    },
    {
      "path": "day4-pair_programming.html",
      "title": "Pair Programming",
      "author": [],
      "contents": "\n\nContents\nBasic principles &\npractices\nTips and Tricks\nfor Effective Team Programming\nAknowledgements\n\nPair programming is an synchronous team activity, where\nseveral programmers get to work together on the same piece of code. This\nis a great way to gain a better sense of what coding style people are\nusing and better understand their way of solving challenges. It is also\na great way to learn from each other. Generally, there is one Driver who\nis the person typing at the computer. The other role is called\nNavigator(s). The Navigator does not write code and focuses on finding\nsolutions to the problem. Their use of computer should be limited to\nsearching online for solutions.\nBasic principles &\npractices\nAdapted from Woody Zuill https://www.agileconnection.com/article/getting-started-mob-programming\nTreat each other with kindness, consideration, and\nrespect - makes group work more fun and sustainable\nDriver/navigator pair programming adapted to work with the\nwhole team - “For an idea to go from your head into the\ncomputer, it must go through someone else’s hands.” Speak at\nthe highest level of abstraction that the driver (and the rest of the\nteam) is able to digest at the moment\nTimed Rotation - 20-60 minutes. We don’t require\nthat everyone take the driver role; it is everyone’s choice whether to\ndo so\nWhole Team - every contributor to the project is an\nintegral part of the whole team; when we don’t have the skills we need\nwithin the team, we find someone who does and invite them to work with\nus to accomplish the needed work\nReflect, Tune, and Adjust Frequently - based on\nagile principle: “At regular intervals, the team reflects on how to\nbecome more effective, then tunes and adjusts its behavior\naccordingly.”\nTips and Tricks\nfor Effective Team Programming\nAdapted from Corey Johannsen: https://blog.newrelic.com/2017/10/31/mob-programming-hurdles/\nSuggest, don’t dictate: Instead of telling the\ndriver what to type into their editor, we explain what we’re trying to\naccomplish and then help the driver find the best solution. We’ve found\nthat drivers learn better this way, and they don’t just end up feeling\nlike a stenographer. Whenever possible, we ask questions that lead the\ndriver to discover the answers on their own.\nStay focused and be present: Shut your laptop and\nput your phone away. I’ve struggled with following this guideline—we all\nhave—and I recognize that the distraction almost always affects the rest\nof the mob. We tell all our mob members to be present, and if you can’t,\nit’s OK to leave until you can be.\nUse a timer, but be ready to pause it: We switch\ndrivers every 20 - 60 minutes. However, we often wander off\nimplementation into design discussions—it’s unavoidable—so this is when\nwe pause the timer. This is another key guideline of our mob: the time\nyou spend driving should be dedicated to writing the code that helps\ncomplete the task, not discussing design solutions.\nSet specific tasks for each session: When our mob\ngathers for a session, we first agree on and create a checklist of the\ntasks we are going to complete, and order them by priority on a\nwhiteboard. This ensures we are all focused on the same task and keeps\nus moving forward. Additionally, this keeps us aligned with Minimal\nMarketable Feature (MMF) work, which we can communicate with our\nengineering and product managers to assure them we’re completing tasks\nthat align with developing small, self-contained features that\ndemonstrate immediate customer value.\nAknowledgements\nThis section reuses a lot of materials from an R Meetup organized by\nthe Santa Barbara R Users group (https://github.com/R-Meetup-SB/hackathon-201806),\nincluding material prepared by Irene Steves.\n\n\n\n",
      "last_modified": "2022-08-24T09:08:26-07:00"
    },
    {
      "path": "day5-sharing_things.html",
      "title": "Sharing Things",
      "author": [],
      "contents": "\n\nContents\nGitHub pages & the R\nMarkdown family\nInteractive web applications\nR Shiny\n\nHtml widgets\n\nplotly\n\nCode\nRepositories\nBinder &\njupyter\n\nCiting your\ncode\n\nData\nRepositories\nYou as an\nAuthor\n\nWhat about your computing\nenvironment\nSession info\nContainers\n\nXaringan\n\nGitHub pages & the R\nMarkdown family\nYou have been using it all week!! and before!! All the course\nmaterial has been developed as an R Markdown based website, distill\nwebsite to be precise (see here for\nmore), and it is also a great way to publish and share your note book\nwith your team and the broader community. Also checkout Quarto that is a cross-language tool to\nrender documents from code!\nInteractive web applications\nWhen you have complex data that you want to not only visualize but\nalso to let the user interact with your data or customize parameters\nused in an analysis, interactive web applications are a great way to\nincrease engagement. The good news is that you do not need to be a web\ndeveloper anymore to spin such applications or dashboards.\nR Shiny\n\nR Shiny is a very interesting framework that lets you write R code\nthat will then be translated into javascript for you and thus let you\ndevelop web application without having to learn any new programming\nlanguage. Note that you will need a server to host the application.\nCheck out this gallery of shiny apps: https://shiny.rstudio.com/gallery/\nGetting started with Shiny: https://shiny.rstudio.com/tutorial/\nHtml widgets\nHtml widgets offer some great interactive data visualization. It is\nmore limited that Shiny because you can not modify parameters to modify\nthe data use, but it has the advantage that it does not need a server to\nrun the widget and it can be inserted directly into an R Markdown.\nHere to get started: https://www.htmlwidgets.org/\nplotly\n\nplotly enable you to\ndevelop great interactive data visualizations. It has the advantage to\nbe available both in R and Python (and javascript). One great\nthing in R is that if you are a ggplot master, you can write your plot\ncode using ggplot and transform it into a plotly plot with one line of\ncode (here for\nan example)\nNote there are also other python libraries to create interactive\nplots, here are a few: https://mode.com/blog/python-interactive-plot-libraries/\nCode Repositories\nGitHub\nGitLab (open source!!): https://about.gitlab.com/\nBitbucket: https://bitbucket.org/product\n…\nDon’t forget to provide information about the versions of\nsoftware and libraries that were used when running this specific\nanalysis\nIt is also a great idea to add license to your project so people know\nhow they can use your code: https://choosealicense.com/\nBinder & jupyter\n\nTransform your git based repo into an interactive\njupyter notebook https://mybinder.org/!! So other researchers can run\nyour code without having to install anything!\n\nTry it: https://github.com/syoh/my-awesome-project\nCiting your code\nNote that it is also possible to assign a DOI\nto cite a specific version of your repository. For example, see here for\nmore information on how to link Zenodo\nand GitHub.\nData Repositories\nAs we discussed earlier, code repositories are note necessarily the\nbest home for your data sets, especially if their format is not text\nbased and if their size is large (>100MB). In addition data\nrepositories offer better support to metadata standard that will help\nyou to describe your data and thus make them more discoverable. Like\ncode repositories, data repositories will version your data creating an\nhistory of your data sets that you can navigate. In addition, most of\nthe data repositories will also offer to mint a DOI to cite your data\n(to be precise a specific version of your data) in a convenient and non\nubiquitous way.\nWe will talk more in depth about data repositories in the Fall, but\nfor now we will mention to entryways to environmental data\nfederation:\nDataONE: a federation of data repositories, https://www.dataone.org/\nre3data: a registry of research data repositories, https://www.re3data.org/\nStarting by searching data in your field and see where other\nresearchers are archiving their data is often a great way to determine\nwhich data repository could be a good home for your own data.\nYou as an Author\n\nIt is important to be able to reference to yourself as a researcher\nand as an author of your work in a non ambiguous manner. From their\nwebsite: ORCID\nis a great way to create a persistent digital identifier (an ORCID iD)\nthat you own and control, and that distinguishes you from every other\nresearcher. ORCID is also more and more use as an authentication\nsystem for many services (e.g. data repositories).\nWhat about your computing\nenvironment\nSession info\nYour analysis was done with specific versions both of the program\nused but also of all the packages involved, as well as the\nspecifications of Operating System (OS) that was used. The good use is\nthat there ar tools to let you capture this information in a systematic\nmanner.\n\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] reticulate_1.25 devtools_2.4.4  usethis_2.1.6  \n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            lubridate_1.8.0     httr_1.4.3         \n [4] rprojroot_2.0.3     tools_4.2.1         profvis_0.3.7      \n [7] backports_1.4.1     bslib_0.4.0         utf8_1.2.2         \n[10] R6_2.5.1            DBI_1.1.3           colorspace_2.0-3   \n[13] urlchecker_1.0.1    withr_2.5.0         tidyselect_1.1.2   \n[16] prettyunits_1.1.1   processx_3.7.0      downlit_0.4.2      \n[19] compiler_4.2.1      rvest_1.0.2         cli_3.3.0          \n[22] xml2_1.3.3          bookdown_0.27       sass_0.4.2         \n[25] scales_1.2.0        readr_2.1.2         callr_3.7.1        \n[28] stringr_1.4.1       digest_0.6.29       rmarkdown_2.14     \n[31] pkgconfig_2.0.3     htmltools_0.5.3     sessioninfo_1.2.2  \n[34] dbplyr_2.2.1        fastmap_1.1.0       highr_0.9          \n[37] htmlwidgets_1.5.4   rlang_1.0.4         readxl_1.4.0       \n[40] rstudioapi_0.14     shiny_1.7.2         jquerylib_0.1.4    \n[43] generics_0.1.3      jsonlite_1.8.0      dplyr_1.0.9        \n[46] distill_1.4         googlesheets4_1.0.0 magrittr_2.0.3     \n[49] Matrix_1.4-1        Rcpp_1.0.9          munsell_0.5.0      \n[52] fansi_1.0.3         lifecycle_1.0.1     stringi_1.7.8      \n[55] yaml_2.3.5          pkgbuild_1.3.1      grid_4.2.1         \n[58] promises_1.2.0.1    forcats_0.5.1       crayon_1.5.1       \n[61] lattice_0.20-45     miniUI_0.1.1.1      haven_2.5.0        \n[64] hms_1.1.1           knitr_1.39          ps_1.7.1           \n[67] pillar_1.8.0        pkgload_1.3.0       reprex_2.0.1       \n[70] glue_1.6.2          evaluate_0.16       remotes_2.4.2      \n[73] modelr_0.1.8        png_0.1-7           vctrs_0.4.1        \n[76] tzdb_0.3.0          httpuv_1.6.5        cellranger_1.1.0   \n[79] gtable_0.3.0        purrr_0.3.4         tidyr_1.2.0        \n[82] assertthat_0.2.1    cachem_1.0.6        ggplot2_3.3.6      \n[85] xfun_0.32           mime_0.12           xtable_1.8-4       \n[88] broom_1.0.0         tidyverse_1.3.2     gitcreds_0.1.1     \n[91] later_1.3.0         googledrive_2.0.0   gargle_1.2.0       \n[94] tibble_3.1.8        memoise_2.0.1       ellipsis_0.3.2     \n[97] here_1.0.1         \n\nor even better:\n\n\ndevtools::session_info()\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2022-08-24\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ─────────────────────────────────────────────────────────\n package       * version date (UTC) lib source\n assertthat      0.2.1   2019-03-21 [1] CRAN (R 4.2.0)\n backports       1.4.1   2021-12-13 [1] CRAN (R 4.2.0)\n bookdown        0.27    2022-06-14 [1] CRAN (R 4.2.0)\n broom           1.0.0   2022-07-01 [1] CRAN (R 4.2.0)\n bslib           0.4.0   2022-07-16 [1] CRAN (R 4.2.0)\n cachem          1.0.6   2021-08-19 [1] CRAN (R 4.2.0)\n callr           3.7.1   2022-07-13 [1] CRAN (R 4.2.0)\n cellranger      1.1.0   2016-07-27 [1] CRAN (R 4.2.0)\n cli             3.3.0   2022-04-25 [1] CRAN (R 4.2.0)\n colorspace      2.0-3   2022-02-21 [1] CRAN (R 4.2.0)\n crayon          1.5.1   2022-03-26 [1] CRAN (R 4.2.0)\n DBI             1.1.3   2022-06-18 [1] CRAN (R 4.2.0)\n dbplyr          2.2.1   2022-06-27 [1] CRAN (R 4.2.0)\n devtools      * 2.4.4   2022-07-20 [1] CRAN (R 4.2.0)\n digest          0.6.29  2021-12-01 [1] CRAN (R 4.2.0)\n distill         1.4     2022-05-12 [1] CRAN (R 4.2.0)\n downlit         0.4.2   2022-07-05 [1] CRAN (R 4.2.0)\n dplyr           1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n ellipsis        0.3.2   2021-04-29 [1] CRAN (R 4.2.0)\n evaluate        0.16    2022-08-09 [1] CRAN (R 4.2.0)\n fansi           1.0.3   2022-03-24 [1] CRAN (R 4.2.0)\n fastmap         1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n forcats         0.5.1   2021-01-27 [1] CRAN (R 4.2.0)\n fs              1.5.2   2021-12-08 [1] CRAN (R 4.2.0)\n gargle          1.2.0   2021-07-02 [1] CRAN (R 4.2.0)\n generics        0.1.3   2022-07-05 [1] CRAN (R 4.2.0)\n ggplot2         3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n gitcreds        0.1.1   2020-12-04 [1] CRAN (R 4.2.0)\n glue            1.6.2   2022-02-24 [1] CRAN (R 4.2.0)\n googledrive     2.0.0   2021-07-08 [1] CRAN (R 4.2.0)\n googlesheets4   1.0.0   2021-07-21 [1] CRAN (R 4.2.0)\n gtable          0.3.0   2019-03-25 [1] CRAN (R 4.2.0)\n haven           2.5.0   2022-04-15 [1] CRAN (R 4.2.0)\n here            1.0.1   2020-12-13 [1] CRAN (R 4.2.0)\n highr           0.9     2021-04-16 [1] CRAN (R 4.2.0)\n hms             1.1.1   2021-09-26 [1] CRAN (R 4.2.0)\n htmltools       0.5.3   2022-07-18 [1] CRAN (R 4.2.0)\n htmlwidgets     1.5.4   2021-09-08 [1] CRAN (R 4.2.0)\n httpuv          1.6.5   2022-01-05 [1] CRAN (R 4.2.0)\n httr            1.4.3   2022-05-04 [1] CRAN (R 4.2.0)\n jquerylib       0.1.4   2021-04-26 [1] CRAN (R 4.2.0)\n jsonlite        1.8.0   2022-02-22 [1] CRAN (R 4.2.0)\n knitr           1.39    2022-04-26 [1] CRAN (R 4.2.0)\n later           1.3.0   2021-08-18 [1] CRAN (R 4.2.0)\n lattice         0.20-45 2021-09-22 [1] CRAN (R 4.2.1)\n lifecycle       1.0.1   2021-09-24 [1] CRAN (R 4.2.0)\n lubridate       1.8.0   2021-10-07 [1] CRAN (R 4.2.0)\n magrittr        2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n Matrix          1.4-1   2022-03-23 [1] CRAN (R 4.2.1)\n memoise         2.0.1   2021-11-26 [1] CRAN (R 4.2.0)\n mime            0.12    2021-09-28 [1] CRAN (R 4.2.0)\n miniUI          0.1.1.1 2018-05-18 [1] CRAN (R 4.2.0)\n modelr          0.1.8   2020-05-19 [1] CRAN (R 4.2.0)\n munsell         0.5.0   2018-06-12 [1] CRAN (R 4.2.0)\n pillar          1.8.0   2022-07-18 [1] CRAN (R 4.2.0)\n pkgbuild        1.3.1   2021-12-20 [1] CRAN (R 4.2.0)\n pkgconfig       2.0.3   2019-09-22 [1] CRAN (R 4.2.0)\n pkgload         1.3.0   2022-06-27 [1] CRAN (R 4.2.0)\n png             0.1-7   2013-12-03 [1] CRAN (R 4.2.0)\n prettyunits     1.1.1   2020-01-24 [1] CRAN (R 4.2.0)\n processx        3.7.0   2022-07-07 [1] CRAN (R 4.2.0)\n profvis         0.3.7   2020-11-02 [1] CRAN (R 4.2.0)\n promises        1.2.0.1 2021-02-11 [1] CRAN (R 4.2.0)\n ps              1.7.1   2022-06-18 [1] CRAN (R 4.2.0)\n purrr           0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n R6              2.5.1   2021-08-19 [1] CRAN (R 4.2.0)\n Rcpp            1.0.9   2022-07-08 [1] CRAN (R 4.2.0)\n readr           2.1.2   2022-01-30 [1] CRAN (R 4.2.0)\n readxl          1.4.0   2022-03-28 [1] CRAN (R 4.2.0)\n remotes         2.4.2   2021-11-30 [1] CRAN (R 4.2.0)\n reprex          2.0.1   2021-08-05 [1] CRAN (R 4.2.0)\n reticulate    * 1.25    2022-05-11 [1] CRAN (R 4.2.0)\n rlang           1.0.4   2022-07-12 [1] CRAN (R 4.2.0)\n rmarkdown       2.14    2022-04-25 [1] CRAN (R 4.2.0)\n rprojroot       2.0.3   2022-04-02 [1] CRAN (R 4.2.0)\n rstudioapi      0.14    2022-08-22 [1] CRAN (R 4.2.1)\n rvest           1.0.2   2021-10-16 [1] CRAN (R 4.2.0)\n sass            0.4.2   2022-07-16 [1] CRAN (R 4.2.0)\n scales          1.2.0   2022-04-13 [1] CRAN (R 4.2.0)\n sessioninfo     1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n shiny           1.7.2   2022-07-19 [1] CRAN (R 4.2.0)\n stringi         1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr         1.4.1   2022-08-20 [1] CRAN (R 4.2.0)\n tibble          3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr           1.2.0   2022-02-01 [1] CRAN (R 4.2.0)\n tidyselect      1.1.2   2022-02-21 [1] CRAN (R 4.2.0)\n tidyverse       1.3.2   2022-07-18 [1] CRAN (R 4.2.0)\n tzdb            0.3.0   2022-03-28 [1] CRAN (R 4.2.0)\n urlchecker      1.0.1   2021-11-30 [1] CRAN (R 4.2.0)\n usethis       * 2.1.6   2022-05-25 [1] CRAN (R 4.2.0)\n utf8            1.2.2   2021-07-24 [1] CRAN (R 4.2.0)\n vctrs           0.4.1   2022-04-13 [1] CRAN (R 4.2.0)\n withr           2.5.0   2022-03-03 [1] CRAN (R 4.2.0)\n xfun            0.32    2022-08-10 [1] CRAN (R 4.2.0)\n xml2            1.3.3   2021-11-30 [1] CRAN (R 4.2.0)\n xtable          1.8-4   2019-04-21 [1] CRAN (R 4.2.0)\n yaml            2.3.5   2022-02-21 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n─ Python configuration ─────────────────────────────────────────────\n python:         /Users/brun/Library/r-miniconda/envs/r-reticulate/bin/python\n libpython:      /Users/brun/Library/r-miniconda/envs/r-reticulate/lib/libpython3.8.dylib\n pythonhome:     /Users/brun/Library/r-miniconda/envs/r-reticulate:/Users/brun/Library/r-miniconda/envs/r-reticulate\n version:        3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:50:56)  [Clang 11.1.0 ]\n numpy:          /Users/brun/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/numpy\n numpy_version:  1.21.4\n\n────────────────────────────────────────────────────────────────────\n\nYou can save all this content to an session_info.txt\nfile and upload it to your repository.\nIn python, using pip freeze > requirements.txt or\nconda list --export > requirements.txt will create a\ntext file listing all the libraries (and their versions) used in a\nspecific python environment. You can actually use this file to\n(re)install all the packages and specific versions into a new python\nenvironment. It is also great practice to add this file to your\nrepository.\nContainers\nA helpful abstraction for capturing the computing environment is a\ncontainer, whereby a container is created from a set of instructions in\na recipe. For the most common containerisation software, Docker, this\nrecipe is called a Dockerfile. Docker is an open platform for\ndeveloping, shipping, and running applications. Docker enables you to\nseparate your applications from your infrastructure and ship the\ncontainers to others. A Docker container can be seen as a computer\ninside your computer.\n\n\n\nFigure 1: http://jsta.github.io/r-docker-tutorial/\n\n\n\nA few good readings:\nDocker for scientific reproducibility: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316\nThe Whole Tale project: combining containers with data repositories\nhttps://wholetale.org/\nDocker tutorial: http://jsta.github.io/r-docker-tutorial/\nEnvironment Management with Docker: https://environments.rstudio.com/docker\nSharing and Running R code using Docker: https://aboland.ie/Docker.html\nXaringan\nXarigan is an R package to create slide deck using R Markdown: https://github.com/yihui/xaringan\n\n\nremotes::install_github('yihui/xaringan')\n\n\nHere is a good introduction to it: https://www.favstats.eu/post/xaringan_tut/\n\n\n\n",
      "last_modified": "2022-08-24T09:08:28-07:00"
    },
    {
      "path": "flex-rmarkdown_syntax.html",
      "title": "Rmarkdown syntax",
      "author": [],
      "contents": "\n\nContents\nFlex session\nThe\nMarkdown syntax\n\nR Markdown\n\n\n\nFlex session\nThe Markdown syntax\n\nYou will find Markdown to be useful for a lot of various tasks and\ntools you will use as a data scientist. The good new is that the syntax\nis very basic and it is easy to get started with. In addition, Markdown\nfiles (.md) are text files that can be opened by any text editor and can\nbe easily be versioned. You can also render markdown document using\npandoc into various document formats (PDF, html, ….).\nHowever it is important to be aware that there are several flavors of\nMarkdown out there that are adding more capabilities to the basic one.\nFor example GitHub has created its own flavor.\nAnother interesting feature of Markdown is that you can use html\nformatting when you want to do some more advanced formatting. In fact,\ndocument styles can be customized with HTML/CSS and math notation can be\nincluded using LaTeX or mathjax.\n$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nDuring this flex session, explore what you can do with with Markdown\nbasic\nhttps://www.markdownguide.org/basic-syntax/\nAnd extended:\nhttps://www.markdownguide.org/extended-syntax/\nR Markdown\nWhat about R markdown in all of this? Well the R Markdown syntax, or\nmore precisely the knitr package, add extra R oriented\nfeatures which allows the translate of R code, figures and more into a\nMarkdown document to produce dynamic and reproducible documents.\n\n\n\n\n\n\n",
      "last_modified": "2022-08-24T09:08:29-07:00"
    },
    {
      "path": "group_project.html",
      "title": "Group Project",
      "author": [],
      "contents": "\n\nContents\nInvestigating\nthe impact of Hurricane Hugo on Stream Chemistry data in Luquillo\nHere are the expectations\nBackground\nGoals\nData\nChemistry\nof stream water data from the Luquillo Mountains\n\nGroup project presentation\n(12-15min)\nReference\n\n\nInvestigating\nthe impact of Hurricane Hugo on Stream Chemistry data in Luquillo\nWe are going to build on the exercise we used the first day and work\non implementing and refining the workflow we developed during that\nsession.\nHere are the expectations\nBy group of 4-5 collaborators: - Find an awesome name for your group\n(one of the hardest step) -\nSetup a shared GitHub repository - Use the MEDS server Taylor as your\nmain computing resource - Use GitHub to manage your code development in\na collaborative manner - Use shared folder on Taylor to manage your data\n/courses/EDS214/my_group_name - Document your work as you\ngo!! - Use GitHub issues to track your work and discuss progress and\ntasks\nBackground\nThe Luquillo Experimental Forest (LEF) has been a center of tropical\nforestry research for nearly a century. In addition, the LEF is a\nrecreation site for over a half a million people per year, a water\nsupply for approximately 20% of Puerto Rico’s population, a regional\ncenter for electronic communication, and a refuge of Caribbean\nbiodiversity. It is the goal of the USDA Forest Service and the\nUniversity of Puerto to promote and maintain the forest’s role as a\ncenter of active and dynamic scientific inquiry. However, to maintain\nthe ecological integrity of the forest while balancing the many demands\nplaced upon it’s resources, certain protocol is required. This guide\nprovides the major protocols that govern research in the LEF. These\nprotocols are designed to help researchers protect the forests, obey the\nlaw, create an amiable and non-discriminatory work environment, and\nprovide a historical record for future scientists\nHere for more information about the Luquillo site: https://lternet.edu/site/luquillo-lter/\n\n\n\nGoals\nCan you recreate that plot (content, not style wise)\nAsk your own question! You are encouraged to define you own question\naround this topic as a team using the latest data set\nData\nData are available from the Environmental Data Initiative (EDI) that is hosting\nmost of the data of the Long Term Ecological Research (LTER) Network.\nChemistry\nof stream water data from the Luquillo Mountains\nMcDowell, W. 2022. Chemistry of stream water from the Luquillo\nMountains ver 4923058. Environmental Data Initiative. https://doi.org/10.6073/pasta/1dd1a59a7e6fbe95fdf8736477d81b83\nGroup project presentation\n(12-15min)\nMajor points to hit:\nGoal/Question + workflow you used to achieve it (aka THE\nplan)\nHow did you set up your project (server, data, code, …)\nHow did you organize your team (tasks, who did what)\nResults\nChallenges\nShare your work! Were to find your ressources\nReference\nSchaefer, D., McDowell, W., Scatena, F., & Asbury, C. (2000).\nEffects of hurricane disturbance on stream water concentrations and\nfluxes in eight tropical forest watersheds of the Luquillo Experimental\nForest, Puerto Rico. Journal of Tropical Ecology, 16(2), 189-207. doi:10.1017/S0266467400001358\n\n\n\n",
      "last_modified": "2022-08-24T12:48:29-07:00"
    },
    {
      "path": "index.html",
      "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
      "description": "This course will introduce students to conceptual organization of workflows as a way to conduct reproducible analyses",
      "author": [],
      "contents": "\n\nContents\nInstructor\nTutor\nImportant\nlinks\nCourse\ndescription\nPredictable daily schedule\nLearning\nobjectives\nGrading\nSessions (subject to\nchange)\nCourse\nrequirements\nComputing\nTextbook\n\n\n\n\n\nFigure 1: Workflow example using the tidyverse.\nNote the program box around the workflow and the iterative nature of the\nanalytical process described. Source: R for Data Science https://r4ds.had.co.nz/\n\n\n\nInstructor\nJulien Brun\n(brun@nceas.ucsb.edu)\nTutor\nCasey O’Hara (cohara@bren.ucsb.edu)\nImportant links\nCourse\nsyllabus\nCode of Conduct\nCourse description\nThe generation and analysis of environmental data is often a complex,\nmulti-step process that may involve the collaboration of many people.\nIncreasingly tools that document and help to organize workflows are\nbeing used to ensure reproducibility, shareability, and transparency of\nthe results. This course will introduce students to the conceptual\norganization of workflows (including code, documents, and data) as a way\nto conduct reproducible analyses. These concepts will be combined with\nthe practice of various software tools and collaborative coding\ntechniques to develop and manage multi-step analytical workflows as a\nteam.\nPredictable daily schedule\nCourse dates: Monday (2022-08-22) - Friday\n(2022-08-26)\nEDS 214 is an intensive 1-week long 2-unit course. Students should\nplan to attend all scheduled sessions. All course requirements will be\ncompleted between 8am and 5pm PST (M - F), i.e. you are not expected to\ndo additional work for EDS 214 outside of those hours, unless you are\nworking with the Teaching Assistant in student hours.\nDaily schedule (subject to change):\nTime (PST)\nActivity\n9:00am - 9:50am\nLecture 1 (50 min)\n9:50am - 10:00am\nBreak 1 (10 min)\n10:00am - 11:00am\nInteractive Session 1 (60 min)\n11:00am - 12:00am\nFlex time (60 min)\n12:00am - 1:15pm\nLunch (75 min)\n1:15pm - 2:00pm\nLecture 2 (45 min)\n2:00pm - 2:10pm\nBreak 2 (10 min)\n2:10pm - 3:20pm\nInteractive Session 2 (70 min)\n3:20pm - 3:30pm\nBreak 3 (10 min)\n3:30pm - 5:00pm\nGroup projects and time with Casey (90 min)\nLearning objectives\nThe goal of EDS 214 - Analytical Workflows and Scientific\nReproducibility is to expose incoming MEDS students to “good enough”\npractices of scientific programming develop skills in environmental data\nscience to produce reproducible research. By the end of the course,\nstudents should be able to:\nDevelop knowledge in scientific analytical\nworkflows To learn how to make your data-riven research\nreproducible, it is important to develop scientific workflows that will\nbe relying on programming to accomplish the necessary tasks to go from\nthe raw data to the results of your analysis (figures, new data,\npublications, …). Scripting languages, even better open ones such as\nR and python, are well-suited for scientists\nto develop reproducible scientific workflows, but are not the only tools\nyou will need to develop reproducible and collaborative\nworkflows\nLearn how to code in a collaborative manner by\npracticing techniques such as code review and pair programming. Become\ncomfortable asking for and conducting code review using git\nand GitHub to create pull request, ask feedback from peers,\nand merge changes into the main repository. Practice pair programming to\ncement the collaborative development of reproducible analytical\nworkflows\nPractice documenting code and data in a\nsystematic way that will enable your collaborators, including your\nfuture self, to understand and reuse your work\nGrading\nThe grading for this course is organized as follow:\n50% Class participation\n50% Group project\nSessions (subject to change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourse requirements\nComputing\nMinimum MEDS device\nrequirements\nHave a ready to be used GitHub Account (https://github.com/)\nTextbook\nR for Data Science: https://r4ds.had.co.nz/\nThe Practice of Reproducible Research: http://www.practicereproducibleresearch.org/\n\n\n\n",
      "last_modified": "2022-08-24T09:08:31-07:00"
    },
    {
      "path": "README.html",
      "author": [],
      "contents": "\n\nContents\nEDS 214: Analytical Workflows and Scientific Reproducibility\nInstructor\nCourse description\n\n\nEDS 214: Analytical Workflows and Scientific Reproducibility\nInstructor\nJulien Brun (brun@nceas.ucsb.edu)\nCourse description\nThe generation and analysis of environmental data is often a complex, multi-step process that may involve the collaboration of many people. Increasingly tools that document help to organize and document workflows are being used to ensure reproducibility and transparency of the results. This course will introduce students to conceptual organization of workflows as a way to conduct reproducible analyses, as well as various software tools that help users to manage multi-step processes that requires tools for storing, managing and sharing workflows, code, documents and data.\n\nThis website template is made with distill by RStudio as an optional starting point for teachers in the Master of Environmental Data Science Program at the Bren School (UC Santa Barbara).\nClick here for a template preview.\n\n\n",
      "last_modified": "2022-08-24T12:48:29-07:00"
    },
    {
      "path": "resources.html",
      "title": "Course resources",
      "author": [],
      "contents": "\n\nContents\nMore\nReferences\n\nMore References\nThe Practice of Reproducible Research-Case Studies and Lessons from\nthe Data-Intensive Sciences:\nKitzes, Justin, Daniel Turek, and Fatma Deniz. n.d. http://www.practicereproducibleresearch.org/\nReproducible Research with R and RStudio, Chrisotpher Gandrud http://christophergandrud.github.io/RepResR-RStudio/index.html\nInitial steps toward reproducible research, Karl Broman, https://kbroman.org/steps2rr/\nReproducibility guide from ROpenSci https://ropensci.github.io/reproducibility-guide/\n\n\n\n",
      "last_modified": "2022-08-24T09:08:32-07:00"
    },
    {
      "path": "rstudio_debugging.html",
      "title": "Debugging in RStudio",
      "author": [],
      "contents": "\n\nContents\nThe debugging interface\n\nThis section is adapted from the more detailed material available\non the RStudio website: https://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio\nDebugging is a broad topic that can be approached in many ways.\nGenerically, at some point you will likely attempt to execute a script\nin R, receive errors and not know exactly what caused the errors. One\napproach would be to run your code line by line, but RStudio has some\nuseful built-in\ndebugging features.\nOne basic approach to debugging is to create a breakpoint in\nyour code – this forces your code to “stop” executing when it reaches\nsome certain function or line number in your code, allowing you then to\nexamine the state of various variables, etc. The easiest way to do this\nis to set the breakpoint by manually clicking next to the desired line\nnumber in the code panel, per\nthis web example:\nhttps://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio#stopping-on-a-lineSetting this editor breakpoint creates tracing code\nassociated with the R function object. You can remove the breakpoint by\nclicking on the red dot by the line number. Also note the Debug toolbar\nhas an option to clear all breakpoints.\nNote: keep in mind that you can’t set breakpoints anywhere.\nIn general, you want to insert breakpoints at top-level\nexpressions or simple, named functions.\nAn alternative way to set breakpoints is with the\nbrowser() function. This must be typed into your code, per\nthis web example:\nhttps://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio(image source: https://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio#stopping-on-a-line)\nThe debugging interface\nOnce your code hits a breakpoint, RStudio enters debugging mode.\nDetails on the debugging interface can\nbe found here, but we summarize the main points below:\nThe Environment tab will display the objects in the\nenvironment of the currently executing function (i.e., the function’s\ndefined arguments)\nThe Traceback literally traces back how you arrived\nat the currently executing function (latest executed command is at the\ntop of the list). This is analagous to the “call stack” in other\nprogramming languages.\nThe Code window highlights the currently executing\nfunction and may create a new tab, named Source Viewer, when\nthe current function the debugger is stepping through is not in the main\nR script.\nThe Console retains most of its normal functionality\nin debugging mode, but contains some additional buttons that appear at\nthe top to facilitate moving through code lines (see below).\n ## Further Reading\nFunctional programming in R: http://adv-r.had.co.nz/Functional-programming.html#functional-programming\nScoping in R: http://adv-r.had.co.nz/Functions.html\n\n\n\n",
      "last_modified": "2022-08-24T09:08:33-07:00"
    },
    {
      "path": "vim.html",
      "title": "Vim, the editor",
      "author": [],
      "contents": "\n\nContents\nOther text\neditors\nYour typical editor works\nlike this…\n\nVim is\ndifferent\nVim has\nmodes\n\nThe keyboard is your\nfriend\nFor the\nimpatient\nNormal mode\nNow\nthat you’ve seen that you can actually type in vim, let’s learn\nabout the magic of Normal mode!\n\nMoving around (“motions”)\nMoving around (“motions”)\nInsert\nDelete (really, this is\n“cut”)\nCopy and\npaste\nSearch and\nreplace\nDo-overs!\nExtras\nSaving\nand quitting\nTabs\nBasic\nediting\nOperators and repetition\nYank & paste\nSearching\nMarks &\nmacros\nVarious\nmotions\nVarious\ncommands\nPlaces to\nlearn\n\nOther text editors\nYour typical editor works\nlike this…\nWhen you type almost any key, the corresponding character (letter,\nnumber, symbol, or whitespace) appears at the cursor\nCertain special keys do special operations (control, alt, command,\nsuper)\nFor a GUI app, you can/must click menus for other functionality\nVim is different\nVim has modes\nNormal\n\nfor almost everything except typing! Insert\n\n\nfor typing Visual\n\n\nfor highlighting a region to operate on\n\n\n(and others you don’t need to know about)\n\nThe keyboard is your friend\n\nFor the impatient\nStart vim in terminal: $ vim test.txt\nPress i to enter insert mode\nType to your heart’s content\nWhen you’re done, hit Esc to leave insert mode\nThen type :w to save your document\nThen type :q to quit\nif you have unsaved changes, vim will complain\nto save, see previous, or do :wq in one step\nto discard changes and quit, do :q!\n\nNormal mode\nNow\nthat you’ve seen that you can actually type in vim, let’s learn\nabout the magic of Normal mode!\nMoving around (“motions”)\nStepping\nby line: up k, down j\nby character: forward l, backward h\nby word: forward w, backward b\nby sentence: forward ), backward (\nby paragraph: forward }, backward {\nMoving around (“motions”)\nJumping\nto start/end of line: 0/$\nto first/last line of file: gg/G\nto a character on current line: f/F{char}\nto identical word: */#\nto line number: :{#} or {#}G\nto top, middle, or bottom of screen: {HML}\nInsert\nstarting right where you are: i\nstarting at the next character – a\nstarting at the end of the current line – A\nstarting at the beginning of the line – I\nstarting on the next line – o\nstarting on the previous line – O\nenter ‘replace mode’ right where you are – R\nreplace current character with one other character –\nr{char}\ndelete current character, then enter insert mode –\ns\ndelete current line, then enter insert mode – S\nDelete (really, this is “cut”)\ndelete character under cursor: x\ndelete character before cursor: X\ndelete to end of line: D\ndelete entire line: dd\ndelete # lines: d#d\ndelete over some motion: d{motion}\nuse backspace in insert mode (like every other editor)\nCopy and paste\ncopy (“yank”) – y{motion}\ncopy (“yank”) –\npaste after cursor – p\npaste before cursor – P\nSearch and replace\nwe already saw some of this with jumping\nsearch for arbitrary expression – /{expression}\nsearch and replace\n:s/foo/bar/\n:s/foo/bar/g\n:%s/foo/bar/g\n\nDo-overs!\nundo change: u\nrepeat last change: .\nredo last undone change: Ctrl-r\nExtras\ntoggle case – ~\nSaving and quitting\nsave and quit: ZZ (or :wq)\nsave and keep editing: :w\nquit without saving: :q\nreally quit without saving: :q!\nTabs\nopen another file in new tab: :tabe <file>\nchange tabs: forward gt, backward gT\nBasic editing\nhttp://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.htmlOperators and repetition\nhttp://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.htmlYank & paste\nhttp://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.htmlSearching\nhttp://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.htmlMarks & macros\nhttp://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.htmlVarious motions\nhttp://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.htmlVarious commands\nhttp://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.htmlPlaces to learn\nThe pictorial\ncheatsheets used in this slide\nColor\nPDF of all 8 diagrams\n\nA nice introduction\nwith some clear prose\nA popular portal to the rest\nof the vim universe\nA bit heavy-handed, but if you really like learning by watching over someone’s\nshoulder…\nLastly, if you just want to learn how to start and quit vim\nall in the first page of tips\n\n\n\n",
      "last_modified": "2022-08-24T09:08:34-07:00"
    }
  ],
  "collections": []
}
