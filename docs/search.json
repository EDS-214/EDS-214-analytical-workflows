{
  "articles": [
    {
      "path": "assignments.html",
      "title": "Assignments",
      "author": [],
      "contents": "\n\nContents\nAssignment materials and dates\nAssignment expectations and grading\n\n\n\nTO UPDATE THIS PAGE: Open and edit the assignments.Rmd file, in the project root, to delete this placeholder text and customize with your own!\n\nAssignment materials and dates\nAssignment materials\nAssigned\nDue date\nA link to the repo\n2021-01-01\n2021-01-08\nA link to the repo\n2021-01-15\n2021-01-23\nA link to the repo\n2021-02-01\n2021-02-09\nA link to the repo\n2021-02-14\n2021-02-23\nAssignment expectations and grading\nThis might also go on the home page & in syllabus\nOr could reinforce here\n\n\n\n\n",
      "last_modified": "2021-08-20T21:55:42-07:00"
    },
    {
      "path": "code_of_conduct.html",
      "title": "Code of Conduct",
      "author": [],
      "contents": "\n\nContents\nOverview\nExamples of behavior that contributes to creating a positive environment include:\nExamples of unacceptable behavior by class participants include:\nOther resources at UCSB\n\nAll enrolled students, auditors, and course visitors are expected to comply with the following code of conduct. We expect cooperation from all members to help ensure a safe and welcoming environment for everybody.\nOverview\nWe are determined to make our courses welcoming, inclusive and harassment-free for everyone regardless of gender, gender identity and expression, race, age, sexual orientation, disability, physical appearance, body size, or religion (or lack thereof). We do not tolerate harassment of class participants, teaching assistants, or instructors in any form. Derogatory, abusive, demeaning or sexual language and imagery is not appropriate or acceptable. Saying something “as a joke” does not make it less offensive, harmful, or consequential.\nAnything not covered here but that exists in the UCSB Student Conduct Code also applies, and will be enforced by UCSB Policy.\nThese expectations and consequences apply to synchronous discussions, office hours, the course Slack workspace, and all other modes of communication, posting or discussion by course participants.\nExamples of behavior that contributes to creating a positive environment include:\nUsing welcoming and inclusive language\nBeing an aware and respectful colleague (raise your hand when asked, respect others’ time and space, include peers in small discussions, don’t dominate meetings, etc.)\nGiving proper credit to the creator (of an idea/material/solution/etc.)\nBeing respectful of differing viewpoints and experiences\nShowing empathy towards all community members\nUnderstanding that an individual’s experience and worldview are influenced by multiple (and often compounding) facets of their identity, and that your perception of a situation/topic/reaction may be very different from your classmates’\nExamples of unacceptable behavior by class participants include:\nDistracting other students in classes in labs, or otherwise distracting from their education\nAny abuse, disrespect or harassment of teaching assistants, other students, or teachers, is not tolerated and will result in disciplinary action as needed\nThe use of unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments or language, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\nMembers asked to stop any harassing behavior are expected to comply immediately. If you are being harassed, notice that someone else is being harassed, or have any other concerns in or related to these classes, you are welcome to contact Allison or use outside resources.\nOther resources at UCSB\nAt Campus Advocacy, Resources and Education (CARE) you can chat with a UCSB staff member in a confidential setting. CARE assists faculty, staff and students who have been impacted by sexual harassment, sexual assault, domestic/dating violence, and stalking. The office is confidential so you can talk with a staff member in private without any reporting obligations.\nPhone: (805) 893-4613.\nOffice is located in the Student Resource Building (SRB) near parking lot 23.\nAnother confidential resource is the Campus Ombuds office. The Ombuds office is particularly helpful if you would like to describe a sensitive issue in a confidential setting and learn more about campus resources to address the issue. The Ombuds office is located at 1205-K Girvetz Hall and their phone number is 805-893-3285. The Ombuds office provides consultation, mediation, and facilitation, among other services, for faculty, staff and students.\nUCSB Academic Counseling is a resource outside of the Bren department that can help with a number of topics from academic planning to “balancing personal difficulties in academics.”\nGraduate counselor: Ryan Sims\nPhone: 805-893-2068\nEmail: ryan.sims@graddiv.ucsb.edu\nThis is a living document, that we are always hoping to improve. If you have suggestions, questions or ideas for how we can update our Code of Conduct, we encourage you to reach out to us and will be grateful for your feedback.\nContributions by:\nAllison Horst\nJessica Couture {“mode”:“full”,“isActive”:false}\n\n\n\n",
      "last_modified": "2021-08-20T21:55:43-07:00"
    },
    {
      "path": "day_1.html",
      "title": "Reproducible & Collaborative Workflows",
      "author": [],
      "contents": "\n\nContents\nDay 1\nIntroduction\nReproducible Workflows\nLecture 1: Definitions and Concepts\nWhy going reproducible\nHow\nPlaning things\nFrom drawings to pseudocode\nHands-on\n\n\nCoding together – Managing Scripts\nLearning Objectives\nWhy collaborative coding\nHow to code together\nTools\n\nGitHub as a collaborative tool\nQuick recap on version control\n\nCollaborating through Forking\nHands-on\n\nFlex session\nThe Markdown syntax \nR Markdown\n\n\nFurther reading\nData and scientific workflow management:\nOpen Science\nCollaborative coding\nCode Review\n\n\nDay 1\nTime (PST)\nActivity\n9:00am - 10:00am\nIntro & Reproducible and Collaborative workflow (60 min)\n10:00am - 10:10am\nBreak 1 (10 min)\n10:10am - 11:30am\nInteractive Session: Drawing board + reporting (80 min)\n11:30am - 12:00am\nFlex time 1: Markdown and more (30 min)\n12:00am - 1:15pm\nLunch (75 min)\n1:15pm - 2:15pm\nCollaborative coding with git (60 min)\n2:15pm - 2:25pm\nBreak 2 (10 min)\n2:25pm - 3:25pm\nInteractive Session: GitHub forking (60 min)\n3:25pm - 3:35pm\nBreak 3 (10 min)\n3:35pm - 4:05pm\nFlex time 2: Find how to knit an R Markdown document while keeping the markdown file? (30 min)\n4:05pm - 4:30pm\nRecap of the day and Q&A (25 min)\nIntroduction\nA few words…\nReproducible Workflows\nLecture 1: Definitions and Concepts\nSlide deck\nWhy going reproducible\nThere are many reasons why it is essential to make your science reproducible and how the necessity of openness is a cornerstone of the integrity and efficacy of the scientific research process. Here we will also be focusing on why making your work reproducible will empower you to iterate quickly, integrate new information more easily to iterate quickly, scale your analysis to larger data sets, and better collaborate by receiving feedback and contributions from others, as well as enable your “future self” to reuse and build from your own work.\nTo make your data-riven research reproducible, it is important to develop scientific workflows that will be relying on programming to accomplish the necessary tasks to go from the raw data to the results (figures, new data, publications, …) of your analysis. Scripting languages, even better open ones, such as R and python, are well-suited for scientists to develop reproducible scientific workflows. Those scripting languages provide a large ecosystem of libraries (also referred to as packages or modules) that are ready to be leveraged to conduct analysis and modeling. In this course we will introduce how to use R, git and GitHub to develop such workflows as a team.\n\n\n\nFigure 1: Workflow example using the tidyverse. Note the program box around the workflow and the iterative nature of the analytical process described. Source: R for Data Science https://r4ds.had.co.nz/\n\n\n\nTwo points to stress about this figure:\nWorkflows are rarely linear… even less so their implementation\nNote the programming box – yes, you’ll need to code this :)\nWorkflows are developed iteratively, and one of the most helpful things you can do as a data scientist is to talk about them with your research team.\nHow\nWe recommend shying away from spreadsheets as an analytical tool, as well as Graphical User Interfaces (GUI) where you need to click on buttons to do your analysis. Although convenient for data exploration, GUI will limit the reproducibility and the scalability of your analysis as human intervention is needed at every step. Spreadsheets can be useful to store tabular data, but it is recommended to script their analysis, as copy-pasting and references to cells are prone to mistake (see Reinhart and Rogof example). It is also very difficult to track changes and to scale your analysis using spreadsheets. In addition, auto-formatting (number, date, character, …) can silently introduce modifications to your data (e.g. One in five genetics papers contains errors thanks to Microsoft Excel).\nPlaning things\nDon’t start implementing nor coding without planning! It is important to stress that scientists write scripts to help them to investigate scientific question(s). Therefore scripting should not drive our analysis and thinking. We strongly recommend you take the time to plan ahead all the steps you need to conduct your analysis. Developing such a scientific workflow will help you to narrow down the tasks that are needed to move forward your analysis.\nFrom drawings to pseudocode\nMaterials\nCherubini et al., 2007Hands-on\nInvestigating the impacts of Hurricane on stream chemistry in Puerto Rico\nCoding together – Managing Scripts\nLearning Objectives\nIn this part of the lesson, you will learn:\nWhy git is useful for reproducible analysis\nHow to use git to track changes to your work over time\nHow to use GitHub to collaborate with others\nHow to write effective commit messages\nHow to structure your commits so your changes are clear to others\nHow to fork a repository to contribute to its content\nHow to create a pull request\nHow to review a pull request\nWhy collaborative coding\nSlides\nEnvironmental Data Science (EDS), as many other data-driven research fields, requires a transdisciplinary approach to tackle challenges that often span across several domains of expertise. Working as a team will leverage know-how from diverse collaborators and be the most efficient way to tackle complex problems in EDS. Consequently collaborative skills are required to work effectively as a member of a team. No matter their focus, highly effective teams share certain characteristics:\nRight size\nDiverse group of people with the right mix of skills, knowledge, and competencies\nAligned purpose and incentives\nEffective organizational structure\nStrong individual contributions\nSupportive team processes and culture\nSince Analytical Workflows are rarely linear! and are developed iteratively, the most efficient way to iterate quickly on your analysis is to use scripts and leave copy-pasting behind. Programming as part of a team is different than writing a script for your(present)self. However learning programming as part of a team is not only critical to the efficacy of your team, it will also you help you to grow as a programmer by:\nMotivating you to document well your work\nHelping you to think how to make your work reusable (by you, your future you and others)\nLearning to read code from collaborators to build upon each others work\nGain further knowledge in software development tools, such as version control\nDeveloping those skills will accelerate your research and open the door for you to contribute to open source projects.\nHow to code together\nIt is important to acknowledge that there are many solutions to the complex research questions you will be facing in EDS. Each of those solutions will have several possible implementations, meaning that more likely you might code this implementation differently than your collaborators. Integrated software engineer teams generally try to mitigate this by developing coding standards and conventions that will guide how to write code and develop specific implementation. In scientific teams in which the collaboration is more loose and maybe more ephemeral as well, developing detailed coding standards will be too much of an overhead. However, we think it is important to acknowledge that coding style may varies among the data scientists of a project and it is a good discussion to have among the team at the beginning of the project. For example, in R it could be trying to use the tidyverse approach as much as possible. We also think there are two activities that will make the team more efficient: Code Review and Pair Programming.\nTools\nThe good news is there are several tools out there that have been designed to make developing code as a team more efficient. In this course, we will focus on getting familiar with the following:\nVersion control system: say goodbye to save as\nCode repository: where we share code and communicate ideas and feedback\n\n\nknitr::include_graphics(\"img/git_explain_xkcd.png\")\n\n\n\n\nFigure 2:  https://xkcd.com/1597/\n\n\n\nGitHub as a collaborative tool\nQuick recap on version control\ngit and GitHub quick recap\nCollaborating through Forking\nMaterials\nHands-on\nFork the repository of the greetings R package to add the option to change the color of the background of the text.\nRepo: https://github.com/brunj7/greetings\nBy team of two:\nTeam member 1: fork the repository\nTeam member 2: fork the fork that team member 1 just created (yes, you can do this!!)\nWork together on Team member 2’s computer to modify the say_aloah() function to allow changing the text background color to blue in addition to the existing green (see here for more about the crayon package)\nOnce done, create a pull request to integrate the new changes tagging team member 1 as reviewer\nTeam member 1, review the code, comment as needed on the pull request, and merge changes\nFlex session\nThe Markdown syntax \nYou will find Markdown to be useful for a lot of various tasks and tools you will use as a data scientist. The good new is that the syntax is very basic and it is easy to get started with. In addition, Markdown files (.md) are text files that can be opened by any text editor and can be easily be versioned. You can also render markdown document using pandoc into various document formats (PDF, html, ….). However it is important to be aware that there are several flavors of Markdown out there that are adding more capabilities to the basic one. For example GitHub has created its own flavor.\nAnother interesting feature of Markdown is that you can use html formatting when you want to do some more advanced formatting. In fact, document styles can be customized with HTML/CSS and math notation can be included using LaTeX or mathjax.\n$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nDuring this flex session, explore what you can do with with Markdown basic\nhttps://www.markdownguide.org/basic-syntax/\nAnd extended:\nhttps://www.markdownguide.org/extended-syntax/\nR Markdown\nWhat about R markdown in all of this? Well the R Markdown syntax, or more precisely the knitr package, add extra R oriented features which allows the translate of R code, figures and more into a Markdown document to produce dynamic and reproducible documents.\n\n\n\nFurther reading\nHere are a few selected publications to help you to learn more about these topics.\nData and scientific workflow management:\nThe Practice of Reproducible Research-Case Studies and Lessons from the Data-Intensive Sciences:\nKitzes, Justin, Daniel Turek, and Fatma Deniz. n.d. http://www.practicereproducibleresearch.org/\nGood enough practices in Scientific Computing:https://doi.org/10.1371/journal.pcbi.1005510\nScript your analysis:https://doi.org/10.1038/nj7638-563a\nPrinciples for data analysis workflows:https://doi.org/10.1371/journal.pcbi.1008770\nBenureau, F.C.Y., Rougier, N.P., 2018. Re-run, Repeat, Reproduce, Reuse, Replicate: Transforming Code into Scientific Contributions. Front. Neuroinform. 0. https://doi.org/10.3389/fninf.2017.00069\nSandve, G.K., Nekrutenko, A., Taylor, J., Hovig, E., 2013. Ten Simple Rules for Reproducible Computational Research. PLOS Computational Biology 9, e1003285. https://doi.org/10.1371/journal.pcbi.1003285\nSome Simple Guidelines for Effective Data Management:https://doi.org/10.1890/0012-9623-90.2.205\nBasic concepts of data management:https://www.dataone.org/education-modules\nOpen Science\nThe Tao of open science for ecology:https://doi.org/10.1890/ES14-00402.1\nChallenges and Opportunities of Open Data in Ecology:https://doi.org/10.1126/science.1197962\nScientific computing: Code alerthttps://doi.org/10.1038/nj7638-563a\nOur path to better science in less time using open data science toolshttps://doi.org/10.1038%2Fs41559-017-0160\nFAIR data guiding principleshttps://doi.org/10.1038/sdata.2016.18\nSkills and Knowledge for Data-Intensive Environmental Research https://doi.org/10.1093/biosci/bix025\nLet go your datahttps://doi.org/10.1038/s41563-019-0539-5\nCollaborative coding\nA new grad’s guide to coding as a team - Atlassian: https://www.atlassian.com/blog/wp-content/uploads/HelloWorldEbook.pdf\n10 tips for efficient programming: https://www.devx.com/enterprise/top-10-tips-for-efficient-team-coding.html\nAgile Manifesto: https://moodle2019-20.ua.es/moodle/pluginfile.php/2213/mod_resource/content/2/agile-manifesto.pdf\nCode Review\nSmall-Group Code Reviews For Education: https://cacm.acm.org/blogs/blog-cacm/175944-small-group-code-reviews-for-education/fulltext\n\n\n\n",
      "last_modified": "2021-08-20T21:55:44-07:00"
    },
    {
      "path": "day_2.html",
      "title": "Developing Analytical Workflows as a Team",
      "author": [],
      "contents": "\n\nContents\nGitHub branches\nGitHub Hands-on\n\nManaging data-driven projects as a team\nFurther reading\nGitHub Workflow\nGit using RStudio\n\n\n\nGitHub branches\nMaterials\nGitHub Hands-on\nGitHub branches using the website\nGitHub branches using RStudio\nManaging data-driven projects as a team\nMaterials\nFurther reading\nGitHub Workflow\nGitHub:\nguides on how to use GitHub: https://guides.github.com/\nGitHub from RStudio: http://r-pkgs.had.co.nz/git.html#git-pull\n\nForking:\nhttps://help.github.com/articles/fork-a-repo/\nhttps://guides.github.com/activities/forking/\n\nComparison of git repository host services: https://www.git-tower.com/blog/git-hosting-services-compared/\nBranches\ninteractive tutorial on branches: http://learngitbranching.js.org/\nusing git in a collaborative environment: https://www.atlassian.com/git/tutorials/comparing-workflows/centralized-workflow https://moodle2019-20.ua.es/moodle/pluginfile.php/2213/mod_resource/content/2/agile-manifesto.pdf\n\nGit using RStudio\nHappy Git and GitHub for the useR: http://happygitwithr.com/\nR packages - Git and GitHub: http://r-pkgs.had.co.nz/git.html#git-init\nGit mainly from the command line:\nInteractive git 101: https://try.github.io/\nVery good tutorial about git: https://www.atlassian.com/git/tutorials/what-is-version-control\nGit tutorial geared towards scientists: http://nyuccl.org/pages/gittutorial/\nShort intro to git basics: https://github.com/mbjones/gitbasics\nGit documentation about the basics: http://gitref.org/basic/\nGit documentation - the basics: https://git-scm.com/book/en/v2/Getting-Started-Git-Basics\nGit terminology: https://www.atlassian.com/git/glossary/terminology\nIn trouble, guide to know what to do: http://justinhileman.info/article/git-pretty/git-pretty.png\nWant to undo something? https://github.com/blog/2019-how-to-undo-almost-anything-with-git\nGit terminology: https://www.atlassian.com/git/glossary/terminology\n8 tips to work better with git: https://about.gitlab.com/2015/02/19/8-tips-to-help-you-work-better-with-git/\nGitPro book (2nd edition): https://git-scm.com/book/en/v2\n\n\n\n",
      "last_modified": "2021-08-20T21:55:44-07:00"
    },
    {
      "path": "day_3.html",
      "title": "Managing your Code and Data on a Remote Server",
      "author": [],
      "contents": "\n\nContents\nWorking on a remote server\nGroup project\n\nWorking on a remote server\nMaterials\nCERN computing centerGroup project\nGroup project\n\n\n\n",
      "last_modified": "2021-08-20T21:55:45-07:00"
    },
    {
      "path": "day_4.html",
      "title": "Scientific Programming as a Team",
      "author": [],
      "contents": "\n\nContents\nCode review feedback\nPair programming\nBasic principles & practices\nTips and Tricks for Effective Team Programming\nAknowledgements\n\n\nA few more thoughts\nMore is not always better\nProject management\n\nDocumentating things\n\nCode review feedback\nCode review is an asynchronous team activity. Despite its impressive name, code review should be seen by the person asking for the review (submitter) as a great opportunity to have one more pair of eyes looking at your code and providing feedback to make your code better. The “reviewer” should see this activity as a great way to learn from others.\nYou actually have already practice this a little when practicing merging pull request!!\nPair programming\nPair programming is an synchronous team activity, where several programmers get to work together on the same piece of code. This is a great way to gain a better sense of what coding style people are using and better understand their way of solving challenges. It is also a great way to learn from each other. Generally, there is one Driver who is the person typing at the computer. The other role is called Navigator(s). The Navigator does not write code and focuses on finding solutions to the problem. Their use of computer should be limited to searching online for solutions.\nBasic principles & practices\nAdapted from Woody Zuill https://www.agileconnection.com/article/getting-started-mob-programming\nTreat each other with kindness, consideration, and respect - makes group work more fun and sustainable\nDriver/navigator pair programming adapted to work with the whole team - “For an idea to go from your head into the computer, it must go through someone else’s hands.” Speak at the highest level of abstraction that the driver (and the rest of the team) is able to digest at the moment\nTimed Rotation - 20-60 minutes. We don’t require that everyone take the driver role; it is everyone’s choice whether to do so\nWhole Team - every contributor to the project is an integral part of the whole team; when we don’t have the skills we need within the team, we find someone who does and invite them to work with us to accomplish the needed work\nReflect, Tune, and Adjust Frequently - based on agile principle: “At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly.”\nTips and Tricks for Effective Team Programming\nAdapted from Corey Johannsen: https://blog.newrelic.com/2017/10/31/mob-programming-hurdles/\nSuggest, don’t dictate: Instead of telling the driver what to type into their editor, we explain what we’re trying to accomplish and then help the driver find the best solution. We’ve found that drivers learn better this way, and they don’t just end up feeling like a stenographer. Whenever possible, we ask questions that lead the driver to discover the answers on their own.\nStay focused and be present: Shut your laptop and put your phone away. I’ve struggled with following this guideline—we all have—and I recognize that the distraction almost always affects the rest of the mob. We tell all our mob members to be present, and if you can’t, it’s OK to leave until you can be.\nUse a timer, but be ready to pause it: We switch drivers every 20 - 60 minutes. However, we often wander off implementation into design discussions—it’s unavoidable—so this is when we pause the timer. This is another key guideline of our mob: the time you spend driving should be dedicated to writing the code that helps complete the task, not discussing design solutions.\nSet specific tasks for each session: When our mob gathers for a session, we first agree on and create a checklist of the tasks we are going to complete, and order them by priority on a whiteboard. This ensures we are all focused on the same task and keeps us moving forward. Additionally, this keeps us aligned with Minimal Marketable Feature (MMF) work, which we can communicate with our engineering and product managers to assure them we’re completing tasks that align with developing small, self-contained features that demonstrate immediate customer value.\nAknowledgements\nThis section reuses a lot of materials from an R Meetup organized by the Santa Barbara R Users group (https://github.com/R-Meetup-SB/hackathon-201806), including material prepared by Irene Steves.\nA few more thoughts\nMore is not always better\nAdding more analysts late into a project can be counterproductive (Brooks’ law)\nOvertime is not the solution as it will increase likeliness of errors and thus frustration. Try to focus on finding at what time of the day you are the most productive at coding instead\nProject management\nThis is a big topic and will be highly influenced by the team you are working with both in terms of practices and tools used to manage the project. On the coding side, there is a lot to borrow from Agile development approach for scientist. In a nutshell: “put it out there fast and iterate”. In other words to wait to try to have the perfect code or analysis before sharing it with your collaborators. It is more efficient to share and discuss an early / draft version, gather feedback and iterate.\nDocumentating things\nMaterials\n\n\n\n",
      "last_modified": "2021-08-20T21:55:45-07:00"
    },
    {
      "path": "day_5.html",
      "title": "Sharing things",
      "author": [],
      "contents": "\n\nContents\nScientific products\nGitHub pages & the R Markdown family\nInteractive web applications\nR Shiny \nHtml widgets\n\nplotly \n\nCode Repositories\nBinder & jupyter \nCiting your code\n\nData Repositories\nYou as an Author \nWhat about your computing environment\nSession info\nContainers\n\n\nScientific products\nGitHub pages & the R Markdown family\nYou have been using it all week!! and before!! All the course material has been developed as an R Markdown based website, distill website to be precise (see here for more), and it is also a great way to publish and share your note book with your team and the broader community.\nInteractive web applications\nWhen you have complex data that you want to not only visualize but also to let the user interact with your data or customize parameters used in an analysis, interactive web applications are a great way to increase engagement. The good news is that you do not need to be a web developer anymore to spin such applications or dashboards.\nR Shiny \nR Shiny is a very interesting framework that lets you write R code that will then be translated into javascript for you and thus let you develop web application without having to learn any new programming language. Note that you will need a server to host the application.\nCheck out this gallery of shiny apps: https://shiny.rstudio.com/gallery/\nGetting started with Shiny: https://shiny.rstudio.com/tutorial/\nHtml widgets\nHtml widgets offer some great interactive data visualization. It is more limited that Shiny because you can not modify parameters to modify the data use, but it has the advantage that it does not need a server to run the widget and it can be inserted directly into an R Markdown.\nHere to get started: https://www.htmlwidgets.org/\nplotly \nplotly enable you to develop great interactive data visualizations. It has the advantage to be available both in R and Python (and javascript). One great thing in R is that if you are a ggplot master, you can write your plot code using ggplot and transform it into a plotly plot with one line of code (here for an example)\nNote there are also other python libraries to create interactive plots, here are a few: https://mode.com/blog/python-interactive-plot-libraries/\nCode Repositories\nGitHub\nGitLab (open source!!): https://about.gitlab.com/\nBitbucket: https://bitbucket.org/product\n…\nDon’t forget to provide information about the versions of software and libraries that were used when running this specific analysis\nIt is also a great idea to add license to your project so people know how they can use your code: https://choosealicense.com/\nBinder & jupyter \nTransform your git based repo into an interactive jupyter notebook https://mybinder.org/!! So other researchers can run your code without having to install anything!\n\nTry it: https://github.com/syoh/my-awesome-project\nCiting your code\nNote that it is also possible to assign a DOI to cite a specific version of your repository. For example, see here for more information on how to link Zenodo and GitHub.\nData Repositories\nAs we discussed earlier, code repositories are note necessarily the best home for your data sets, especially if their format is not text based and if their size is large (>100MB). In addition data repositories offer better support to metadata standard that will help you to describe your data and thus make them more discoverable. Like code repositories, data repositories will version your data creating an history of your data sets that you can navigate. In addition, most of the data repositories will also offer to mint a DOI to cite your data (to be precise a specific version of your data) in a convenient and non ubiquitous way.\nWe will talk more in depth about data repositories in the Fall, but for now we will mention to entryways to environmental data federation:\nDataONE: a federation of data repositories, https://www.dataone.org/\nre3data: a registry of research data repositories, https://www.re3data.org/\nStarting by searching data in your field and see where other researchers are archiving their data is often a great way to determine which data repository could be a good home for your own data.\nYou as an Author \nIt is important to be able to reference to yourself as a researcher and as an author of your work in a non ambiguous manner. From their website: ORCID is a great way to create a persistent digital identifier (an ORCID iD) that you own and control, and that distinguishes you from every other researcher. ORCID is also more and more use as an authentication system for many services (e.g. data repositories).\nWhat about your computing environment\nSession info\nYour analysis was done with specific versions both of the program used but also of all the packages involved, as well as the specifications of Operating System (OS) that was used. The good use is that there ar tools to let you capture this information in a systematic manner.\n\n\nsessionInfo()\n\n\nR version 4.1.0 (2021-05-18)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Catalina 10.15.7\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] devtools_2.4.2 usethis_2.0.1 \n\nloaded via a namespace (and not attached):\n [1] compiler_4.1.0    highr_0.9         remotes_2.4.0    \n [4] prettyunits_1.1.1 tools_4.1.0       testthat_3.0.4   \n [7] pkgload_1.2.1     digest_0.6.27     pkgbuild_1.2.0   \n[10] downlit_0.2.1     jsonlite_1.7.2    evaluate_0.14    \n[13] memoise_2.0.0     lifecycle_1.0.0   png_0.1-7        \n[16] rlang_0.4.11      cli_3.0.1         rstudioapi_0.13  \n[19] distill_1.2       yaml_2.2.1        xfun_0.24        \n[22] fastmap_1.1.0     withr_2.4.2       stringr_1.4.0    \n[25] knitr_1.33        desc_1.3.0        fs_1.5.0         \n[28] vctrs_0.3.8       rprojroot_2.0.2   glue_1.4.2       \n[31] here_1.0.1        R6_2.5.0          processx_3.5.2   \n[34] fansi_0.5.0       rmarkdown_2.9     bookdown_0.22    \n[37] sessioninfo_1.1.1 purrr_0.3.4       callr_3.7.0      \n[40] magrittr_2.0.1    ps_1.6.0          htmltools_0.5.1.1\n[43] ellipsis_0.3.2    stringi_1.7.3     cachem_1.0.5     \n[46] crayon_1.4.1     \n\nor even better:\n\n\ndevtools::session_info()\n\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Catalina 10.15.7      \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-08-20                  \n\n─ Packages ─────────────────────────────────────────────────────────\n package     * version date       lib source        \n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)\n cachem        1.0.5   2021-05-15 [1] CRAN (R 4.1.0)\n callr         3.7.0   2021-04-20 [1] CRAN (R 4.1.0)\n cli           3.0.1   2021-07-17 [1] CRAN (R 4.1.0)\n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)\n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)\n devtools    * 2.4.2   2021-06-07 [1] CRAN (R 4.1.0)\n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)\n distill       1.2     2021-01-13 [1] CRAN (R 4.1.0)\n downlit       0.2.1   2020-11-04 [1] CRAN (R 4.1.0)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.1.0)\n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)\n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.1.0)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.1.0)\n fs            1.5.0   2020-07-31 [1] CRAN (R 4.1.0)\n glue          1.4.2   2020-08-27 [1] CRAN (R 4.1.0)\n here          1.0.1   2020-12-13 [1] CRAN (R 4.1.0)\n highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)\n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)\n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)\n knitr         1.33    2021-04-24 [1] CRAN (R 4.1.0)\n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.1.0)\n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)\n memoise       2.0.0   2021-01-26 [1] CRAN (R 4.1.0)\n pkgbuild      1.2.0   2020-12-15 [1] CRAN (R 4.1.0)\n pkgload       1.2.1   2021-04-06 [1] CRAN (R 4.1.0)\n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)\n prettyunits   1.1.1   2020-01-24 [1] CRAN (R 4.1.0)\n processx      3.5.2   2021-04-30 [1] CRAN (R 4.1.0)\n ps            1.6.0   2021-02-28 [1] CRAN (R 4.1.0)\n purrr         0.3.4   2020-04-17 [1] CRAN (R 4.1.0)\n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)\n remotes       2.4.0   2021-06-02 [1] CRAN (R 4.1.0)\n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)\n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)\n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)\n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.1.0)\n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)\n stringi       1.7.3   2021-07-16 [1] CRAN (R 4.1.0)\n stringr       1.4.0   2019-02-10 [1] CRAN (R 4.1.0)\n testthat      3.0.4   2021-07-01 [1] CRAN (R 4.1.0)\n usethis     * 2.0.1   2021-02-10 [1] CRAN (R 4.1.0)\n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.1.0)\n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)\n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)\n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)\n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library\n\nYou can save all this content to an session_info.txt file and upload it to your repository.\nIn python, using pip freeze > requirements.txt or conda list --export > requirements.txt will create a text file listing all the libraries (and their versions) used in a specific python environment. You can actually use this file to (re)install all the packages and specific versions into a new python environment. It is also great practice to add this file to your repository.\nContainers\n\n\n\n",
      "last_modified": "2021-08-20T21:55:48-07:00"
    },
    {
      "path": "day1-git_github_recap.html",
      "title": "git and GitHub recap",
      "author": [],
      "contents": "\n\nContents\nVersion Control with git and GitHub\nThe problem with save_as\ngit\nRepository\n\nGitHub\nLet’s look at a repository on GitHub\n\n\n\nVersion Control with git and GitHub\nAka – Say goodbye to script_JB_03v5b.R !!\nThe problem with save_as\n\n\n\nEvery file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control.\nVersion control systems help you track all of the changes to your files, without the spaghetti mess that ensues from simple file renaming. In other words, version control is a system that helps you to manage the different versions of your files in an organized manner. It will help you to never have to duplicate files using save as as a way to keep different versions of a file (see below). Version control help you to create a timeline of snapshots containing the different versions of a file. At any point in time, you will be able to roll back to a specific version. Bonus: you can add a short description (commit message) to remember what each specific version is about.\nWhat is the difference between git and GitHub?\ngit: is a version control software used to track files in a folder (a repository)\ngit creates a timeline or history of your files\n\nGitHub: is a code repository in the cloud that enables users to store their git repositories and share them with others. Github also add many features to manage projects and document your work.\ngit\n\nThis section focuses on the code versioning system called Git. Note that there are others, such as Mercurial or svn for example.\nGit is a free and open source distributed version control system. It has many functionalities and was originally geared towards software development and production environment. In fact, Git was initially designed and developed in 2005 by Linux kernel developers (including Linus Torvalds) to track the development of the Linux kernel. Here is a fun video of Linus Torvalds touting Git to Google.\nHow does it work?\nGit can be enabled on a specific folder/directory on your file system to version files within that directory (including sub-directories). In git (and other version control systems) terms, this “tracked folder” is called a repository (which formally is a specific data structure storing versioning information).\nWhat git is not:\nGit is not a backup per se\nGit is not good at versioning large files (there are workarounds) => not meant for data\nRepository\nGit can be enabled on a specific folder/directory on your file system to version files within that directory (including sub-directories). In git (and other version control systems) terms, this “tracked folder” is called a repository (which formally is a specific data structure storing versioning information).\nAlthough there many ways to start a new repository, GitHub (or any other cloud solutions, such as GitLab) provide among the most convenient way of starting a repository.\n\nGitHub\nGitHub is a company that hosts git repositories online and provides several collaboration features (among which forking). GitHub fosters a great user community and has built a nice web interface to git, also adding great visualization/rendering capacities of your data.\nGitHub.com: https://github.com\nA user account: https://github.com/brunj7\nAn organization account: https://github.com/nceas\nNCEAS GitHub instance: https://github.nceas.ucsb.edu/\nLet’s look at a repository on GitHub\nThis screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes.\n\nIf we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson and seananderson were fixing things in June and July:\n\nAnd finally, if we drill into the changes made on June 13, we can see exactly what was changed in each file:\n Tracking these changes, and seeing how they relate to released versions of software and files is exactly what Git and GitHub are good for. We will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow.\n\n\n\n",
      "last_modified": "2021-08-20T21:55:48-07:00"
    },
    {
      "path": "day1-github_forking.html",
      "title": "Collaborating using GitHub forking",
      "author": [],
      "contents": "\nA fork is a copy of a repository that will be stored under your user account. Forking a repository allows you to freely experiment with changes without affecting the original project. We can create a fork on Github by clicking the “fork” button in the top right corner of our repository web page. \nMost commonly, forks are used to either propose changes to someone else’s project or to use someone else’s project as a starting point for your own idea.\nWhen you are satisfied with your work, you can initiate a Pull Request to initiate discussion about your modifications and requesting to integrate your changes to the main repository. Your commit history allows the original repository administrators to see exactly what changes would be merged if they accept your request. Do this by going to the original repository and clicking the “New pull request” button!\n\n\n\nNext, click “compare across forks”, and use the dropdown menus to select your fork as the “head fork” and the original repository as the “base fork”.\n\nThen type a title and description for the changes you would like to make. By using GitHub’s @mention syntax in your Pull Request message, you can ask for feedback from specific people or teams.\nThis workflow is recommended when you do not have push/write access to a repository, such as contributing to a open source software or R package, or if you are heavily changing a project.\n\n\n\n",
      "last_modified": "2021-08-20T21:55:49-07:00"
    },
    {
      "path": "day1-hands-on_drawings_p2.html",
      "title": "Hands-on: Planning your work",
      "author": [],
      "contents": "\n\nContents\nGood news we found data for one more watershed :)\nThis is your lucky day!!\n\nGood news we found data for one more watershed :)\nActually our team also collected data for an adjacent watershed!!\n\n\n\n=> How should you modify your workflow to handle this new situation? (10min)\nThis is your lucky day!!\nWe found data for additional chemistry measurements while skimming through the database :)\nWe now want to be able to produce the following plot\n\n\n\n=> How should you modify your workflow to handle this new situation? (5min)\n\n\n\n",
      "last_modified": "2021-08-20T21:55:49-07:00"
    },
    {
      "path": "day1-hands-on_drawings.html",
      "title": "Hands-on: Planning your work",
      "author": [],
      "contents": "\n\nContents\nGoal\nA few things to know:\nStudy sites\nOutput\n\nTo Do:\nRemember\nOh wait…\n\nReferences\n\n\nGoal\nDraw the workflow to combine 4 datasets about stream flow and water chemistry in a way that will let you investigate the impact of the 1989 Hurricane Hugo on Stream Chemistry in the Luquillo Mountains of Puerto Rico\nA few things to know:\nTable structures are different => only some variables / columns overlap among the different sites\nUnits used among the various sites are different\nPeriod covered is not perfectly aligned for all the time-series. Some sites start or end before others, but there is a period of overlap\nStudy sites\n\n\n\nOutput\nWe want to be able to produce this plot\n\n\n\nTo Do:\nDraw the data processing steps to harmonize those data into one dataset that will let you compare time-series data and reproduce the above plot (20min)\nRemember\nEach node represents a step or an input/output\nEach connecting edge represents data flow or processing\nOh wait…\nActually here is more\nReferences\nExercise based on: Schaefer, D., McDowell, W., Scatena, F., & Asbury, C. (2000). Effects of hurricane disturbance on stream water concentrations and fluxes in eight tropical forest watersheds of the Luquillo Experimental Forest, Puerto Rico. Journal of Tropical Ecology, 16(2), 189-207. doi:10.1017/S0266467400001358\nData available here: McDowell, W. 2021. Chemistry of stream water from the Luquillo Mountains ver 4923052. Environmental Data Initiative. https://doi.org/10.6073/pasta/ddb4d61aea2fb18b77540e9b0c424684 (Accessed 2021-08-06).\n\n\n\n",
      "last_modified": "2021-08-20T21:55:50-07:00"
    },
    {
      "path": "day1-pseudocode.html",
      "title": "Flow charts and pseudocode",
      "author": [],
      "contents": "\n\nContents\nFlow charts\nPseudocode\nFurther reading\n\nFlow charts\nFlowcharts are useful to visualize and develop analytical workflow. It guides planning and anticipating the various computing and analytical tasks that will be required to complete an analysis. It also helps explaining the different steps to your collaborators and team. You can use a flowchart to spell out the logic behind a analytical workflow before ever starting to code. It can help to organize big-picture thinking and provide a guide when it comes time to code. More specifically, flowcharts can:\nVisualize the sequence of the different phases of the analytical process from data collection to implementing analyses\nBetter define the scope and resources needed both in terms of project data management and computing resources needed\nDiscuss who will be in charge and involved in the development of the different parts of the workflow\nList the products / outputs that will result from your analysis - such as data, codes, publications, web presence, … - and discuss how to best preserve and share them\nThere are conventions on how to use symbols to represent different parts of a workflow (see here for example). Although it is good to be aware of those conventions for sharing a workflow with your community, within a team the most important aspect is to be coherent and stick to this usage overtime.\nThe main benefit for the project is the process and discussion as a team to develop the workflow. It will ensure that everybody is on the same page and has the opportunity to provide inputs on the project. This workflow should be updated regularly as the project evolves.\nA few workflow examples\nPseudocode\nOften, programmers may write pseudocode as a next step, to provide greater detail than the flowchart in terms of processing steps and implementation. It will help you to define where iterations will be needed but also detect repeating blocks that might be well suited to be handled via the development of a function.\nThis technique aims at developing a sequence of pragrammatical steps in a manner that is easy to understand for anyone with basic programming knowledge. Pseudocode can be implemented more or less formally and at various levels of details. One additional advantage of going through this process is that it is agnostic of the tools / programming languages that you will be using to develop your analytical workflow.\nIn this course, we will be focusing on the process more than the exact syntax to use, keeping the level of details at a level that provide more details than a flow chart.\nFurther reading\nWhat is a flowchart: https://www.lucidchart.com/pages/what-is-a-flowchart-tutorial\nHow to write pseudocode: https://www.wikihow.com/Write-Pseudocode\nAn Introduction to Writing Good Pseudocode: https://towardsdatascience.com/pseudocode-101-an-introduction-to-writing-good-pseudocode-1331cb855be7\nHow to write Pseudocode: A beginner’s guide https://blog.usejournal.com/how-to-write-pseudocode-a-beginners-guide-29956242698\n\n\n\n",
      "last_modified": "2021-08-20T21:55:50-07:00"
    },
    {
      "path": "day2-github_branches.html",
      "title": "Topic 8: Dropdown list from a navigation bar item",
      "author": [],
      "contents": "\n\nContents\nCollaborating through write / push access\nBranches\nWorking with branches\nCreating a new branch\nUsing a branch\n\n\n\nCollaborating through write / push access\nWhen you collaborate closely and actively with colleagues, you do not want necessarily to have to review all their changes through pull requests. You can then give them write access (git push) to your repository to allow them to directly edit and contribute to its content. This is the workflow we will recommend to use within your working group.\nAdding collaborators to a repository\nClick on the repository\nOn the top tabs, click \nOn the left pane, click Manage access and click on “Invite a Collaborator” to enter the usernames you want to add\ncollaboratorsUnder this collaborative workflow, we recommend to use git branches combined with pull requests to avoid conflicts and to track and discuss collaborators contributions.\nBranches\nadapted from https://www.atlassian.com/git/tutorials/git-mergeWhat are branches? Well in fact nothing new, as the master is a branch. A branch represents an independent line of development, parallel to the master (branch).\nWhy should you use branches? For 2 main reasons:\nWe want the master to only keep a version of the code that is working\nWe want to version the code we are developing to add/test new features (for now we mostly talk about feature branch) in our script without altering the version on the master.\nWorking with branches\nCreating a new branch\nIn RStudio, you can create a branch using the git tab.\nClick on the branch button\n\n\n\nFill the branch name in the new branch window; in this example, we are going to use test for the name; leave the other options as default and click create\n\n\n\nyou will be directly creating a local and remote branch and switch to it\n\n\n\nCongratulations you just created your first branch!\nLet us check on Github:\n\n\n\nAs you can see, now there are two branches on our remote repository: - master - test\nUsing a branch\nHere there is nothing new. The workflow is exactly the same as we did before, except our commits will be created on the test branch instead of the master branch.\n\n\n\n",
      "last_modified": "2021-08-20T21:55:51-07:00"
    },
    {
      "path": "day2-github_commit_messages.html",
      "title": "On good commit messages",
      "author": [],
      "contents": "\n\nContents\nReferences\n\n\n\n\nFigure 1: https://xkcd.com/1296/\n\n\n\nCommit messages are critical for others, including your future self, to understand the motivation behind the changes that were implemented. Combined into the history of your file and repository, those messages help to understand the content and its evolution over time\nSo what is a good commit message? Well I think the answer actually starts with what is a good commit? Which actually begins when you are choosing how to group files during the staging.\nA good commit:\nIncorporate changes that have one common purpose\nGroup files that need to be kept synchronized or that are working together towards this common purpose\nA good commit message:\nShould be short (50-72 characters max)\nShould be descriptive (what is the goal of those changes)\nUse imperative mood (to match git default messages)\nNote: you can add a body to a commit message if you want to describe the content in greater details, but keep the first line (message) separated.\nReferences\nHow to Write a Git Commit Message: https://chris.beams.io/posts/git-commit/\n\n\n\n",
      "last_modified": "2021-08-20T21:55:52-07:00"
    },
    {
      "path": "day2-handson_github_rstudio.html",
      "title": "GitHub branches using RStudio",
      "author": [],
      "contents": "\n\nContents\nIs your fav’ iconic !?!\nNote\n\nIs your fav’ iconic !?!\nWrite and R script to compare the csv with your favorite desserts you modified previously with the top 42 most iconic American desserts\nCreate a new branch using RStudio\nAdd a new script named desserts_match_myinitials.R\nThis script should read both csv files into R and try to determine if your favorite dessert is part of the most iconic desserts!\nYou can download the iconic data here: https://github.com/brunj7/EDS-214-analytical-workflows/blob/main/data/iconic_desserts.csv\nNote\nHere is how we created the iconic listing\n\n\nlibrary(tidyverse)\nlibrary(rvest)\n\n# read the webpage code\nwebpage <- read_html(\"https://www.eatthis.com/iconic-desserts-united-states/\")\n\n# Extract the desserts listing\ndessert_elements<- html_elements(webpage, \"h2\")\ndessert_listing <- dessert_elements %>% \n  html_text2() %>% # extracting the text associated with this type of elements of the webpage\n  as_tibble() %>% # make it a data frame\n  rename(dessert = value) %>% # better name for the column\n  head(.,-3) %>% # 3 last ones were not desserts \n  rowid_to_column(\"rank\") %>% # adding a column using the row number as proxy for the rank\n  write_csv(\"data/iconic_desserts.csv\") # save it as csv\n\n\n\n\n\n\n",
      "last_modified": "2021-08-20T21:55:53-07:00"
    },
    {
      "path": "day2-handson_github_website.html",
      "title": "GitHub branches and more using GitHub website",
      "author": [],
      "contents": "\n\nContents\nOur asks\nPrompt\n\nIn this section we will be using the GitHub.com website and show you do not need to be a programmer to use version control and edit files on GitHub, nor using branches\nChecks: If you have not already created a GitHub username, please do so now: - GitHub: https://github.com - Follow optional advice on choosing your username\nOur asks\nAs a Team of two: - Help each other, everyone is bringing different skills! Talk it out! - Listen to each other; avoid judgment and solutioneering. - Have fun!\nPrompt\nWe want to log the information about people favorite dessert using a repository.\nPerson 1 (owner):\nCreate a repository using the following these instructions steps 1-6\n\nAdd Person 2 as collaborator following these instructions\nEdit the README to:\nReplace the title (first line starting with #) with something better! Maybe Favorite Desserts\nAdd your name and your favorite dessert below the title: e.g. - Julien: crepes\n\n\n\n\nPerson 2: Create a branch\nCheck your email to accept the invitation\nGo to the repository website (link is provided in the invitation)\nCreate a new branch named after your first name\n\n\n\nStart editing the README.md by clicking on pen at the top of the README.md file to edit it\n\n\n\nAdd your name and your favorite dessert below the title: e.g. - Sophia: chocolate\nAdd a descriptive message\n\n\n\nCommit your changes\n\n\n\nCreate a pull request to merge those changes to the main repository\nPerson 1: add a file\nDownload this csv file about your favorite desserts to your computer\nJust drag and drop it on the Github web page of your repository to upload it\nAdd a short message about the file e.g. Adding dessert csv & hit Commit changes\nYour have has been uploaded. Click on the filename to see it!\nYou should have something similar to this repo: https://github.com/brunj7/favorite-desserts\nBonus\nPerson 2: Try to edit the csv file directly on GitHub!\nPerson 1: Your turn!\nNo need to be a programmer to contribute to analytical workflows with GitHub!!\n\n\n\n",
      "last_modified": "2021-08-20T21:55:53-07:00"
    },
    {
      "path": "day2-projects_team.html",
      "title": "Managing data-driven projects as a team",
      "author": [],
      "contents": "\n\nContents\nManaging your files as a team\nNaming things\nExercise\n\nOrganizing things\nCode\nScripting languages\nStructure of a script\nA few programming practices that will help a long way\nNotebooks and scripts\n\nData\n\n\nManaging your files as a team\nOur goal is to centralize the management of your files (data, codes, …). Try to avoid having data sets spread among laptops or other personal computers; this makes it difficult for other team members to redo a particular analysis and it can become difficult to know which version of the data was used for a specific analysis. We recommend asking your institution if there are servers or cloud services available to you and use those tools to centralize your data management. This will also make sure that all your collaborators will be able to access the same version of the data using the same path.\nThere are many tools out there to help with file management. Here are a few questions to ask your teammates when organizing your project:\nCan everybody have access to this tool? This should overrule the “best” tool => maximize adoption\nWhat team practices should you set on how to use these tools? Example: naming convention for files\nAllow flexibility – acknowledge the technological level varies among collaborators. Empower them by showing how to best use these tools rather than doing it for them!\nNaming things\n\nDevelop naming conventions for files and folder:\nAvoid spaces (use underscores or dashes)\nAvoid punctuation or special characters\nTry to leverage alphabetical order (e.g. start with dates: 2020-05-08)\nUse descriptive naming (lite metadata)\nUse folders to structure/organize content\nKeep it simple\nMake it programmatically useful:\nUseful to select files (Wildcard *, regular expression)\nBut don’t forget Humans need to read file names too!!\n\nExercise\nWhich filename would be the most useful?\n06-2020-08-sensor2-plot1.csv\n2020-05-08_light-sensor-1_plot-1.csv\nMeasurement 1.csv\n2020-05-08-light-sensor-1-plot-2.csv\n2020-05-08-windSensor1-plot3.csv\n** The most important is to make it consistent! **\nGood reference on this topic from Jenny Bryan (RStudio).\nOrganizing things\nAs we discussed previously, it is good practice to encapsulate your project and repositories are a good unit to start with. In this section we will talk about the case when your data sets start to be numerous or large enough that it is not possible anymore to keep them in your repository. Generally when you reach this amount of data to deal with, it also means that your personal computer might not be the best computer to efficiently process those data sets. Thus using a remote server (on premise or in the cloud) might become necessary.\nCodes should be managed using version controlled (git and GitHub for this course)   The repository should be stored in your home folder and the management of the different contributions should be resolved using git and GitHub and not by sharing directly the scripts. The main reason is that git does already the work for us by tracking the changes between the various versions, but also tracking which collaborator has made those changes and when.\nData sets should should be centralized in a shared folder that is available to all   It is very rare in an Environmental Data Science project that you will discover all the data you need from the start. In addition, a lot of environmental data sets are time-series, and one year later might need to be updated as your project progresses. Our goal here is to setup ourselves in a way that will help us to avoid duplication of data. Every time you duplicate a data set (for example on your own laptop), there is a risk that at some point it will become its own version.\nCode\nVersion control systems have been originally designed to track changes by rows in small text files, in other words they are well suited to manage codes. It is thus recommended to use them to track changes that you and your collaborators are making to the various scripts of your project. Mote on how to best do this in the section below.\nScripting languages\nCompared to other programming languages (such as C, fortran, …), scripting languages are not required to be compiled to be executable. One consequence is that, generally, scripts will execute more slowly than a compiled executable program, because they need an interpreter. However, the more natural language oriented syntax of scripts make them easier to learn and use. In addition, numerous libraries are available to streamline scientific analysis.\nStructure of a script\nA script can be divided into several main sections. Each scripting language has its own syntax and style, but these main components are generally accepted:\nFrom the top to the bottom of your script:\nSummary explaining the purpose of the script\nAttribution: authors, contributors, date of last update, contact info\nImport of external modules / packages\nConstant definitions (g = 9.81)\nFunction definitions (ideally respecting the order in which they are called)\nMain code calling the different functions\nA few programming practices that will help a long way\nComment your code. This will allow you to inform your collaborators (but also your future self!) about the tasks your script accomplishes\nUse variables and constants instead of repeating values in different places of the code. This will let you update those values more easily\nChoose descriptive names for your variables and functions, not generic ones. If you store a list of files, do not use x for the variable name, use instead files. Even better use input_files if you are listing the files you are importing.\nBe consistent in terms of style (input_files, inputFiles,…) used to name variables and functions. Just pick one and stick to it!\nkeep it simple, stupid (KISS). Do not create overly complicated or nested statements. Break your tasks in several simple lines of code instead of embedding a lot of executions in one (complicated line). It will save you time while debugging and make your code more readable to others\nGo modular! Break down tasks into small code fragments such as functions or code chunks. It will make your code reusable for you and others (if well documented). Keep functions simple; they should only implement one or few (related) tasks\nDon’t Repeat Yourself (DRY). If you start copy/pasting part of your code changing a few parameters => write a function and call it several times with different parameters. Add flow control such as loops and conditions. It will be easier to debug, change and maintain\nTest your code. Test your code against values you would expect or computed with another software. Try hedge cases, such as NA, negative values, ….\nIterate with small steps, implement few changes at a time to your code. Test, fix, and move forward!\nWe hope this overview section about scientific programming has raised your interest in learning more about best practices and tools for developing reproducible workflows using scripting languages.\nNotebooks and scripts\nWith the increasing popularity of notebooks in scientific projects, it can sometimes be confusing to know when to use one. The good news is that there is not really a wrong or right here and that actually both can be used in a complementary manner and each data scientist will have her/his preference. However here are a few tips to help you decide:\nKeep your Notebook at a length you will feel comfortable reading. It if starts to be a long it might be time to think about if some code could be move to another document (scripts or another notebook)\nScripts might be better suited for tasks you need to rerun frequently\nIf you have developed many functions for your analysis, it might be worth storing them in a script outside your main notebook\nData\nIt is recommended to keep the raw-data you are collecting separated from any data you might generate at various steps of your workflow. This will help you to trace back any problems, but also make your work more reproducible because you started your processing directly from the original data. There are different ways of ensuring this. A common one is to create a raw-data (sub)folder to store the data. You can even play with the file access settings to make those files read-only.\nhttps://github.com/benmarwick/rrtools\n\n\n",
      "last_modified": "2021-08-20T21:55:54-07:00"
    },
    {
      "path": "day3-cli_advanced.html",
      "title": "Working on a remote server",
      "author": [],
      "contents": "\n\nContents\nAdvanced Topics at the command line\nConnecting to a remote server via ssh\nUnix systems are multi-user\nGetting help\nfinding stuff\nGetting things done\nSome useful, special commands using the Control key\nProcess management\nWhat about “space”\nHistory\n\nA sampling of simple commands for dealing with files\nGet into the flow, with pipes\nText editing\nSome editors\n\nSearching advanced utilities\ngrep\nLet’s look at our text file\n\nCreate custom commands with “alias”\n\nAknowledgements\n\nAdvanced Topics at the command line\nWe will not cover this during class, it is for your reference. You will also have the opportunity to further practice and learn about the command line in EDS-215.\nConnecting to a remote server via ssh\nFrom the gitbash (MS Windows) or the terminal (Mac) type:\nssh taylor.bren.ucsb.edu\nYou will be prompted for your username and password.\naurora_sshYou can also directly add your username:\nssh brun@taylor.bren.ucsb.edu\nIn this case, you will be only asked for your password as you already specified which user you want to connect with.\nUnix systems are multi-user\nWho else is logged into this machine? who\nWho is logged into “this shell”? whoami\nGetting help\n<command> -h, <command> --help\nman, info, apropos, whereis\nSearch the web!\nfinding stuff\nShow me my Rmarkdown files!\nfind . -iname '*.Rmd'\nWhich files are larger than 1GB?\nfind . -size +1G\nWith more details about the files:\nfind . -size +1G -ls\nGetting things done\nSome useful, special commands using the Control key\nCancel (abort) a command: Ctrl-c Note: very different than Windows!!\nStop (suspend) a command: Ctrl-z\nCtrl-z can be used to suspend, then background a process\nProcess management\nLike Windows Task Manager, OSX Activity Monitor\ntop, ps, jobs (hit q to get out!)\nkill to delete an unwanted job or process\nForeground and background: &\nWhat about “space”\nHow much storage is available on this system? df -h\nHow much storage am “I” using overall? du -hs <folder>\nHow much storage am “I” using, by sub directory? du -h <folder>\nHistory\nSee your command history: history\nRe-run last command: !! (pronounced “bang-bang”)\nRe-run 32th command: !32\nRe-run 5th from last command: !-5\nRe-run last command that started with ‘c’: !c\nA sampling of simple commands for dealing with files\nwc count lines, words, and/or characters\ndiff compare two files for differences\nsort sort lines in a file\nuniq report or filter out repeated lines in a file\nGet into the flow, with pipes\nstdin, stdout, stderr$ ls *.png | wc -l\n$ ls *.png | wc -l > pngcount.txt\n$ diff <(sort file1.txt) <(sort file2.txt)\n$ ls foo 2>/dev/null\nnote use of * as character wildcard for zero or more matches (same in Mac and Windows)\n? matches single character; _ is SQL query equivalent\nText editing\nSome editors\nvim\nemacs\nnano\n$ nano .bashrc\nSearching advanced utilities\ngrep search files for text\nsed filter and transform text\nfind advanced search for files/directories\ngrep\nShow all lines containing “bug” in my R scripts\n$ grep bug *.R\nNow count the number of occurrences per file\n$ grep -c bug *.R\nPrint the names of files that contain bug\n$ grep -l bug *.R\nLet’s look at our text file\ncat print file(s)\nhead print first few lines of file(s)\ntail print last few lines of file(s)\nless “pager” – view file interactively (type q to quit command)\nod --t “octal dump” – to view file’s underlying binary/octal/hexadecimal/ASCII format\n$ brun@aurora:~/data$ head -3 env.csv\nEnvID,LocID,MinDate,MaxDate,AnnPPT,MAT,MaxAT,MinAT,WeatherS,Comments\n1,*Loc ID,-888,-888,-888,-888,-888,-888,-888,-888\n1,10101,-888,-888,-888,-888,-888,-888,-888,-888\n\n$ brun@aurora:~/data$ head -3 env.csv | od -cx\n0000000   E   n   v   I   D   ,   L   o   c   I   D   ,   M   i   n   D\n           6e45    4976    2c44    6f4c    4963    2c44    694d    446e\n0000020   a   t   e   ,   M   a   x   D   a   t   e   ,   A   n   n   P\n           7461    2c65    614d    4478    7461    2c65    6e41    506e\n0000040   P   T   ,   M   A   T   ,   M   a   x   A   T   ,   M   i   n\n           5450    4d2c    5441    4d2c    7861    5441    4d2c    6e69\n0000060   A   T   ,   W   e   a   t   h   e   r   S   ,   C   o   m   m\n           5441    572c    6165    6874    7265    2c53    6f43    6d6d\n0000100   e   n   t   s  \\r  \\n   1   ,   *   L   o   c       I   D   ,\n           6e65    7374    0a0d    2c31    4c2a    636f    4920    2c44\n0000120   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -\n           382d    3838    2d2c    3838    2c38    382d    3838    2d2c\n0000140   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -   8\n           3838    2c38    382d    3838    2d2c    3838    2c38    382d\n0000160   8   8   ,   -   8   8   8  \\r  \\n   1   ,   1   0   1   0   1\n           3838    2d2c    3838    0d38    310a    312c    3130    3130\n0000200   ,   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,\n           2d2c    3838    2c38    382d    3838    2d2c    3838    2c38\n0000220   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -\n           382d    3838    2d2c    3838    2c38    382d    3838    2d2c\n0000240   8   8   8   ,   -   8   8   8  \\r  \\n\n           3838    2c38    382d    3838    0a0d\nCreate custom commands with “alias”\nalias lwc='ls *.jpg | wc -l'\nYou can create a number of custom aliases that are available whenever you log in, by putting commands such as the above in your shell start-up file, e.g. .bashrc\nAknowledgements\nThis section reuses a lot of materials from NCEAS Open Science for Synthesis (OSS) intensive summer schools and other training. Contributions to this content have been made by Mark Schildhauer, Matt Jones, Jim Regetz and many others.\n\n\n\n",
      "last_modified": "2021-08-20T21:55:54-07:00"
    },
    {
      "path": "day3-remote_server.html",
      "title": "Working on a remote server",
      "author": [],
      "contents": "\n\nContents\nLearning Objectives\nWhy working on a remote machine?\nCollaborative Setup\nWhat does working on a remote server means?\nRStudio Server\nConnecting to MEDS Analytical Server\nFile structure\nR packages\n\n\nCommand line\nIntroduction to UNIX and its siblings\nSome Unix hallmarks\n\nThe Command Line Interface (CLI)\nWhy the CLI is worth learning\nThe Terminal from RStudio\nNavigating and managing files/directories in *NIX\nPermissions\nExercise\n\nGeneral command syntax\n\nUploading Files to a server\nRStudio\nsFTP Software\nscp\nYour turn\n\n\nAdvanced Topics at the command line\nAknowledgements\n\nLearning Objectives\nIn this lesson, you will learn:\nHow to connect to a remote server\nGet familiar with RStudio server\nGet a short introduction to the command line (CLI)\nWhy working on a remote machine?\nOften the main motivation is to scale your analysis beyond what a personal computer can handle. R being pretty memory intensive, moving to a server often provides you more RAM and thus allows to load larger data in R without the need of slicing your data into chunks. But there are also other advantages, here are the main for scientist:\nPower: More CPUs/Cores (24/32/48), More RAM (256/384GB)\nCapacity: More disk space and generally faster storage (in highly optimized RAID arrays)\nSecurity: Data are spread across multiple drives and have nightly backups\nCollaboration: shared folders for code, data, and other materials; same software versions\n=> The operating system is more likely going to be Linux!!\nCollaborative Setup\nThere are additional reasons of particular importance in a collaborative set up, such as a working group: - Centralizing data management: As you know synthesis science is data intensive and often require to deal with a large number of heterogeneous data files. It can be complicated to make sure every collaborators as access to all the data they need. It is even harder to ensure that the exact same version of the data is used by everyone. Moving your workflow to a server, will allow to have only one copy of the data that you can share with all your collaborators. Even better, since everyone can access the same data, everybody will have the exact same path in their script!! - Make sure your files are safe: Generally, servers are managed by a System Administrator. This person is in charge of keeping the server up-to-date, secured from malwares and set up back up strategies to ensure all the files on the server are backed up. When using cloud solutions, you should always check if a back up plan is available for the resources your using.\nWhat does working on a remote server means?\nWhat does it mean for your workflow? The good news is that RStudio Server makes it very easy for RStudio users to start using a server for their analysis. The main changes are about: - File management: you will need to learn to move files (including your R scripts) to the server - Package installation: You can still install the R packages you need under your user (with some limitations). However some R packages will be already installed at the server level.\nRStudio Server\nFrom an user perspective, RStudio Server is your familiar RStudio interface in your web browser. The big difference however is that with RStudio Server the computation will be running on the remote machine instead of your local personal computer. This also means that the files you are seeing through the RStudio Server interface are located on the remote machine. And this also include your R packages!!! This remote file management is the main change you will have to adopt in your workflow.\nTo help with remote files management, the RStudio Server interface as few additional features that we will be discussing in the following sections.\n\n\n\nConnecting to MEDS Analytical Server\nGot to: https://taylor.bren.ucsb.edu/\nEnter your credentials\nYou are in!\n\n\n\nClick on the New Session button. You can see that you are able to start both an R (Studio) and jupyter notebook session. Let’s take a few minutes to experiment with the different options.\nFor this session, we are going to select the RStudio option and hit Start Session.\n\n\n\nYou should now see a very familiar interface :) Except it is running on the server with a lot of resources at your fingertips!!\nFile structure\nLet’s explore explore a little bit the file structure on the server. By default on a Linux server, you are located in the home folder. This folder is only accessible to you and it is where you can store your personal files on a server. You should see 2 folders: R and H\n\n\n\nThe R folder is where your local R packages will be installed, you can ignore it. The H is your H drive that the Bren School is offering to all its students. If you click on it you should see any files you have uploaded there.\nLet us make a folder named github by click on the New Folder button at the top of the tab. We will use this folder (also named directory in linux/unix terms) to clone any GitHub repository.\nR packages\nIf we go to the Packages tab, we can see a long list of packages that have already be installed by our system administrator (Brad). Those packages have been installed server wide, meaning that all the users have access to them.\n\n\n\nA user can also installed her/his own packages. Let’s try to install the remote package that lets you install R packages directly from GitHub: install.packages(\"remotes\"). Once done, note a new section that appeared on the Packages tab named User Library. Each of us have now its own copy of the package installed (in this R folder we were talking about a few minutes ago).\n\n\n\nA few notes:\nIn this example we will have made a better choice to have the remotes package installed once at the system level\nSome R packages depend on external libraries that need to be installed on the server. Those libraries will have to be installed by the system administrator first before you can install the R package\nInstalling an R package on a linux machine generally requires compilation of the code and will thus take more time to install than when you install it from pre-compiled binaries\nLook now inside you R folder!!\nCommand line\nIntroduction to UNIX and its siblings\nUNIX\nOriginally developed at AT&T Bell Labs circa 1970. Has experienced a long, multi-branched evolutionary path\n\nPOSIX (Portable Operating System Interface)\na set of specifications of what an OS needs to qualify as “a Unix”, to enhance interoperability among all the “Unix” variants\n\nVarious Unices\nThe unix family treeOS X\nis a Unix!\n\n\nLinux\nis not fully POSIX-compliant, but certainly can be regarded as functionally Unix\n\n\nSome Unix hallmarks\nSupports multi-users, multi-processes\nHighly modular: many small tools that do one thing well, and can be combined\nCulture of text files and streams\nPrimary OS on HPC (High Performance Computing Systems)\nMain OS on which Internet was built\nThe Command Line Interface (CLI)\nThe CLI provides a direct way to interact with the Operating System, by typing in commands.\nWhy the CLI is worth learning\nTypically much more extensive access to features, commands, options\nCommand statements can be written down, saved (scripts!)\nEasier automation\nMuch “cheaper” to do work on a remote system (no need to transmit all the graphical stuff over the network)\nThe Terminal from RStudio\nYou can access the command line directly from RStudio by using the Terminal tab next to your R console.\n\nNavigating and managing files/directories in *NIX\npwd: Know where you are\nls: List the content of the directory\ncd: Go inside a directory\n~ : Home directory\n. : Here (current directory)\n..: Up one level (upper directory)\ngo to my “Home” directory: cd ~\ngo up one directory level: cd ..\nlist the content: ls\nlist the content showing hidden files: ls -a note that -a is referred as an option (modifies the command)\nMore files/directories manipulations:\nmkdir: Create a directory\ncp: Copy a file\nmv: Move a file\nrm /rmdir: Remove a file / directory use those carefully, there is no return / Trash!!\nPermissions\nAll files have permissions and ownership.\nFile permissionsChange permissions: chmod\nChange ownership: chown\nList files showing ownership and permissions: ls -l\nbrun@taylor:/courses/EDS214$ ls -l\ntotal 16\ndrwxrwxr-x+ 3 brun      esmdomainusers 4096 Aug 20 04:49 data\ndrwxrwxr-x+ 2 katherine esmdomainusers 4096 Aug 18 18:32 example    \nClear contents in terminal window: clear\nExercise\nNavigate to the repo we created and inspect its content using the CLI\nNote: typing is not your thing? the <tab> key is your friend! One hit it will auto-complete the file/directory/path name for you. If there are many options, hit it twice to see the options.\nGeneral command syntax\ncommand [options] [arguments]\nwhere command must be an executable file on your PATH * echo $PATH\nand options can usually take two forms * short form: -a * long form: --all\nYou can combine the options:\nls -ltrh\nWhat do these options do?\nman ls\nTip: hit q to exit the help!\nUploading Files to a server\nYou have several options to upload files to the server. Some are more convenient if you have few files, like RStudio interface, some are more built for uploading a lot of files at one, like specific software… and you guessed it the CLI :)\nRStudio\nYou can only upload one file at the time (you can zip a folder to trick it):\n\nsFTP Software\nAn efficient protocol to upload files is FTP (File Transfer Protocol). The s stands for secured. Any software supporting those protocols will work to transfer files.\nWe recommend the following free software:\nMac users: cyberduck\nWindows: WinSCP\n\nscp\nThe scp command is another convenient way to transfer a single file or directory using the CLI. You can run it from taylor or from your local computer. Here is the basic syntax:\nscp <\/source/path> <hostname:/path/to/destination/>\nHere is an example of my uploading the file 10min-loop.R to taylor from my laptop. The destination directory on taylor is /home/brun/:\nscp 10min-loop.R brun@taylor.bren.ucsb.edu:/home/brun/\n BTW try to open and run that script for fun!!\nIf you want to upload an entire folder, you can add the -r option to the command. The general syntax is:\nscp -r /path/to/source-folder user@server:/path/to/destination-folder/\nHere is an example uploading all the images in the myplot folder\nscp -r myplot brun@taylor.bren.ucsb.edu:/home/brun/plots\nYour turn\nUpload files from your local machine to the server using the different techniques mentioned above, You can download the 10min-loop.R file to your local machine from https://aurora.nceas.ucsb.edu/~brun/10min-loop.R\nAdvanced Topics at the command line\nWe will not cover this during class, it is for your reference. You will also have the opportunity to further practice and learn about the command line in EDS-215.\nAdvanced topics CLI\nAknowledgements\nThis section reuses a lot of materials from NCEAS Open Science for Synthesis (OSS) intensive summer schools and other training. Contributions to this content have been made by Mark Schildhauer, Matt Jones, Jim Regetz and many others.\n\n\n\n",
      "last_modified": "2021-08-21T08:45:11-07:00"
    },
    {
      "path": "day4-documenting.html",
      "title": "Documenting things",
      "author": [],
      "contents": "\n\nContents\nThe power of README\nMaking your code readable\nComments\nHeader\nInline\nFunctions\n\nLeveraging Notebooks\nA few thoughts on Notebooks and Scripts\n\nHands-on\nMetadata\nHuman readable\nMachine readable\nData provenance & semantics\n\nLicensing\nFurther Reading\n\nThe power of README\nREADME are not a new thing. They have been around computer projects since the early days. One great thing about the popularization of supporting the markdown syntax and its web rendering in most code repositories, is that you can move beyond a simple text file and start to present a compelling entry point to your project that can link to various parts and external resources.\nMaking your code readable\nIt is important to make your code easy to read if you hope that others will reuse it. It starts with using a consistent syntax withing your scripts (at least within a project).\nHere is a good style guide for R: http://adv-r.had.co.nz/Style.html\nSyle guide for Python: https://www.python.org/dev/peps/pep-0008/\n\nimport this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\nThere is also the visual aspect of the code that should not be neglected. Like a prose, if you receive a long text without any paragraphs, you might be not very excited about reading it. Indentation, spaces, and empty lines should be leveraged to make a script visually inviting and easy to read. The good news is that most of the Integrated Development Environment (IDE) will help you to do so by auto formatting your scripts according to conventions. Note that also a lot of IDEs, such as RStudio, rely on some conventions to ease the navigation of scripts and notebooks. For example, try to add four - or # after a line starting with one or several # in an R Script!\nComments\nReal Programmers don’t comment their code. If it was hard to write, it should be hard to understand.\n_Tom Van Vleck, based on people he knew (https://multicians.org/thvv/realprogs.html)_\nJoke aside, it is really hard to comment too much your code, because even steps that might seem trivial today might not be so anymore in a few weeks or months for now. In addition, a well commented code is more likely to be read by others. Note also that comments should work in complement of the code and should not being seen as work around vague naming conventions of variables or functions.\n\n\nx <- 9.81  #  gravitational acceleration\n\ngravity_acc <- 9.81  # earth gravitational acceleration\n\n\n\nHeader\nIt is good to add a header to your script that will provide basic information such as:\nPurpose of the script (Long title style)\nWho are the authors\nA contact email\nOptional:\nA longer description about the script purpose\nA starting date and potentially last updated one, although this information becomes redundant with repository information\nNote that R Studio does something similar by default when creating an new R Markdown document!\nInline\nIt does not matter if you are using a script or notebook. It is important to provide comments along your code to complement it by:\nexplaining what the code does\ncapturing decisions that were made on the analytical side. For example, why a specific value was used for a threshold.\nspecifying when some code was added to handel an edge case such as an unexpected value in the data (so a newuser doesn’t have to guess what does lines of code and might want to delete them assuming it is not necessary)\nOther thoughts:\nIt is OK to state (what seems) the obvious\nTry to keep it to the point\nFunctions\nLeveraging Notebooks\nA few thoughts on Notebooks and Scripts\nHands-on\nLet’s try to improve the readability and documentation of this repository: https://github.com/brunj7/better-comments\nFollow the instructions on the README\nMetadata\nHuman readable\nMachine readable\nData provenance & semantics\nLicensing\nIt is a good practice to add a license to a repository / project. It will help to clarify what are the expectations regarding using and potentially contributing to this work.\nHere is a good website to choose a license:\nHere is also good set of instructions on how to make this happen on a GitHub repository: https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/licensing-a-repository\nNote that for content (such as this course), there is also another type of licensing that can be used: https://creativecommons.org/licenses/\nFurther Reading\nPython Hichhiker’s guide: https://docs.python-guide.org/writing/style/\nRoxygen2: https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html\nIntroduction to Software Engineering by Jason Coposky: https://github.com/NCEAS/training/blob/master/2014-oss/day-09/IntroductionToSoftwareClass.pdf\nFunctional programming in R: http://adv-r.had.co.nz/Functional-programming.html#functional-programming\nScoping in R: http://adv-r.had.co.nz/Functions.html\nhttps://www.quora.com/What-is-the-difference-between-programming-languages-markup-languages-and-scripting-languages\nhttp://stackoverflow.com/questions/17253545/scripting-language-vs-programming-language\n\n\n\n",
      "last_modified": "2021-08-20T21:55:58-07:00"
    },
    {
      "path": "group_project.html",
      "title": "Group Project",
      "author": [],
      "contents": "\n\nContents\nGoal\nData\nChemistry of stream water from the Luquillo Mountains\n\nReference\n\nWe are going to build on the exercise we used the first day and work on implementing and refining the workflow we developed during that session.\nHere are the expectations: - By group of 3-4 collaborators - Find an awesome name for your group (one of the hardest step) - Setup a shared GitHub repository - Use the MEDS server Taylor as your main computing resource - Use GitHub to manage your code development in a collaborative manner - Use shared folder on Taylor to manage your data /courses/EDS214/my_group_name - Document your work as you go!! - Use GitHub issues to track your work and discuss progress and tasks\nGoal\nInvestigating the impact of Hurricane Hugo on Stream Chemistry data in Luquillo\nThe Luquillo Experimental Forest (LEF) has been a center of tropical forestry research for nearly a century. In addition, the LEF is a recreation site for over a half a million people per year, a water supply for approximately 20% of Puerto Rico’s population, a regional center for electronic communication, and a refuge of Caribbean biodiversity. It is the goal of the USDA Forest Service and the University of Puerto to promote and maintain the forest’s role as a center of active and dynamic scientific inquiry. However, to maintain the ecological integrity of the forest while balancing the many demands placed upon it’s resources, certain protocol is required. This guide provides the major protocols that govern research in the LEF. These protocols are designed to help researchers protect the forests, obey the law, create an amiable and non-discriminatory work environment, and provide a historical record for future scientists\nHere for more information about the Luquillo site: https://lternet.edu/site/luquillo-lter/\nYou are encouraged to define you own question around this topic as a team. To help you to get started, you could work towards reproducing this plot and extend the time-series to the latest available data:\n\n\n\nData\nData are available from the Environmental Data Initiative (EDI) that is hosting most of the data of the Long Term Ecological Research (LTER) Network.\nChemistry of stream water from the Luquillo Mountains\nMcDowell, W. 2021. Chemistry of stream water from the Luquillo Mountains ver 4923052. Environmental Data Initiative. https://doi.org/10.6073/pasta/ddb4d61aea2fb18b77540e9b0c424684\nReference\nSchaefer, D., McDowell, W., Scatena, F., & Asbury, C. (2000). Effects of hurricane disturbance on stream water concentrations and fluxes in eight tropical forest watersheds of the Luquillo Experimental Forest, Puerto Rico. Journal of Tropical Ecology, 16(2), 189-207. doi:10.1017/S0266467400001358\n\n\n\n",
      "last_modified": "2021-08-20T21:55:58-07:00"
    },
    {
      "path": "index.html",
      "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
      "description": "This course will introduce students to conceptual organization of workflows as a way to conduct reproducible analyses",
      "author": [],
      "contents": "\n\nContents\nInstructor\nTutor\nImportant links\nCourse description\nPredictable daily schedule\nLearning objectives\nSessions (subject to change)\nCourse requirements\nComputing\nTextbook\n\n\n\n\n\nFigure 1: Workflow example using the tidyverse. Note the program box around the workflow and the iterative nature of the analytical process described. Source: R for Data Science https://r4ds.had.co.nz/\n\n\n\nInstructor\nJulien Brun (brun@nceas.ucsb.edu)\nTutor\nCasey O’Hara (cohara@bren.ucsb.edu)\nImportant links\nCourse syllabus\nCode of Conduct\nCourse description\nThe generation and analysis of environmental data is often a complex, multi-step process that may involve the collaboration of many people. Increasingly tools that document and help to organize workflows are being used to ensure reproducibility, shareability, and transparency of the results. This course will introduce students to the conceptual organization of workflows (including code, documents, and data) as a way to conduct reproducible analyses. These concepts will be combined with the practice of various software tools and collaborative coding techniques to develop and manage multi-step analytical workflows as a team.\nPredictable daily schedule\nCourse dates: Monday (2021-08-02) - Friday (2021-08-06)\nEDS 214 is an intensive 1-week long 2-unit course. Students should plan to attend all scheduled sessions. All course requirements will be completed between 8am and 5pm PST (M - F), i.e. you are not expected to do additional work for EDS 214 outside of those hours, unless you are working with the Teaching Assistant in student hours.\nTentative daily schedule (subject to change):\nTime (PST)\nActivity\n9:00am - 10:00am\nLecture 1 (60 min)\n10:00am - 10:10am\nBreak 1 (10 min)\n10:10am - 11:30am\nInteractive Session 1 (80 min)\n11:30am - 12:00am\nFlex time (30 min)\n12:00am - 1:15pm\nLunch (75 min)\n1:15pm - 2:00pm\nLecture 2 (45 min)\n2:00pm - 2:10pm\nBreak 2 (10 min)\n2:10pm - 3:10pm\nInteractive Session 2 (60 min)\n3:10pm - 3:20pm\nBreak 3 (10 min)\n3:20pm - 4:30pm\nGroup projects and/or flex time (70 min)\nLearning objectives\nThe goal of EDS 214 (Analytical Workflows and Scientific Reproducibility) is to expose incoming MEDS students to “good enough” practices of scientific programming develop skills in environmental data science to produce reproducible research. By the end of the course, students should be able to:\nDevelop knowledge in scientific analytical workflows To learn how to make your data-riven research reproducible, it is important to develop scientific workflows that will be relying on programming to accomplish the necessary tasks to go from the raw data to the results of your analysis (figures, new data, publications, …). Scripting languages, even better open ones such as R and python, are well-suited for scientists to develop reproducible scientific workflows, but are not the only tools you will need to develop reproducible and collaborative workflows\nLearn how to code in a collaborative manner by practicing techniques such as code review and pair programming. Become comfortable asking for and conducting code review using git and GitHub to create pull request, ask feedback from peers, and merge changes into the main repository. Practice pair programming to cement the collaborative development of reproducible analytical workflows\nPractice documenting code and data in a systematic way that will enable your collaborators, including your future self, to understand and reuse your work\nSessions (subject to change)\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"Monday 8/23: morning\",\"Monday 8/23: afternoon\",\"Tuesday 8/24: morning\",\"Tuesday 8/24: afternoon\",\"Wednesday 8/25: morning (late start)\",\"Wednesday 8/25: afternoon\",\"Thursday 8/26: morning\",\"Thursday 8/26: afternoon\",\"Friday 8/27: morning\",\"Friday 8/27 afternoon\"],[\"Repoducible workflows\",\"Collaborating with Github 101\",\"Collaborating with Github 102\",\"Organizing your project\",\"Working on a remote server\",\"Working on a remote server\",\"Code review and pair programming\",\"Documenting things\",\"Sharing things\",\"Presentations\"],[\"Planning things: from diagrams to pseudo code\",\"Forking and Pull Requests\",\"Branches and issues\",\"Notebooks, scripts, data and more\",\"RStudio server &amp; the command line\",\"Organizing projects on a server: how to manage data and codes\",\"Team project\",\"Team project\",\"Team project\",\"Team project\"],[\"the Markdown syntax\",\"Comparing R and python approaches to notebooks\",\"Open\",\"MEDS IT team (Brad &amp; Kat) -- Computing ressources\",\"No flex sessions - Wednesday late start\",\"Debugging with RStudio\",\"Meet your next instructor  --  Jim Frew,  Bren School, UCSB\",\"Capstone project proposal --  Jamie\",\"Xaringan\",\"Work on your online presence (GitHub profile and distill website)\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>Day / Session<\\/th>\\n      <th>Topics<\\/th>\\n      <th>Interactive Sessions<\\/th>\\n      <th>Flex Sessions<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nCourse requirements\nComputing\nMinimum MEDS device requirements\nHave a ready to be used GitHub Account (https://github.com/)\nTextbook\nR for Data Science: https://r4ds.had.co.nz/\nThe Practice of Reproducible Research: http://www.practicereproducibleresearch.org/\n\n\n\n",
      "last_modified": "2021-08-20T21:56:01-07:00"
    },
    {
      "path": "README.html",
      "author": [],
      "contents": "\n\nContents\nEDS 214: Analytical Workflows and Scientific Reproducibility\nInstructor\nCourse description\n\n\nEDS 214: Analytical Workflows and Scientific Reproducibility\nInstructor\nJulien Brun (brun@nceas.ucsb.edu)\nCourse description\nThe generation and analysis of environmental data is often a complex, multi-step process that may involve the collaboration of many people. Increasingly tools that document help to organize and document workflows are being used to ensure reproducibility and transparency of the results. This course will introduce students to conceptual organization of workflows as a way to conduct reproducible analyses, as well as various software tools that help users to manage multi-step processes that requires tools for storing, managing and sharing workflows, code, documents and data.\n\nThis website template is made with distill by RStudio as an optional starting point for teachers in the Master of Environmental Data Science Program at the Bren School (UC Santa Barbara).\nClick here for a template preview.\n\n\n",
      "last_modified": "2021-08-21T08:45:11-07:00"
    },
    {
      "path": "resources.html",
      "title": "Course resources",
      "author": [],
      "contents": "\n\nContents\nMore References\n\nMore References\nThe Practice of Reproducible Research-Case Studies and Lessons from the Data-Intensive Sciences:\nKitzes, Justin, Daniel Turek, and Fatma Deniz. n.d. http://www.practicereproducibleresearch.org/\nReproducible Research with R and RStudio, Chrisotpher Gandrud http://christophergandrud.github.io/RepResR-RStudio/index.html\nInitial steps toward reproducible research, Karl Broman, https://kbroman.org/steps2rr/\nReproducibility guide from ROpenSci https://ropensci.github.io/reproducibility-guide/\n\n\n\n",
      "last_modified": "2021-08-20T21:56:02-07:00"
    },
    {
      "path": "rstudio_debugging.html",
      "title": "Debugging in RStudio",
      "author": [],
      "contents": "\n\nContents\nThe debugging interface\n\nThis section is adapted from the more detailed material available on the RStudio website: https://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio\nDebugging is a broad topic that can be approached in many ways. Generically, at some point you will likely attempt to execute a script in R, receive errors and not know exactly what caused the errors. One approach would be to run your code line by line, but RStudio has some useful built-in debugging features.\nOne basic approach to debugging is to create a breakpoint in your code – this forces your code to “stop” executing when it reaches some certain function or line number in your code, allowing you then to examine the state of various variables, etc. The easiest way to do this is to set the breakpoint by manually clicking next to the desired line number in the code panel, per this web example:\nhttps://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio#stopping-on-a-lineSetting this editor breakpoint creates tracing code associated with the R function object. You can remove the breakpoint by clicking on the red dot by the line number. Also note the Debug toolbar has an option to clear all breakpoints.\nNote: keep in mind that you can’t set breakpoints anywhere. In general, you want to insert breakpoints at top-level expressions or simple, named functions.\nAn alternative way to set breakpoints is with the browser() function. This must be typed into your code, per this web example:\nhttps://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio(image source: https://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio#stopping-on-a-line)\nThe debugging interface\nOnce your code hits a breakpoint, RStudio enters debugging mode. Details on the debugging interface can be found here, but we summarize the main points below:\nThe Environment tab will display the objects in the environment of the currently executing function (i.e., the function’s defined arguments)\nThe Traceback literally traces back how you arrived at the currently executing function (latest executed command is at the top of the list). This is analagous to the “call stack” in other programming languages.\nThe Code window highlights the currently executing function and may create a new tab, named Source Viewer, when the current function the debugger is stepping through is not in the main R script.\nThe Console retains most of its normal functionality in debugging mode, but contains some additional buttons that appear at the top to facilitate moving through code lines (see below).\nDebugging buttons on the console\n\n\n",
      "last_modified": "2021-08-20T21:56:02-07:00"
    }
  ],
  "collections": []
}
